# Ollama Disaggregated Inference Setup

## âœ… Correct Architecture: DGX Prefill â†’ Mac Decode (Both Ollama)

This is the proper disaggregated setup using Ollama's KV cache passing.

### How It Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CODE ANALYSIS PIPELINE (Qwen)                    â”‚
â”‚                                                               â”‚
â”‚  Step 1: DGX Spark 1 (Prefill)                               â”‚
â”‚  â”œâ”€ Ollama: qwen3-coder:30b                                  â”‚
â”‚  â”œâ”€ Process prompt â†’ Generate KV cache                       â”‚
â”‚  â”œâ”€ Fast GPU prefill: ~2-3 seconds                           â”‚
â”‚  â””â”€ Return: prompt + context (KV cache)                      â”‚
â”‚                    â†“                                          â”‚
â”‚  Step 2: Mac Studio 2 (Decode)                               â”‚
â”‚  â”œâ”€ Ollama: qwen3-coder:30b                                  â”‚
â”‚  â”œâ”€ Receive KV cache from DGX                                â”‚
â”‚  â”œâ”€ Generate tokens using cached context                     â”‚
â”‚  â”œâ”€ Fast decode: ~8-10 seconds                               â”‚
â”‚  â””â”€ Return: Generated response                               â”‚
â”‚                                                               â”‚
â”‚  Total: ~10-13 seconds                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           FEEDBACK GENERATION PIPELINE (GPT-OSS)              â”‚
â”‚                                                               â”‚
â”‚  Step 1: DGX Spark 2 (Prefill)                               â”‚
â”‚  â”œâ”€ Ollama: gpt-oss:120b                                     â”‚
â”‚  â”œâ”€ Process prompt â†’ Generate KV cache                       â”‚
â”‚  â”œâ”€ Fast GPU prefill: ~3-4 seconds                           â”‚
â”‚  â””â”€ Return: prompt + context (KV cache)                      â”‚
â”‚                    â†“                                          â”‚
â”‚  Step 2: Mac Studio 1 (Decode)                               â”‚
â”‚  â”œâ”€ Ollama: gpt-oss:120b                                     â”‚
â”‚  â”œâ”€ Receive KV cache from DGX                                â”‚
â”‚  â”œâ”€ Generate tokens using cached context                     â”‚
â”‚  â”œâ”€ Fast decode: ~10-12 seconds                              â”‚
â”‚  â””â”€ Return: Generated response                               â”‚
â”‚                                                               â”‚
â”‚  Total: ~13-16 seconds                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

BOTH PIPELINES RUN IN PARALLEL
Total grading time: ~16 seconds (max of both)
```

## Configuration

### model_config.py
```python
CODE_MODEL = "disaggregated:qwen3-coder:30b"      # DGX Spark 1 â†’ Mac Studio 2
FEEDBACK_MODEL = "disaggregated:gpt-oss:120b"     # DGX Spark 2 â†’ Mac Studio 1

MODEL_SETTINGS = {
    "disaggregated:qwen3-coder:30b": {
        "prefill_url": "http://169.254.150.103:11434",  # DGX Spark 1 Ollama
        "decode_url": "http://169.254.150.102:11434",   # Mac Studio 2 Ollama
        "model_name": "qwen3-coder:30b"
    },
    "disaggregated:gpt-oss:120b": {
        "prefill_url": "http://169.254.150.104:11434",  # DGX Spark 2 Ollama
        "decode_url": "http://localhost:11434",         # Mac Studio 1 Ollama
        "model_name": "gpt-oss:120b"
    }
}
```

## Required Ollama Instances

### DGX Spark 1 (169.254.150.103)
- âœ… Ollama running on port 11434
- âœ… Model: `qwen3-coder:30b` loaded
- ğŸ¯ Role: Prefill for code analysis

### DGX Spark 2 (169.254.150.104)
- âœ… Ollama running on port 11434
- âœ… Model: `gpt-oss:120b` loaded
- ğŸ¯ Role: Prefill for feedback generation

### Mac Studio 1 (localhost)
- âœ… Ollama running on port 11434
- âœ… Model: `gpt-oss:120b` loaded
- ğŸ¯ Role: Decode for feedback generation

### Mac Studio 2 (169.254.150.102)
- âš ï¸ Ollama needs to be running on port 11434
- âš ï¸ Model: `qwen3-coder:30b` needs to be loaded
- ğŸ¯ Role: Decode for code analysis

## Setup Mac Studio 2

On Mac Studio 2, run:
```bash
# Pull the model if not already present
ollama pull qwen3-coder:30b

# Start Ollama (should auto-start, but verify)
ollama serve
```

Verify it's working:
```bash
curl http://169.254.150.102:11434/api/tags
```

## How Ollama KV Cache Passing Works

1. **Prefill Phase** (DGX):
   - Ollama processes the prompt
   - Generates KV cache (context)
   - Returns `context` array with cache data

2. **Decode Phase** (Mac):
   - Receives prompt + context from prefill
   - Ollama uses the context to skip re-processing
   - Only generates new tokens
   - Much faster than regenerating from scratch

3. **Key Benefit**:
   - DGX does heavy lifting (prefill)
   - Mac does efficient token generation
   - Total time is faster than either alone

## Performance Expectations

### Prefill (DGX)
- Qwen 3.0 Coder: ~2-3 seconds
- GPT-OSS 120B: ~3-4 seconds

### Decode (Mac)
- Qwen 3.0 Coder: ~8-10 seconds
- GPT-OSS 120B: ~10-12 seconds

### Total (Parallel)
- Both pipelines: ~16 seconds
- 2x faster than sequential
- 2x faster than single model

## Testing

Test the setup:
```bash
# Test Qwen pipeline
python3 -c "
from disaggregated_client import DisaggregatedClient
client = DisaggregatedClient(
    'http://169.254.150.103:11434',
    'http://169.254.150.102:11434',
    'qwen3-coder:30b'
)
result = client.generate('def hello():', max_tokens=100)
print(f'Response: {result[\"response\"][:100]}')
print(f'Time: {result[\"total_time\"]:.2f}s')
"

# Test GPT-OSS pipeline
python3 -c "
from disaggregated_client import DisaggregatedClient
client = DisaggregatedClient(
    'http://169.254.150.104:11434',
    'http://localhost:11434',
    'gpt-oss:120b'
)
result = client.generate('Provide feedback:', max_tokens=100)
print(f'Response: {result[\"response\"][:100]}')
print(f'Time: {result[\"total_time\"]:.2f}s')
"
```

## Advantages Over Previous Attempts

âœ… **Uses Ollama on both sides**: No Ollamaâ†’MLX incompatibility
âœ… **True KV cache passing**: Mac doesn't regenerate from scratch
âœ… **Faster**: DGX prefill + Mac decode is optimal
âœ… **Reliable**: Ollama's context passing is well-tested
âœ… **Simple**: No custom servers needed

## Ready to Use

Once Mac Studio 2 has Ollama running with qwen3-coder:30b, the system is ready!

Restart the Streamlit app and grade a submission - you should see:
- "Using disaggregated inference for code"
- "Using disaggregated inference for feedback"
- Faster grading times (~16s total)
