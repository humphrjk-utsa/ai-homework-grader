{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Homework Assignment - Lesson 7: String Manipulation and Date/Time Data\n",
    "\n",
    "**Student Name:** [Anathalia Marc-Charles]\n",
    "\n",
    "**Student ID:** [zbf047]\n",
    "\n",
    "**Date Submitted:** [10/12/2025]\n",
    "\n",
    "**Due Date:** [10/12/2025]\n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "Master string manipulation with `stringr` and date/time operations with `lubridate` for real-world business data cleaning and analysis.\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "By completing this assignment, you will:\n",
    "- Clean and standardize messy text data using `stringr` functions\n",
    "- Parse and manipulate dates using `lubridate` functions\n",
    "- Extract information from text and dates for business insights\n",
    "- Combine string and date operations for customer segmentation\n",
    "- Create business-ready reports from raw data\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Complete all tasks in this notebook\n",
    "- Write your code in the designated TODO sections\n",
    "- Use the pipe operator (`%>%`) wherever possible\n",
    "- Add comments explaining your logic\n",
    "- Run all cells to verify your code works\n",
    "- Answer all reflection questions\n",
    "\n",
    "## Datasets\n",
    "\n",
    "You will work with three CSV files:\n",
    "- `customer_feedback.csv` - Customer reviews with messy text\n",
    "- `transaction_log.csv` - Transaction records with dates\n",
    "- `product_catalog.csv` - Product descriptions needing standardization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1_intro",
   "metadata": {},
   "source": [
    "## Part 1: Data Import and Initial Exploration\n",
    "\n",
    "**Business Context:** Before cleaning data, you must understand its structure and quality issues.\n",
    "\n",
    "**Your Tasks:**\n",
    "1. Load required packages (`tidyverse` and `lubridate`)\n",
    "2. Import all three CSV files from the `data/` directory\n",
    "3. Examine the structure and identify data quality issues\n",
    "4. Display sample rows to understand the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "setup",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Packages loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Task 1.1: Load Required Packages\n",
    "# TODO: Load tidyverse (includes stringr)\n",
    "library(tidyverse)\n",
    "\n",
    "\n",
    "# TODO: Load lubridate\n",
    "library(lubridate)\n",
    "\n",
    "cat(\"✅ Packages loaded successfully!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "import_data",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "'/workspaces/assignment-2-version3-anathaliajmc/data'"
      ],
      "text/latex": [
       "'/workspaces/assignment-2-version3-anathaliajmc/data'"
      ],
      "text/markdown": [
       "'/workspaces/assignment-2-version3-anathaliajmc/data'"
      ],
      "text/plain": [
       "[1] \"/workspaces/assignment-2-version3-anathaliajmc/data\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'/workspaces/assignment-2-version3-anathaliajmc/data'"
      ],
      "text/latex": [
       "'/workspaces/assignment-2-version3-anathaliajmc/data'"
      ],
      "text/markdown": [
       "'/workspaces/assignment-2-version3-anathaliajmc/data'"
      ],
      "text/plain": [
       "[1] \"/workspaces/assignment-2-version3-anathaliajmc/data\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data imported successfully!\n",
      "Feedback rows: 100 \n",
      "Transaction rows: 150 \n",
      "Product rows: 75 \n"
     ]
    }
   ],
   "source": [
    "# Task 1.2: Import Datasets\n",
    "getwd()\n",
    "setwd(\"/workspaces/assignment-2-version3-anathaliajmc/data\")\n",
    "getwd()\n",
    "# TODO: Import customer_feedback.csv into a variable called 'feedback'\n",
    "feedback <- read.csv(\"customer_feedback (1).csv\")\n",
    "\n",
    "# TODO: Import transaction_log.csv into a variable called 'transactions'\n",
    "transactions <- read.csv(\"transaction_log.csv\")\n",
    "\n",
    "# TODO: Import product_catalog.csv into a variable called 'products'\n",
    "products <- read.csv(\"product_catalog.csv\")\n",
    "\n",
    "cat(\"✅ Data imported successfully!\\n\")\n",
    "cat(\"Feedback rows:\", nrow(feedback), \"\\n\")\n",
    "cat(\"Transaction rows:\", nrow(transactions), \"\\n\")\n",
    "cat(\"Product rows:\", nrow(products), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "explore_data",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CUSTOMER FEEDBACK DATA ===\n",
      "Feedback structure:\n",
      "'data.frame':\t100 obs. of  6 variables:\n",
      " $ FeedbackID   : int  1 2 3 4 5 6 7 8 9 10 ...\n",
      " $ CustomerID   : int  12 40 34 1 47 13 13 37 49 23 ...\n",
      " $ Customer_Name: chr  \"Bob Wilson\" \"   sarah johnson   \" \"jane smith\" \"JANE SMITH\" ...\n",
      " $ Feedback_Text: chr  \"Highly recommend this item\" \"   Excellent service   \" \"Poor quality control\" \"average product, nothing special\" ...\n",
      " $ Contact_Info : chr  \"bob.wilson@test.org\" \"555-123-4567\" \"jane_smith@company.com\" \"jane_smith@company.com\" ...\n",
      " $ Feedback_Date: chr  \"2024-02-23\" \"2024-01-21\" \"2023-09-02\" \"2023-08-21\" ...\n",
      "\n",
      "First few feedback rows:\n",
      "  FeedbackID CustomerID       Customer_Name                    Feedback_Text\n",
      "1          1         12          Bob Wilson       Highly recommend this item\n",
      "2          2         40    sarah johnson                Excellent service   \n",
      "3          3         34          jane smith             Poor quality control\n",
      "4          4          1          JANE SMITH average product, nothing special\n",
      "5          5         47       michael brown      AMAZING customer support!!!\n",
      "            Contact_Info Feedback_Date\n",
      "1    bob.wilson@test.org    2024-02-23\n",
      "2           555-123-4567    2024-01-21\n",
      "3 jane_smith@company.com    2023-09-02\n",
      "4 jane_smith@company.com    2023-08-21\n",
      "5           555-123-4567    2023-04-24\n",
      "\n",
      "=== TRANSACTION DATA ===\n",
      "Transactions structure:\n",
      "'data.frame':\t150 obs. of  5 variables:\n",
      " $ LogID               : int  1 2 3 4 5 6 7 8 9 10 ...\n",
      " $ CustomerID          : int  26 21 12 6 32 27 31 30 31 13 ...\n",
      " $ Transaction_DateTime: chr  \"4/5/24 14:30\" \"3/15/24 14:30\" \"3/15/24 14:30\" \"3/20/24 9:15\" ...\n",
      " $ Amount              : num  277 175 252 215 269 ...\n",
      " $ Status              : chr  \"Pending\" \"Pending\" \"Pending\" \"Pending\" ...\n",
      "\n",
      "First few transactions rows:\n",
      "  LogID CustomerID Transaction_DateTime Amount    Status\n",
      "1     1         26         4/5/24 14:30 277.22   Pending\n",
      "2     2         21        3/15/24 14:30 175.16   Pending\n",
      "3     3         12        3/15/24 14:30 251.71   Pending\n",
      "4     4          6         3/20/24 9:15 214.98   Pending\n",
      "5     5         32         3/20/24 9:15 268.91 Completed\n",
      "\n",
      "=== PRODUCT CATALOG DATA ===\n",
      "Products structure:\n",
      "'data.frame':\t75 obs. of  5 variables:\n",
      " $ ProductID          : int  1 2 3 4 5 6 7 8 9 10 ...\n",
      " $ Product_Description: chr  \"Apple iPhone 14 Pro - 128GB - Space Black\" \"samsung galaxy s23 ultra 256gb\" \"Apple iPhone 14 Pro - 128GB - Space Black\" \"Apple iPhone 14 Pro - 128GB - Space Black\" ...\n",
      " $ Category           : chr  \"TV\" \"TV\" \"Audio\" \"Shoes\" ...\n",
      " $ Price              : num  964 1817 853 649 586 ...\n",
      " $ In_Stock           : chr  \"Limited\" \"Yes\" \"Yes\" \"Yes\" ...\n",
      "\n",
      "First few products rows:\n",
      "  ProductID                       Product_Description    Category   Price\n",
      "1         1 Apple iPhone 14 Pro - 128GB - Space Black          TV  963.53\n",
      "2         2            samsung galaxy s23 ultra 256gb          TV 1817.44\n",
      "3         3 Apple iPhone 14 Pro - 128GB - Space Black       Audio  852.79\n",
      "4         4 Apple iPhone 14 Pro - 128GB - Space Black       Shoes  648.58\n",
      "5         5            samsung galaxy s23 ultra 256gb Electronics  586.35\n",
      "  In_Stock\n",
      "1  Limited\n",
      "2      Yes\n",
      "3      Yes\n",
      "4      Yes\n",
      "5  Limited\n"
     ]
    }
   ],
   "source": [
    "# Task 1.3: Initial Data Exploration\n",
    "\n",
    "cat(\"=== CUSTOMER FEEDBACK DATA ===\\n\")\n",
    "# TODO: Display structure of feedback using str()\n",
    "cat(\"Feedback structure:\\n\")\n",
    "str(feedback)\n",
    "# TODO: Display first 5 rows of feedback\n",
    "cat(\"\\nFirst few feedback rows:\\n\")\n",
    "print(head(feedback, 5))\n",
    "cat(\"\\n=== TRANSACTION DATA ===\\n\")\n",
    "\n",
    "\n",
    "# TODO: Display structure of transactions\n",
    "cat(\"Transactions structure:\\n\")\n",
    "str(transactions)\n",
    "# TODO: Display first 5 rows of transactions\n",
    "cat(\"\\nFirst few transactions rows:\\n\")\n",
    "print(head(transactions, 5))\n",
    "\n",
    "\n",
    "\n",
    "cat(\"\\n=== PRODUCT CATALOG DATA ===\\n\")\n",
    "# TODO: Display structure of products\n",
    "cat(\"Products structure:\\n\")\n",
    "str(products)\n",
    "# TODO: Display first 5 rows of products\n",
    "cat(\"\\nFirst few products rows:\\n\")\n",
    "print(head(products, 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2_intro",
   "metadata": {},
   "source": [
    "## Part 2: String Cleaning and Standardization\n",
    "\n",
    "**Business Context:** Product names and feedback text often have inconsistent formatting that prevents accurate analysis.\n",
    "\n",
    "**Your Tasks:**\n",
    "1. Clean product names (remove extra spaces, standardize case)\n",
    "2. Standardize product categories\n",
    "3. Clean customer feedback text\n",
    "4. Extract customer names from feedback\n",
    "\n",
    "**Key Functions:** `str_trim()`, `str_squish()`, `str_to_lower()`, `str_to_upper()`, `str_to_title()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "clean_products",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product Name Cleaning Results:\n",
      "                         Product_Description\n",
      "1  Apple iPhone 14 Pro - 128GB - Space Black\n",
      "2             samsung galaxy s23 ultra 256gb\n",
      "3  Apple iPhone 14 Pro - 128GB - Space Black\n",
      "4  Apple iPhone 14 Pro - 128GB - Space Black\n",
      "5             samsung galaxy s23 ultra 256gb\n",
      "6  Apple iPhone 14 Pro - 128GB - Space Black\n",
      "7   DELL XPS 13 Laptop - Intel i7 - 16GB RAM\n",
      "8         hp envy printer - wireless - color\n",
      "9   Nike Air Max 270 - Size 10 - Black/White\n",
      "10         LG 55\" 4K Smart TV - OLED Display\n",
      "                          product_name_clean\n",
      "1  Apple Iphone 14 Pro - 128gb - Space Black\n",
      "2             Samsung Galaxy S23 Ultra 256gb\n",
      "3  Apple Iphone 14 Pro - 128gb - Space Black\n",
      "4  Apple Iphone 14 Pro - 128gb - Space Black\n",
      "5             Samsung Galaxy S23 Ultra 256gb\n",
      "6  Apple Iphone 14 Pro - 128gb - Space Black\n",
      "7   Dell Xps 13 Laptop - Intel I7 - 16gb Ram\n",
      "8         Hp Envy Printer - Wireless - Color\n",
      "9   Nike Air Max 270 - Size 10 - Black/White\n",
      "10         Lg 55\" 4k Smart Tv - Oled Display\n"
     ]
    }
   ],
   "source": [
    "# Task 2.1: Clean Product Names\n",
    "# TODO: Create a new column 'product_name_clean' that:\n",
    "#   - Removes leading/trailing whitespace using str_trim()\n",
    "#   - Converts to Title Case using str_to_title()\n",
    "\n",
    "products_clean <- products %>%\n",
    "  mutate(\n",
    "    # Your code here:\n",
    "    product_name_clean = str_trim(Product_Description),\n",
    "    \n",
    "    product_name_clean = str_to_title(product_name_clean)\n",
    "  )\n",
    "\n",
    "# Display before and after\n",
    "cat(\"Product Name Cleaning Results:\\n\")\n",
    "products_clean %>%\n",
    "  select(Product_Description, product_name_clean) %>%\n",
    "  head(10) %>%\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "clean_categories",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original categories:\n",
      "[1] \"TV\"          \"Audio\"       \"Shoes\"       \"Electronics\" \"Computers\"  \n",
      "\n",
      "Cleaned categories:\n",
      "[1] \"Tv\"          \"Audio\"       \"Shoes\"       \"Electronics\" \"Computers\"  \n"
     ]
    }
   ],
   "source": [
    "# Task 2.2: Standardize Product Categories\n",
    "# TODO: Create a new column 'category_clean' that:\n",
    "#   - Converts category to Title Case\n",
    "#   - Removes any extra whitespace\n",
    "\n",
    "products_clean <- products_clean %>%\n",
    "  mutate(\n",
    "    # Your code here:\n",
    "    category_clean = str_trim(Category),\n",
    "    category_clean = str_to_title(Category)\n",
    "  )\n",
    "\n",
    "# Show unique categories before and after\n",
    "cat(\"Original categories:\\n\")\n",
    "print(unique(products$Category))\n",
    "\n",
    "cat(\"\\nCleaned categories:\\n\")\n",
    "print(unique(products_clean$category_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "clean_feedback",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedback Cleaning Sample:\n",
      "                     Feedback_Text                   feedback_clean\n",
      "1       Highly recommend this item       highly recommend this item\n",
      "2             Excellent service                   excellent service\n",
      "3             Poor quality control             poor quality control\n",
      "4 average product, nothing special average product, nothing special\n",
      "5      AMAZING customer support!!!      amazing customer support!!!\n"
     ]
    }
   ],
   "source": [
    "# Task 2.3: Clean Customer Feedback Text\n",
    "# TODO: Create a new column 'feedback_clean' that:\n",
    "#   - Converts text to lowercase using str_to_lower()\n",
    "#   - Removes extra whitespace using str_squish()\n",
    "\n",
    "feedback_clean <- feedback %>%\n",
    "  mutate(\n",
    "    # Your code here:\n",
    "    feedback_clean = str_to_lower(Feedback_Text),\n",
    "    feedback_clean = str_squish(feedback_clean)\n",
    "  )\n",
    "\n",
    "# Display sample\n",
    "cat(\"Feedback Cleaning Sample:\\n\")\n",
    "feedback_clean %>%\n",
    "  select(Feedback_Text, feedback_clean) %>%\n",
    "  head(5) %>%\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3_intro",
   "metadata": {},
   "source": [
    "## Part 3: Pattern Detection and Extraction\n",
    "\n",
    "**Business Context:** Identifying products with specific features and extracting specifications helps with inventory management and marketing.\n",
    "\n",
    "**Your Tasks:**\n",
    "1. Identify products with specific keywords (wireless, premium, gaming)\n",
    "2. Extract numerical specifications from product names\n",
    "3. Detect sentiment words in customer feedback\n",
    "4. Extract email addresses from feedback\n",
    "\n",
    "**Key Functions:** `str_detect()`, `str_extract()`, `str_count()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "detect_features",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product Feature Detection:\n",
      "                          product_name_clean is_wireless is_premium is_gaming\n",
      "1  Apple Iphone 14 Pro - 128gb - Space Black       FALSE       TRUE     FALSE\n",
      "2             Samsung Galaxy S23 Ultra 256gb       FALSE      FALSE     FALSE\n",
      "3  Apple Iphone 14 Pro - 128gb - Space Black       FALSE       TRUE     FALSE\n",
      "4  Apple Iphone 14 Pro - 128gb - Space Black       FALSE       TRUE     FALSE\n",
      "5             Samsung Galaxy S23 Ultra 256gb       FALSE      FALSE     FALSE\n",
      "6  Apple Iphone 14 Pro - 128gb - Space Black       FALSE       TRUE     FALSE\n",
      "7   Dell Xps 13 Laptop - Intel I7 - 16gb Ram       FALSE      FALSE     FALSE\n",
      "8         Hp Envy Printer - Wireless - Color        TRUE      FALSE     FALSE\n",
      "9   Nike Air Max 270 - Size 10 - Black/White       FALSE      FALSE     FALSE\n",
      "10         Lg 55\" 4k Smart Tv - Oled Display       FALSE      FALSE     FALSE\n",
      "\n",
      "Feature Summary:\n",
      "Wireless products: 17 \n",
      "Premium products: 13 \n",
      "Gaming products: 0 \n"
     ]
    }
   ],
   "source": [
    "# Task 3.1: Detect Product Features\n",
    "# TODO: Create three new columns:\n",
    "#   - is_wireless: TRUE if product name contains \"wireless\" (case-insensitive)\n",
    "#   - is_premium: TRUE if product name contains \"pro\", \"premium\", or \"deluxe\"\n",
    "#   - is_gaming: TRUE if product name contains \"gaming\" or \"gamer\"\n",
    "# Hint: Use str_detect() with str_to_lower() for case-insensitive matching\n",
    "# Hint: Use | (pipe) in regex for OR conditions\n",
    "\n",
    "products_clean <- products_clean %>%\n",
    "  mutate(\n",
    "    # Your code here:\n",
    "    is_wireless = str_detect(str_to_lower(product_name_clean), \"wireless\"),\n",
    "    is_premium = str_detect(str_to_lower(product_name_clean), \"pro|premium|deluxe\"),\n",
    "    is_gaming = str_detect(str_to_lower(product_name_clean), \"gaming|gamer\")\n",
    "  )\n",
    "\n",
    "# Display results\n",
    "cat(\"Product Feature Detection:\\n\")\n",
    "products_clean %>%\n",
    "  select(product_name_clean, is_wireless, is_premium, is_gaming) %>%\n",
    "  head(10) %>%\n",
    "  print()\n",
    "\n",
    "# Summary statistics\n",
    "cat(\"\\nFeature Summary:\\n\")\n",
    "cat(\"Wireless products:\", sum(products_clean$is_wireless), \"\\n\")\n",
    "cat(\"Premium products:\", sum(products_clean$is_premium), \"\\n\")\n",
    "cat(\"Gaming products:\", sum(products_clean$is_gaming), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "extract_specs",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Product Specifications:\n",
      "                          product_name_clean size_number\n",
      "1  Apple Iphone 14 Pro - 128gb - Space Black          14\n",
      "2             Samsung Galaxy S23 Ultra 256gb          23\n",
      "3  Apple Iphone 14 Pro - 128gb - Space Black          14\n",
      "4  Apple Iphone 14 Pro - 128gb - Space Black          14\n",
      "5             Samsung Galaxy S23 Ultra 256gb          23\n",
      "6  Apple Iphone 14 Pro - 128gb - Space Black          14\n",
      "7   Dell Xps 13 Laptop - Intel I7 - 16gb Ram          13\n",
      "8   Nike Air Max 270 - Size 10 - Black/White         270\n",
      "9          Lg 55\" 4k Smart Tv - Oled Display          55\n",
      "10       Sony Wh-1000xm4 Wireless Headphones        1000\n"
     ]
    }
   ],
   "source": [
    "# Task 3.2: Extract Product Specifications\n",
    "# TODO: Create a new column 'size_number' that extracts the first number from product_name\n",
    "# Hint: Use str_extract() with pattern \"\\\\\\\\d+\" to match one or more digits\n",
    "\n",
    "products_clean <- products_clean %>%\n",
    "  mutate(\n",
    "    # Your code here:\n",
    "    size_number = str_extract(product_name_clean, \"\\\\d+\"),\n",
    "  )\n",
    "\n",
    "# Display products with extracted sizes\n",
    "cat(\"Extracted Product Specifications:\\n\")\n",
    "products_clean %>%\n",
    "  filter(!is.na(size_number)) %>%\n",
    "  select(product_name_clean, size_number) %>%\n",
    "  head(10) %>%\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "sentiment_analysis",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis Results:\n",
      "                     feedback_clean positive_words negative_words\n",
      "1        highly recommend this item              0              0\n",
      "2                 excellent service              1              0\n",
      "3              poor quality control              0              0\n",
      "4  average product, nothing special              0              0\n",
      "5       amazing customer support!!!              1              0\n",
      "6       amazing customer support!!!              1              0\n",
      "7  average product, nothing special              0              0\n",
      "8              good value for money              0              0\n",
      "9        highly recommend this item              0              0\n",
      "10       highly recommend this item              0              0\n",
      "   sentiment_score\n",
      "1                0\n",
      "2                1\n",
      "3                0\n",
      "4                0\n",
      "5                1\n",
      "6                1\n",
      "7                0\n",
      "8                0\n",
      "9                0\n",
      "10               0\n",
      "\n",
      "Overall Sentiment Summary:\n",
      "Average sentiment score: 0.18 \n",
      "Positive reviews: 30 \n",
      "Negative reviews: 20 \n"
     ]
    }
   ],
   "source": [
    "# Task 3.3: Simple Sentiment Analysis\n",
    "# TODO: Create three new columns:\n",
    "#   - positive_words: count of positive words (\"great\", \"excellent\", \"love\", \"amazing\")\n",
    "#   - negative_words: count of negative words (\"bad\", \"terrible\", \"hate\", \"awful\")\n",
    "#   - sentiment_score: positive_words - negative_words\n",
    "# Hint: Use str_count() to count pattern occurrences\n",
    "\n",
    "feedback_clean <- feedback_clean %>%\n",
    "  mutate(\n",
    "    # Your code here:\n",
    "    positive_words = str_count(feedback_clean, \"great|excellent|love|amazing\"),\n",
    "    negative_words = str_count(feedback_clean, \"bad|terrible|hate|awful\"),\n",
    "    sentiment_score = positive_words - negative_words\n",
    "  )\n",
    "\n",
    "# Display sentiment analysis results\n",
    "cat(\"Sentiment Analysis Results:\\n\")\n",
    "feedback_clean %>%\n",
    "  select(feedback_clean, positive_words, negative_words, sentiment_score) %>%\n",
    "  head(10) %>%\n",
    "  print()\n",
    "\n",
    "# Summary\n",
    "cat(\"\\nOverall Sentiment Summary:\\n\")\n",
    "cat(\"Average sentiment score:\", mean(feedback_clean$sentiment_score), \"\\n\")\n",
    "cat(\"Positive reviews:\", sum(feedback_clean$sentiment_score > 0), \"\\n\")\n",
    "cat(\"Negative reviews:\", sum(feedback_clean$sentiment_score < 0), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part4_intro",
   "metadata": {},
   "source": [
    "## Part 4: Date Parsing and Component Extraction\n",
    "\n",
    "**Business Context:** Transaction dates need to be parsed and analyzed to understand customer behavior patterns.\n",
    "\n",
    "**Your Tasks:**\n",
    "1. Parse transaction dates from text to Date objects\n",
    "2. Extract date components (year, month, day, weekday)\n",
    "3. Identify weekend vs weekday transactions\n",
    "4. Extract quarter and month names\n",
    "\n",
    "**Key Functions:** `ymd()`, `mdy()`, `dmy()`, `year()`, `month()`, `day()`, `wday()`, `quarter()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "parse_dates",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“\u001b[1m\u001b[22mThere was 1 warning in `mutate()`.\n",
      "\u001b[1m\u001b[22m\u001b[36mℹ\u001b[39m In argument: `date_parsed = parse_date_time(Transaction_DateTime, orders =\n",
      "  c(\"mdy HM\", \"dmy HMS\"))`.\n",
      "Caused by warning:\n",
      "\u001b[33m!\u001b[39m  33 failed to parse.”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date Parsing Results:\n",
      "    Transaction_DateTime         date_parsed\n",
      "1           4/5/24 14:30 2024-04-05 14:30:00\n",
      "2          3/15/24 14:30 2024-03-15 14:30:00\n",
      "3          3/15/24 14:30 2024-03-15 14:30:00\n",
      "4           3/20/24 9:15 2024-03-20 09:15:00\n",
      "5           3/20/24 9:15 2024-03-20 09:15:00\n",
      "6           3/20/24 9:15 2024-03-20 09:15:00\n",
      "7           3/20/24 9:15 2024-03-20 09:15:00\n",
      "8          3/15/24 14:30 2024-03-15 14:30:00\n",
      "9    25-03-2024 16:45:30 2024-03-25 16:45:30\n",
      "10          4/5/24 14:30 2024-04-05 14:30:00\n",
      "11  2024-04-01T10:30:00Z                <NA>\n",
      "12          3/20/24 9:15 2024-03-20 09:15:00\n",
      "13          3/20/24 9:15 2024-03-20 09:15:00\n",
      "14          3/20/24 9:15 2024-03-20 09:15:00\n",
      "15          3/20/24 9:15 2024-03-20 09:15:00\n",
      "16  2024-04-01T10:30:00Z                <NA>\n",
      "17   25-03-2024 16:45:30 2024-03-25 16:45:30\n",
      "18          4/5/24 14:30 2024-04-05 14:30:00\n",
      "19          3/20/24 9:15 2024-03-20 09:15:00\n",
      "20          3/20/24 9:15 2024-03-20 09:15:00\n",
      "21   25-03-2024 16:45:30 2024-03-25 16:45:30\n",
      "22   25-03-2024 16:45:30 2024-03-25 16:45:30\n",
      "23          3/20/24 9:15 2024-03-20 09:15:00\n",
      "24   25-03-2024 16:45:30 2024-03-25 16:45:30\n",
      "25          4/5/24 14:30 2024-04-05 14:30:00\n",
      "26         3/15/24 14:30 2024-03-15 14:30:00\n",
      "27  2024-04-01T10:30:00Z                <NA>\n",
      "28          4/5/24 14:30 2024-04-05 14:30:00\n",
      "29         3/15/24 14:30 2024-03-15 14:30:00\n",
      "30          4/5/24 14:30 2024-04-05 14:30:00\n",
      "31          3/20/24 9:15 2024-03-20 09:15:00\n",
      "32   25-03-2024 16:45:30 2024-03-25 16:45:30\n",
      "33          3/20/24 9:15 2024-03-20 09:15:00\n",
      "34  2024-04-01T10:30:00Z                <NA>\n",
      "35         3/15/24 14:30 2024-03-15 14:30:00\n",
      "36         3/15/24 14:30 2024-03-15 14:30:00\n",
      "37         3/15/24 14:30 2024-03-15 14:30:00\n",
      "38          3/20/24 9:15 2024-03-20 09:15:00\n",
      "39  2024-04-01T10:30:00Z                <NA>\n",
      "40   25-03-2024 16:45:30 2024-03-25 16:45:30\n",
      "41          4/5/24 14:30 2024-04-05 14:30:00\n",
      "42         3/15/24 14:30 2024-03-15 14:30:00\n",
      "43          3/20/24 9:15 2024-03-20 09:15:00\n",
      "44          3/20/24 9:15 2024-03-20 09:15:00\n",
      "45         3/15/24 14:30 2024-03-15 14:30:00\n",
      "46          3/20/24 9:15 2024-03-20 09:15:00\n",
      "47         3/15/24 14:30 2024-03-15 14:30:00\n",
      "48          3/20/24 9:15 2024-03-20 09:15:00\n",
      "49          3/20/24 9:15 2024-03-20 09:15:00\n",
      "50  2024-04-01T10:30:00Z                <NA>\n",
      "51          4/5/24 14:30 2024-04-05 14:30:00\n",
      "52          4/5/24 14:30 2024-04-05 14:30:00\n",
      "53  2024-04-01T10:30:00Z                <NA>\n",
      "54          4/5/24 14:30 2024-04-05 14:30:00\n",
      "55          4/5/24 14:30 2024-04-05 14:30:00\n",
      "56          4/5/24 14:30 2024-04-05 14:30:00\n",
      "57          4/5/24 14:30 2024-04-05 14:30:00\n",
      "58  2024-04-01T10:30:00Z                <NA>\n",
      "59  2024-04-01T10:30:00Z                <NA>\n",
      "60   25-03-2024 16:45:30 2024-03-25 16:45:30\n",
      "61  2024-04-01T10:30:00Z                <NA>\n",
      "62   25-03-2024 16:45:30 2024-03-25 16:45:30\n",
      "63         3/15/24 14:30 2024-03-15 14:30:00\n",
      "64          3/20/24 9:15 2024-03-20 09:15:00\n",
      "65  2024-04-01T10:30:00Z                <NA>\n",
      "66  2024-04-01T10:30:00Z                <NA>\n",
      "67  2024-04-01T10:30:00Z                <NA>\n",
      "68   25-03-2024 16:45:30 2024-03-25 16:45:30\n",
      "69   25-03-2024 16:45:30 2024-03-25 16:45:30\n",
      "70          3/20/24 9:15 2024-03-20 09:15:00\n",
      "71   25-03-2024 16:45:30 2024-03-25 16:45:30\n",
      "72          4/5/24 14:30 2024-04-05 14:30:00\n",
      "73   25-03-2024 16:45:30 2024-03-25 16:45:30\n",
      "74   25-03-2024 16:45:30 2024-03-25 16:45:30\n",
      "75  2024-04-01T10:30:00Z                <NA>\n",
      "76          3/20/24 9:15 2024-03-20 09:15:00\n",
      "77  2024-04-01T10:30:00Z                <NA>\n",
      "78          4/5/24 14:30 2024-04-05 14:30:00\n",
      "79   25-03-2024 16:45:30 2024-03-25 16:45:30\n",
      "80          4/5/24 14:30 2024-04-05 14:30:00\n",
      "81         3/15/24 14:30 2024-03-15 14:30:00\n",
      "82          4/5/24 14:30 2024-04-05 14:30:00\n",
      "83          3/20/24 9:15 2024-03-20 09:15:00\n",
      "84          4/5/24 14:30 2024-04-05 14:30:00\n",
      "85  2024-04-01T10:30:00Z                <NA>\n",
      "86  2024-04-01T10:30:00Z                <NA>\n",
      "87  2024-04-01T10:30:00Z                <NA>\n",
      "88          3/20/24 9:15 2024-03-20 09:15:00\n",
      "89          3/20/24 9:15 2024-03-20 09:15:00\n",
      "90          3/20/24 9:15 2024-03-20 09:15:00\n",
      "91          4/5/24 14:30 2024-04-05 14:30:00\n",
      "92  2024-04-01T10:30:00Z                <NA>\n",
      "93   25-03-2024 16:45:30 2024-03-25 16:45:30\n",
      "94   25-03-2024 16:45:30 2024-03-25 16:45:30\n",
      "95         3/15/24 14:30 2024-03-15 14:30:00\n",
      "96   25-03-2024 16:45:30 2024-03-25 16:45:30\n",
      "97          3/20/24 9:15 2024-03-20 09:15:00\n",
      "98         3/15/24 14:30 2024-03-15 14:30:00\n",
      "99         3/15/24 14:30 2024-03-15 14:30:00\n",
      "100  25-03-2024 16:45:30 2024-03-25 16:45:30\n",
      "101 2024-04-01T10:30:00Z                <NA>\n",
      "102  25-03-2024 16:45:30 2024-03-25 16:45:30\n",
      "103        3/15/24 14:30 2024-03-15 14:30:00\n",
      "104  25-03-2024 16:45:30 2024-03-25 16:45:30\n",
      "105         4/5/24 14:30 2024-04-05 14:30:00\n",
      "106 2024-04-01T10:30:00Z                <NA>\n",
      "107         4/5/24 14:30 2024-04-05 14:30:00\n",
      "108 2024-04-01T10:30:00Z                <NA>\n",
      "109         3/20/24 9:15 2024-03-20 09:15:00\n",
      "110         3/20/24 9:15 2024-03-20 09:15:00\n",
      "111 2024-04-01T10:30:00Z                <NA>\n",
      "112  25-03-2024 16:45:30 2024-03-25 16:45:30\n",
      "113 2024-04-01T10:30:00Z                <NA>\n",
      "114  25-03-2024 16:45:30 2024-03-25 16:45:30\n",
      "115         4/5/24 14:30 2024-04-05 14:30:00\n",
      "116        3/15/24 14:30 2024-03-15 14:30:00\n",
      "117 2024-04-01T10:30:00Z                <NA>\n",
      "118 2024-04-01T10:30:00Z                <NA>\n",
      "119 2024-04-01T10:30:00Z                <NA>\n",
      "120        3/15/24 14:30 2024-03-15 14:30:00\n",
      "121 2024-04-01T10:30:00Z                <NA>\n",
      "122         4/5/24 14:30 2024-04-05 14:30:00\n",
      "123        3/15/24 14:30 2024-03-15 14:30:00\n",
      "124         4/5/24 14:30 2024-04-05 14:30:00\n",
      "125  25-03-2024 16:45:30 2024-03-25 16:45:30\n",
      "126         3/20/24 9:15 2024-03-20 09:15:00\n",
      "127         3/20/24 9:15 2024-03-20 09:15:00\n",
      "128 2024-04-01T10:30:00Z                <NA>\n",
      "129        3/15/24 14:30 2024-03-15 14:30:00\n",
      "130 2024-04-01T10:30:00Z                <NA>\n",
      "131 2024-04-01T10:30:00Z                <NA>\n",
      "132         4/5/24 14:30 2024-04-05 14:30:00\n",
      "133        3/15/24 14:30 2024-03-15 14:30:00\n",
      "134        3/15/24 14:30 2024-03-15 14:30:00\n",
      "135        3/15/24 14:30 2024-03-15 14:30:00\n",
      "136         3/20/24 9:15 2024-03-20 09:15:00\n",
      "137        3/15/24 14:30 2024-03-15 14:30:00\n",
      "138         4/5/24 14:30 2024-04-05 14:30:00\n",
      "139 2024-04-01T10:30:00Z                <NA>\n",
      "140        3/15/24 14:30 2024-03-15 14:30:00\n",
      "141         4/5/24 14:30 2024-04-05 14:30:00\n",
      "142 2024-04-01T10:30:00Z                <NA>\n",
      "143        3/15/24 14:30 2024-03-15 14:30:00\n",
      "144  25-03-2024 16:45:30 2024-03-25 16:45:30\n",
      "145  25-03-2024 16:45:30 2024-03-25 16:45:30\n",
      "146  25-03-2024 16:45:30 2024-03-25 16:45:30\n",
      "147         4/5/24 14:30 2024-04-05 14:30:00\n",
      "148         3/20/24 9:15 2024-03-20 09:15:00\n",
      "149         3/20/24 9:15 2024-03-20 09:15:00\n",
      "150  25-03-2024 16:45:30 2024-03-25 16:45:30\n"
     ]
    }
   ],
   "source": [
    "# Task 4.1: Parse Transaction Dates\n",
    "# TODO: Create a new column 'date_parsed' that parses the transaction_date column\n",
    "# Hint: Check the format of transaction_date first, then use ymd(), mdy(), or dmy()\n",
    "\n",
    "transactions_clean <- transactions %>%\n",
    "  mutate(\n",
    "    # Your code here:\n",
    "  date_parsed = parse_date_time(Transaction_DateTime, orders = c(\"mdy HM\", \"dmy HMS\"))\n",
    "  )\n",
    "\n",
    "# Verify parsing worked\n",
    "cat(\"Date Parsing Results:\\n\")\n",
    "transactions_clean %>%\n",
    "  select(Transaction_DateTime, date_parsed) %>%\n",
    "  head(Inf) %>%\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "extract_components",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date Component Extraction:\n",
      "            date_parsed trans_month_name trans_day trans_weekday trans_quarter\n",
      "1   2024-04-05 14:30:00            April         5        Friday             2\n",
      "2   2024-03-15 14:30:00            March        15        Friday             1\n",
      "3   2024-03-15 14:30:00            March        15        Friday             1\n",
      "4   2024-03-20 09:15:00            March        20     Wednesday             1\n",
      "5   2024-03-20 09:15:00            March        20     Wednesday             1\n",
      "6   2024-03-20 09:15:00            March        20     Wednesday             1\n",
      "7   2024-03-20 09:15:00            March        20     Wednesday             1\n",
      "8   2024-03-15 14:30:00            March        15        Friday             1\n",
      "9   2024-03-25 16:45:30            March        25        Monday             1\n",
      "10  2024-04-05 14:30:00            April         5        Friday             2\n",
      "11                 <NA>             <NA>        NA          <NA>            NA\n",
      "12  2024-03-20 09:15:00            March        20     Wednesday             1\n",
      "13  2024-03-20 09:15:00            March        20     Wednesday             1\n",
      "14  2024-03-20 09:15:00            March        20     Wednesday             1\n",
      "15  2024-03-20 09:15:00            March        20     Wednesday             1\n",
      "16                 <NA>             <NA>        NA          <NA>            NA\n",
      "17  2024-03-25 16:45:30            March        25        Monday             1\n",
      "18  2024-04-05 14:30:00            April         5        Friday             2\n",
      "19  2024-03-20 09:15:00            March        20     Wednesday             1\n",
      "20  2024-03-20 09:15:00            March        20     Wednesday             1\n",
      "21  2024-03-25 16:45:30            March        25        Monday             1\n",
      "22  2024-03-25 16:45:30            March        25        Monday             1\n",
      "23  2024-03-20 09:15:00            March        20     Wednesday             1\n",
      "24  2024-03-25 16:45:30            March        25        Monday             1\n",
      "25  2024-04-05 14:30:00            April         5        Friday             2\n",
      "26  2024-03-15 14:30:00            March        15        Friday             1\n",
      "27                 <NA>             <NA>        NA          <NA>            NA\n",
      "28  2024-04-05 14:30:00            April         5        Friday             2\n",
      "29  2024-03-15 14:30:00            March        15        Friday             1\n",
      "30  2024-04-05 14:30:00            April         5        Friday             2\n",
      "31  2024-03-20 09:15:00            March        20     Wednesday             1\n",
      "32  2024-03-25 16:45:30            March        25        Monday             1\n",
      "33  2024-03-20 09:15:00            March        20     Wednesday             1\n",
      "34                 <NA>             <NA>        NA          <NA>            NA\n",
      "35  2024-03-15 14:30:00            March        15        Friday             1\n",
      "36  2024-03-15 14:30:00            March        15        Friday             1\n",
      "37  2024-03-15 14:30:00            March        15        Friday             1\n",
      "38  2024-03-20 09:15:00            March        20     Wednesday             1\n",
      "39                 <NA>             <NA>        NA          <NA>            NA\n",
      "40  2024-03-25 16:45:30            March        25        Monday             1\n",
      "41  2024-04-05 14:30:00            April         5        Friday             2\n",
      "42  2024-03-15 14:30:00            March        15        Friday             1\n",
      "43  2024-03-20 09:15:00            March        20     Wednesday             1\n",
      "44  2024-03-20 09:15:00            March        20     Wednesday             1\n",
      "45  2024-03-15 14:30:00            March        15        Friday             1\n",
      "46  2024-03-20 09:15:00            March        20     Wednesday             1\n",
      "47  2024-03-15 14:30:00            March        15        Friday             1\n",
      "48  2024-03-20 09:15:00            March        20     Wednesday             1\n",
      "49  2024-03-20 09:15:00            March        20     Wednesday             1\n",
      "50                 <NA>             <NA>        NA          <NA>            NA\n",
      "51  2024-04-05 14:30:00            April         5        Friday             2\n",
      "52  2024-04-05 14:30:00            April         5        Friday             2\n",
      "53                 <NA>             <NA>        NA          <NA>            NA\n",
      "54  2024-04-05 14:30:00            April         5        Friday             2\n",
      "55  2024-04-05 14:30:00            April         5        Friday             2\n",
      "56  2024-04-05 14:30:00            April         5        Friday             2\n",
      "57  2024-04-05 14:30:00            April         5        Friday             2\n",
      "58                 <NA>             <NA>        NA          <NA>            NA\n",
      "59                 <NA>             <NA>        NA          <NA>            NA\n",
      "60  2024-03-25 16:45:30            March        25        Monday             1\n",
      "61                 <NA>             <NA>        NA          <NA>            NA\n",
      "62  2024-03-25 16:45:30            March        25        Monday             1\n",
      "63  2024-03-15 14:30:00            March        15        Friday             1\n",
      "64  2024-03-20 09:15:00            March        20     Wednesday             1\n",
      "65                 <NA>             <NA>        NA          <NA>            NA\n",
      "66                 <NA>             <NA>        NA          <NA>            NA\n",
      "67                 <NA>             <NA>        NA          <NA>            NA\n",
      "68  2024-03-25 16:45:30            March        25        Monday             1\n",
      "69  2024-03-25 16:45:30            March        25        Monday             1\n",
      "70  2024-03-20 09:15:00            March        20     Wednesday             1\n",
      "71  2024-03-25 16:45:30            March        25        Monday             1\n",
      "72  2024-04-05 14:30:00            April         5        Friday             2\n",
      "73  2024-03-25 16:45:30            March        25        Monday             1\n",
      "74  2024-03-25 16:45:30            March        25        Monday             1\n",
      "75                 <NA>             <NA>        NA          <NA>            NA\n",
      "76  2024-03-20 09:15:00            March        20     Wednesday             1\n",
      "77                 <NA>             <NA>        NA          <NA>            NA\n",
      "78  2024-04-05 14:30:00            April         5        Friday             2\n",
      "79  2024-03-25 16:45:30            March        25        Monday             1\n",
      "80  2024-04-05 14:30:00            April         5        Friday             2\n",
      "81  2024-03-15 14:30:00            March        15        Friday             1\n",
      "82  2024-04-05 14:30:00            April         5        Friday             2\n",
      "83  2024-03-20 09:15:00            March        20     Wednesday             1\n",
      "84  2024-04-05 14:30:00            April         5        Friday             2\n",
      "85                 <NA>             <NA>        NA          <NA>            NA\n",
      "86                 <NA>             <NA>        NA          <NA>            NA\n",
      "87                 <NA>             <NA>        NA          <NA>            NA\n",
      "88  2024-03-20 09:15:00            March        20     Wednesday             1\n",
      "89  2024-03-20 09:15:00            March        20     Wednesday             1\n",
      "90  2024-03-20 09:15:00            March        20     Wednesday             1\n",
      "91  2024-04-05 14:30:00            April         5        Friday             2\n",
      "92                 <NA>             <NA>        NA          <NA>            NA\n",
      "93  2024-03-25 16:45:30            March        25        Monday             1\n",
      "94  2024-03-25 16:45:30            March        25        Monday             1\n",
      "95  2024-03-15 14:30:00            March        15        Friday             1\n",
      "96  2024-03-25 16:45:30            March        25        Monday             1\n",
      "97  2024-03-20 09:15:00            March        20     Wednesday             1\n",
      "98  2024-03-15 14:30:00            March        15        Friday             1\n",
      "99  2024-03-15 14:30:00            March        15        Friday             1\n",
      "100 2024-03-25 16:45:30            March        25        Monday             1\n",
      "101                <NA>             <NA>        NA          <NA>            NA\n",
      "102 2024-03-25 16:45:30            March        25        Monday             1\n",
      "103 2024-03-15 14:30:00            March        15        Friday             1\n",
      "104 2024-03-25 16:45:30            March        25        Monday             1\n",
      "105 2024-04-05 14:30:00            April         5        Friday             2\n",
      "106                <NA>             <NA>        NA          <NA>            NA\n",
      "107 2024-04-05 14:30:00            April         5        Friday             2\n",
      "108                <NA>             <NA>        NA          <NA>            NA\n",
      "109 2024-03-20 09:15:00            March        20     Wednesday             1\n",
      "110 2024-03-20 09:15:00            March        20     Wednesday             1\n",
      "111                <NA>             <NA>        NA          <NA>            NA\n",
      "112 2024-03-25 16:45:30            March        25        Monday             1\n",
      "113                <NA>             <NA>        NA          <NA>            NA\n",
      "114 2024-03-25 16:45:30            March        25        Monday             1\n",
      "115 2024-04-05 14:30:00            April         5        Friday             2\n",
      "116 2024-03-15 14:30:00            March        15        Friday             1\n",
      "117                <NA>             <NA>        NA          <NA>            NA\n",
      "118                <NA>             <NA>        NA          <NA>            NA\n",
      "119                <NA>             <NA>        NA          <NA>            NA\n",
      "120 2024-03-15 14:30:00            March        15        Friday             1\n",
      "121                <NA>             <NA>        NA          <NA>            NA\n",
      "122 2024-04-05 14:30:00            April         5        Friday             2\n",
      "123 2024-03-15 14:30:00            March        15        Friday             1\n",
      "124 2024-04-05 14:30:00            April         5        Friday             2\n",
      "125 2024-03-25 16:45:30            March        25        Monday             1\n",
      "126 2024-03-20 09:15:00            March        20     Wednesday             1\n",
      "127 2024-03-20 09:15:00            March        20     Wednesday             1\n",
      "128                <NA>             <NA>        NA          <NA>            NA\n",
      "129 2024-03-15 14:30:00            March        15        Friday             1\n",
      "130                <NA>             <NA>        NA          <NA>            NA\n",
      "131                <NA>             <NA>        NA          <NA>            NA\n",
      "132 2024-04-05 14:30:00            April         5        Friday             2\n",
      "133 2024-03-15 14:30:00            March        15        Friday             1\n",
      "134 2024-03-15 14:30:00            March        15        Friday             1\n",
      "135 2024-03-15 14:30:00            March        15        Friday             1\n",
      "136 2024-03-20 09:15:00            March        20     Wednesday             1\n",
      "137 2024-03-15 14:30:00            March        15        Friday             1\n",
      "138 2024-04-05 14:30:00            April         5        Friday             2\n",
      "139                <NA>             <NA>        NA          <NA>            NA\n",
      "140 2024-03-15 14:30:00            March        15        Friday             1\n",
      "141 2024-04-05 14:30:00            April         5        Friday             2\n",
      "142                <NA>             <NA>        NA          <NA>            NA\n",
      "143 2024-03-15 14:30:00            March        15        Friday             1\n",
      "144 2024-03-25 16:45:30            March        25        Monday             1\n",
      "145 2024-03-25 16:45:30            March        25        Monday             1\n",
      "146 2024-03-25 16:45:30            March        25        Monday             1\n",
      "147 2024-04-05 14:30:00            April         5        Friday             2\n",
      "148 2024-03-20 09:15:00            March        20     Wednesday             1\n",
      "149 2024-03-20 09:15:00            March        20     Wednesday             1\n",
      "150 2024-03-25 16:45:30            March        25        Monday             1\n"
     ]
    }
   ],
   "source": [
    "# Task 4.2: Extract Date Components\n",
    "# TODO: Create the following new columns:\n",
    "#   - trans_year: Extract year from date_parsed\n",
    "#   - trans_month: Extract month number from date_parsed\n",
    "#   - trans_month_name: Extract month name (use label=TRUE, abbr=FALSE)\n",
    "#   - trans_day: Extract day of month from date_parsed\n",
    "#   - trans_weekday: Extract weekday name (use label=TRUE, abbr=FALSE)\n",
    "#   - trans_quarter: Extract quarter from date_parsed\n",
    "\n",
    "transactions_clean <- transactions_clean %>%\n",
    "  mutate(\n",
    "    # Your code here:\n",
    "    trans_year = year(date_parsed),\n",
    "    trans_month = month(date_parsed),\n",
    "    trans_month_name = month(date_parsed, label = TRUE, abbr = FALSE),\n",
    "    trans_day = day(date_parsed),\n",
    "    trans_weekday = wday(date_parsed, label = TRUE, abbr=FALSE),\n",
    "    trans_quarter = quarter(date_parsed)\n",
    "  )\n",
    "\n",
    "# Display results\n",
    "cat(\"Date Component Extraction:\\n\")\n",
    "transactions_clean %>%\n",
    "  select(date_parsed, trans_month_name, trans_day, trans_weekday, trans_quarter) %>%\n",
    "  head(Inf) %>%\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "weekend_flag",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weekend vs Weekday Transactions:\n",
      "\n",
      "FALSE \n",
      "  150 \n",
      "\n",
      "Percentage of weekend transactions: 0 %\n"
     ]
    }
   ],
   "source": [
    "# Task 4.3: Identify Weekend Transactions\n",
    "# TODO: Create a new column 'is_weekend' that is TRUE if the transaction was on Saturday or Sunday\n",
    "# Hint: Use wday() which returns 1 for Sunday and 7 for Saturday\n",
    "# Hint: Use %in% c(1, 7) to check if day is weekend\n",
    "\n",
    "transactions_clean <- transactions_clean %>%\n",
    "  mutate(\n",
    "    # Your code here:\n",
    "    is_weekend = wday(date_parsed) %in% c(1, 7)\n",
    "  )\n",
    "\n",
    "# Summary\n",
    "cat(\"Weekend vs Weekday Transactions:\\n\")\n",
    "table(transactions_clean$is_weekend) %>% print()\n",
    "\n",
    "cat(\"\\nPercentage of weekend transactions:\",\n",
    "    round(sum(transactions_clean$is_weekend) / nrow(transactions_clean) * 100, 1), \"%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part5_intro",
   "metadata": {},
   "source": [
    "## Part 5: Date Calculations and Customer Recency Analysis\n",
    "\n",
    "**Business Context:** Understanding how recently customers transacted helps identify at-risk customers for re-engagement campaigns.\n",
    "\n",
    "**Your Tasks:**\n",
    "1. Calculate days since each transaction\n",
    "2. Categorize customers by recency (Recent, Moderate, Old)\n",
    "3. Identify customers who haven't transacted in 90+ days\n",
    "4. Calculate average days between transactions per customer\n",
    "\n",
    "**Key Functions:** `today()`, date arithmetic, `case_when()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "calculate_recency",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Days Since Transaction:\n",
      "   CustomerID         date_parsed days_since\n",
      "1          21 2024-03-15 14:30:00        577\n",
      "2          12 2024-03-15 14:30:00        577\n",
      "3          30 2024-03-15 14:30:00        577\n",
      "4          45 2024-03-15 14:30:00        577\n",
      "5           2 2024-03-15 14:30:00        577\n",
      "6          18 2024-03-15 14:30:00        577\n",
      "7          34 2024-03-15 14:30:00        577\n",
      "8          48 2024-03-15 14:30:00        577\n",
      "9          28 2024-03-15 14:30:00        577\n",
      "10         30 2024-03-15 14:30:00        577\n"
     ]
    }
   ],
   "source": [
    "# Task 5.1: Calculate Days Since Transaction\n",
    "# TODO: Create a new column 'days_since' that calculates days from date_parsed to today()\n",
    "# Hint: Use as.numeric(today() - date_parsed)\n",
    "\n",
    "transactions_clean <- transactions_clean %>%\n",
    "  mutate(\n",
    "    # Your code here:\n",
    "    date_only = as.Date(date_parsed),\n",
    "    days_since = as.numeric(today() - date_only)\n",
    "  )\n",
    "\n",
    "# Display results\n",
    "cat(\"Days Since Transaction:\\n\")\n",
    "transactions_clean %>%\n",
    "  select(CustomerID, date_parsed, days_since) %>%\n",
    "  arrange(desc(days_since)) %>%\n",
    "  head(10) %>%\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "recency_categories",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recency Category Distribution:\n",
      "\n",
      "At Risk \n",
      "    150 \n",
      "\n",
      "At-Risk Customers (>90 days):\n",
      "    CustomerID         date_parsed days_since\n",
      "1           21 2024-03-15 14:30:00        577\n",
      "2           12 2024-03-15 14:30:00        577\n",
      "3           30 2024-03-15 14:30:00        577\n",
      "4           45 2024-03-15 14:30:00        577\n",
      "5            2 2024-03-15 14:30:00        577\n",
      "6           18 2024-03-15 14:30:00        577\n",
      "7           34 2024-03-15 14:30:00        577\n",
      "8           48 2024-03-15 14:30:00        577\n",
      "9           28 2024-03-15 14:30:00        577\n",
      "10          30 2024-03-15 14:30:00        577\n",
      "11          33 2024-03-15 14:30:00        577\n",
      "12          11 2024-03-15 14:30:00        577\n",
      "13          36 2024-03-15 14:30:00        577\n",
      "14           6 2024-03-15 14:30:00        577\n",
      "15           8 2024-03-15 14:30:00        577\n",
      "16          38 2024-03-15 14:30:00        577\n",
      "17          49 2024-03-15 14:30:00        577\n",
      "18          28 2024-03-15 14:30:00        577\n",
      "19          44 2024-03-15 14:30:00        577\n",
      "20          33 2024-03-15 14:30:00        577\n",
      "21          29 2024-03-15 14:30:00        577\n",
      "22          14 2024-03-15 14:30:00        577\n",
      "23          17 2024-03-15 14:30:00        577\n",
      "24          17 2024-03-15 14:30:00        577\n",
      "25          48 2024-03-15 14:30:00        577\n",
      "26          41 2024-03-15 14:30:00        577\n",
      "27          42 2024-03-15 14:30:00        577\n",
      "28           6 2024-03-20 09:15:00        572\n",
      "29          32 2024-03-20 09:15:00        572\n",
      "30          27 2024-03-20 09:15:00        572\n",
      "31          31 2024-03-20 09:15:00        572\n",
      "32          30 2024-03-20 09:15:00        572\n",
      "33          33 2024-03-20 09:15:00        572\n",
      "34          13 2024-03-20 09:15:00        572\n",
      "35           7 2024-03-20 09:15:00        572\n",
      "36          34 2024-03-20 09:15:00        572\n",
      "37           9 2024-03-20 09:15:00        572\n",
      "38          21 2024-03-20 09:15:00        572\n",
      "39           1 2024-03-20 09:15:00        572\n",
      "40          17 2024-03-20 09:15:00        572\n",
      "41          48 2024-03-20 09:15:00        572\n",
      "42           1 2024-03-20 09:15:00        572\n",
      "43          14 2024-03-20 09:15:00        572\n",
      "44          42 2024-03-20 09:15:00        572\n",
      "45          50 2024-03-20 09:15:00        572\n",
      "46           8 2024-03-20 09:15:00        572\n",
      "47          31 2024-03-20 09:15:00        572\n",
      "48          21 2024-03-20 09:15:00        572\n",
      "49          25 2024-03-20 09:15:00        572\n",
      "50          12 2024-03-20 09:15:00        572\n",
      "51          31 2024-03-20 09:15:00        572\n",
      "52          28 2024-03-20 09:15:00        572\n",
      "53           8 2024-03-20 09:15:00        572\n",
      "54          43 2024-03-20 09:15:00        572\n",
      "55          25 2024-03-20 09:15:00        572\n",
      "56          21 2024-03-20 09:15:00        572\n",
      "57          13 2024-03-20 09:15:00        572\n",
      "58          28 2024-03-20 09:15:00        572\n",
      "59          34 2024-03-20 09:15:00        572\n",
      "60          37 2024-03-20 09:15:00        572\n",
      "61          24 2024-03-20 09:15:00        572\n",
      "62          31 2024-03-25 16:45:30        567\n",
      "63          25 2024-03-25 16:45:30        567\n",
      "64          35 2024-03-25 16:45:30        567\n",
      "65          12 2024-03-25 16:45:30        567\n",
      "66          39 2024-03-25 16:45:30        567\n",
      "67           2 2024-03-25 16:45:30        567\n",
      "68          11 2024-03-25 16:45:30        567\n",
      "69          48 2024-03-25 16:45:30        567\n",
      "70          44 2024-03-25 16:45:30        567\n",
      "71           6 2024-03-25 16:45:30        567\n",
      "72           7 2024-03-25 16:45:30        567\n",
      "73          31 2024-03-25 16:45:30        567\n",
      "74          14 2024-03-25 16:45:30        567\n",
      "75          36 2024-03-25 16:45:30        567\n",
      "76           3 2024-03-25 16:45:30        567\n",
      "77          45 2024-03-25 16:45:30        567\n",
      "78          24 2024-03-25 16:45:30        567\n",
      "79           7 2024-03-25 16:45:30        567\n",
      "80          48 2024-03-25 16:45:30        567\n",
      "81          38 2024-03-25 16:45:30        567\n",
      "82          50 2024-03-25 16:45:30        567\n",
      "83          45 2024-03-25 16:45:30        567\n",
      "84          27 2024-03-25 16:45:30        567\n",
      "85           1 2024-03-25 16:45:30        567\n",
      "86           9 2024-03-25 16:45:30        567\n",
      "87          35 2024-03-25 16:45:30        567\n",
      "88          35 2024-03-25 16:45:30        567\n",
      "89          42 2024-03-25 16:45:30        567\n",
      "90          26 2024-04-05 14:30:00        556\n",
      "91          13 2024-04-05 14:30:00        556\n",
      "92          44 2024-04-05 14:30:00        556\n",
      "93          22 2024-04-05 14:30:00        556\n",
      "94          13 2024-04-05 14:30:00        556\n",
      "95          35 2024-04-05 14:30:00        556\n",
      "96          30 2024-04-05 14:30:00        556\n",
      "97          48 2024-04-05 14:30:00        556\n",
      "98           4 2024-04-05 14:30:00        556\n",
      "99          44 2024-04-05 14:30:00        556\n",
      "100          8 2024-04-05 14:30:00        556\n",
      "101         12 2024-04-05 14:30:00        556\n",
      "102         40 2024-04-05 14:30:00        556\n",
      "103         10 2024-04-05 14:30:00        556\n",
      "104         17 2024-04-05 14:30:00        556\n",
      "105         29 2024-04-05 14:30:00        556\n",
      "106          5 2024-04-05 14:30:00        556\n",
      "107         19 2024-04-05 14:30:00        556\n",
      "108          4 2024-04-05 14:30:00        556\n",
      "109         30 2024-04-05 14:30:00        556\n",
      "110         23 2024-04-05 14:30:00        556\n",
      "111         28 2024-04-05 14:30:00        556\n",
      "112         10 2024-04-05 14:30:00        556\n",
      "113         46 2024-04-05 14:30:00        556\n",
      "114         13 2024-04-05 14:30:00        556\n",
      "115         19 2024-04-05 14:30:00        556\n",
      "116         48 2024-04-05 14:30:00        556\n",
      "117         38 2024-04-05 14:30:00        556\n",
      "118         28                <NA>         NA\n",
      "119          1                <NA>         NA\n",
      "120         48                <NA>         NA\n",
      "121         40                <NA>         NA\n",
      "122         24                <NA>         NA\n",
      "123          1                <NA>         NA\n",
      "124         25                <NA>         NA\n",
      "125         11                <NA>         NA\n",
      "126         29                <NA>         NA\n",
      "127          8                <NA>         NA\n",
      "128         33                <NA>         NA\n",
      "129         33                <NA>         NA\n",
      "130         41                <NA>         NA\n",
      "131         29                <NA>         NA\n",
      "132         19                <NA>         NA\n",
      "133         33                <NA>         NA\n",
      "134         36                <NA>         NA\n",
      "135         43                <NA>         NA\n",
      "136         26                <NA>         NA\n",
      "137          7                <NA>         NA\n",
      "138         32                <NA>         NA\n",
      "139         30                <NA>         NA\n",
      "140         35                <NA>         NA\n",
      "141         45                <NA>         NA\n",
      "142          3                <NA>         NA\n",
      "143          6                <NA>         NA\n",
      "144         15                <NA>         NA\n",
      "145         24                <NA>         NA\n",
      "146         35                <NA>         NA\n",
      "147         30                <NA>         NA\n",
      "148         24                <NA>         NA\n",
      "149         35                <NA>         NA\n",
      "150         41                <NA>         NA\n"
     ]
    }
   ],
   "source": [
    "# Task 5.2: Categorize by Recency\n",
    "# TODO: Create a new column 'recency_category' using case_when():\n",
    "#   - \"Recent\" if days_since <= 30\n",
    "#   - \"Moderate\" if days_since <= 90\n",
    "#   - \"At Risk\" if days_since > 90\n",
    "\n",
    "transactions_clean <- transactions_clean %>%\n",
    "  mutate(\n",
    "    # Your code here:\n",
    "    recency_category = case_when(\n",
    "      days_since <= 30 ~ \"Recent\",\n",
    "      days_since <= 90 ~ \"Moderate\",\n",
    "      TRUE ~ \"At Risk\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "# Display distribution\n",
    "cat(\"Recency Category Distribution:\\n\")\n",
    "table(transactions_clean$recency_category) %>% print()\n",
    "\n",
    "# Show at-risk customers\n",
    "cat(\"\\nAt-Risk Customers (>90 days):\\n\")\n",
    "transactions_clean %>%\n",
    "  filter(recency_category == \"At Risk\") %>%\n",
    "  select(CustomerID, date_parsed, days_since) %>%\n",
    "  arrange(desc(days_since)) %>%\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part6_intro",
   "metadata": {},
   "source": [
    "## Part 6: Combined String and Date Operations\n",
    "\n",
    "**Business Context:** Create personalized customer outreach messages based on purchase recency.\n",
    "\n",
    "**Your Tasks:**\n",
    "1. Extract first names from customer names\n",
    "2. Create personalized messages based on recency\n",
    "3. Analyze transaction patterns by weekday\n",
    "4. Identify best customers (recent + high value)\n",
    "\n",
    "**Key Functions:** Combine `str_extract()`, date calculations, `case_when()`, `group_by()`, `summarize()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "extract_names",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personalized Customer Messages:\n",
      "   CustomerID first_name days_since\n",
      "1          26         26        556\n",
      "2          21         21        577\n",
      "3          12         12        577\n",
      "4           6          6        572\n",
      "5          32         32        572\n",
      "6          27         27        572\n",
      "7          31         31        572\n",
      "8          30         30        577\n",
      "9          31         31        567\n",
      "10         13         13        556\n",
      "                                         personalized_message\n",
      "1  Hi 26 , it's been a while! Here's a special offer for you.\n",
      "2  Hi 21 , it's been a while! Here's a special offer for you.\n",
      "3  Hi 12 , it's been a while! Here's a special offer for you.\n",
      "4   Hi 6 , it's been a while! Here's a special offer for you.\n",
      "5  Hi 32 , it's been a while! Here's a special offer for you.\n",
      "6  Hi 27 , it's been a while! Here's a special offer for you.\n",
      "7  Hi 31 , it's been a while! Here's a special offer for you.\n",
      "8  Hi 30 , it's been a while! Here's a special offer for you.\n",
      "9  Hi 31 , it's been a while! Here's a special offer for you.\n",
      "10 Hi 13 , it's been a while! Here's a special offer for you.\n"
     ]
    }
   ],
   "source": [
    "# Task 6.1: Extract First Names and Create Personalized Messages\n",
    "# TODO: Create two new columns:\n",
    "#   - first_name: Extract first name from customer_name (everything before first space)\n",
    "#   - personalized_message: Create message based on recency_category\n",
    "#     * Recent: \"Hi [name]! Thanks for your recent purchase!\"\n",
    "#     * Moderate: \"Hi [name], we miss you! Check out our new products.\"\n",
    "#     * At Risk: \"Hi [name], it's been a while! Here's a special offer for you.\"\n",
    "# Hint: Use str_extract() with pattern \"^\\\\\\\\w+\" for first name\n",
    "# Hint: Use paste() to combine strings in case_when()\n",
    "\n",
    "customer_outreach <- transactions_clean %>%\n",
    "  mutate(\n",
    "    # Your code here:\n",
    "    first_name = str_extract(CustomerID, \"\\\\d+\"),\n",
    "    personalized_message = case_when(\n",
    "    days_since <= 30 ~ paste(\"Hi\", CustomerID, \"! Thanks for your recent purchase!\"),\n",
    "    days_since <= 60 ~ paste(\"Hi\", CustomerID, \", we miss you! Check out our new products.\"),\n",
    "    TRUE ~ paste(\"Hi\", CustomerID, \", it's been a while! Here's a special offer for you.\")))\n",
    "  \n",
    "\n",
    "# Display personalized messages\n",
    "cat(\"Personalized Customer Messages:\\n\")\n",
    "customer_outreach %>%\n",
    "  select(CustomerID, first_name, days_since, personalized_message) %>%\n",
    "  head(10) %>%\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "weekday_analysis",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction Patterns by Weekday:\n",
      "\u001b[90m# A tibble: 4 × 4\u001b[39m\n",
      "  trans_weekday transaction_count total_amount avg_amount\n",
      "  \u001b[3m\u001b[90m<ord>\u001b[39m\u001b[23m                     \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m        \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m      \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m\n",
      "\u001b[90m1\u001b[39m Friday                       55       \u001b[4m1\u001b[24m\u001b[4m4\u001b[24m789.       269.\n",
      "\u001b[90m2\u001b[39m Wednesday                    34        \u001b[4m7\u001b[24m578.       223.\n",
      "\u001b[90m3\u001b[39m \u001b[31mNA\u001b[39m                           33        \u001b[4m7\u001b[24m569.       229.\n",
      "\u001b[90m4\u001b[39m Monday                       28        \u001b[4m7\u001b[24m798.       278.\n",
      "\n",
      "🔥 Busiest day: Friday \n"
     ]
    }
   ],
   "source": [
    "# Task 6.2: Analyze Transaction Patterns by Weekday\n",
    "# TODO: Group by trans_weekday and calculate:\n",
    "#   - transaction_count: number of transactions\n",
    "#   - total_amount: sum of amount (if available)\n",
    "#   - avg_amount: average amount per transaction\n",
    "# TODO: Arrange by transaction_count descending\n",
    "\n",
    "weekday_patterns <- transactions_clean %>%\n",
    "  # Your code here:\n",
    "  group_by(trans_weekday) %>%\n",
    "  summarize(\n",
    "  transaction_count = n(),\n",
    "  total_amount = sum(Amount),\n",
    "  avg_amount = round(mean(Amount), 2),\n",
    "  .groups = 'drop' ) %>%\n",
    "  arrange(desc(transaction_count))\n",
    "\n",
    "\n",
    "# Display results\n",
    "cat(\"Transaction Patterns by Weekday:\\n\")\n",
    "print(weekday_patterns)\n",
    "\n",
    "# Identify busiest day\n",
    "busiest_day <- weekday_patterns$trans_weekday[1]\n",
    "cat(\"\\n🔥 Busiest day:\", as.character(busiest_day), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "monthly_analysis",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monthly Transaction Patterns:\n",
      "\u001b[90m# A tibble: 3 × 3\u001b[39m\n",
      "  trans_month_name transaction_count unique_customers\n",
      "  \u001b[3m\u001b[90m<ord>\u001b[39m\u001b[23m                        \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m            \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m\n",
      "\u001b[90m1\u001b[39m March                           89               37\n",
      "\u001b[90m2\u001b[39m April                           28               20\n",
      "\u001b[90m3\u001b[39m \u001b[31mNA\u001b[39m                              33               23\n"
     ]
    }
   ],
   "source": [
    "# Task 6.3: Monthly Transaction Analysis\n",
    "# TODO: Group by trans_month_name and calculate:\n",
    "#   - transaction_count\n",
    "#   - unique_customers: use n_distinct(customer_name)\n",
    "# TODO: Arrange by trans_month (to show chronological order)\n",
    "\n",
    "monthly_patterns <- transactions_clean %>%\n",
    "  # Your code here:\n",
    "  group_by(trans_month_name) %>%\n",
    "  summarize(\n",
    "    transaction_count = n(),\n",
    "    unique_customers = n_distinct(CustomerID),\n",
    "    .groups = 'drop'\n",
    "\n",
    "  ) \n",
    "\n",
    "# Display results\n",
    "cat(\"Monthly Transaction Patterns:\\n\")\n",
    "print(monthly_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part7_intro",
   "metadata": {},
   "source": [
    "## Part 7: Business Intelligence Summary\n",
    "\n",
    "**Business Context:** Create an executive summary that combines all your analyses into actionable insights.\n",
    "\n",
    "**Your Tasks:**\n",
    "1. Calculate key metrics across all datasets\n",
    "2. Identify top products and categories\n",
    "3. Summarize customer sentiment\n",
    "4. Provide data-driven recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "business_summary",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n",
      "         BUSINESS INTELLIGENCE SUMMARY\n",
      "= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n",
      "\n",
      "📦 PRODUCT ANALYSIS\n",
      "─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ \n",
      "total products: 75 \n",
      "wireless_products: 17 \n",
      "Premium products: 13 \n",
      "Most common category: Tv \n",
      "\n",
      "💬 CUSTOMER SENTIMENT\n",
      "─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ \n",
      "Total feedback entries: 100 \n",
      "Average sentiment score: 0.18 \n",
      "Percentage of positive reviews: 30 %\n",
      "Percentage of negative reviews: 20 %\n",
      "\n",
      "📊 TRANSACTION PATTERNS\n",
      "─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ \n",
      "Total transactions: 150 \n",
      "Date range (earliest to latest): 19797 - 19818 \n",
      "Busiest weekday: Friday \n",
      "Percentage of weekend transactions: 0 %\n",
      "\n",
      "👥 CUSTOMER RECENCY\n",
      "─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ \n",
      "Number of at-risk customers\n",
      "At Risk \n",
      "    150 \n",
      "Percentage needing re-engagement: 100 %\n"
     ]
    }
   ],
   "source": [
    "# Task 7.1: Create Business Intelligence Dashboard\n",
    "\n",
    "cat(\"\\n\", rep(\"=\", 60), \"\\n\")\n",
    "cat(\"         BUSINESS INTELLIGENCE SUMMARY\\n\")\n",
    "cat(rep(\"=\", 60), \"\\n\\n\")\n",
    "\n",
    "# Product Analysis\n",
    "cat(\"📦 PRODUCT ANALYSIS\\n\")\n",
    "cat(rep(\"─\", 30), \"\\n\")\n",
    "# TODO: Calculate and display:\n",
    "\n",
    "\n",
    "#   - Total number of products\n",
    "cat(\"total products:\", n_distinct(products_clean$ProductID), \"\\n\")\n",
    "\n",
    "\n",
    "#   - Number of wireless products\n",
    "cat(\"wireless_products:\", sum(products_clean$is_wireless), \"\\n\")\n",
    "\n",
    "\n",
    "#   - Number of premium products\n",
    "cat(\"Premium products:\", sum(products_clean$is_premium), \"\\n\")\n",
    "#   - Most common category\n",
    "common_category <-products_clean$category_clean [1]\n",
    "cat(\"Most common category:\", (common_category), \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Customer Sentiment\n",
    "cat(\"\\n💬 CUSTOMER SENTIMENT\\n\")\n",
    "cat(rep(\"─\", 30), \"\\n\")\n",
    "# TODO: Calculate and display:\n",
    "#   - Total feedback entries\n",
    "cat(\"Total feedback entries:\", nrow(feedback_clean), \"\\n\")\n",
    "#   - Average sentiment score\n",
    "cat(\"Average sentiment score:\", mean(feedback_clean$sentiment_score), \"\\n\")\n",
    "#   - Percentage of positive reviews\n",
    "cat(\"Percentage of positive reviews:\", round(sum(feedback_clean$sentiment_score > 0) / nrow(feedback_clean) * 100, 1), \"%\\n\")\n",
    "#   - Percentage of negative reviews\n",
    "cat(\"Percentage of negative reviews:\", round(sum(feedback_clean$sentiment_score < 0) / nrow(feedback_clean) * 100, 1), \"%\\n\")\n",
    "\n",
    "\n",
    "# Transaction Patterns\n",
    "cat(\"\\n📊 TRANSACTION PATTERNS\\n\")\n",
    "cat(rep(\"─\", 30), \"\\n\")\n",
    "# TODO: Calculate and display:\n",
    "#   - Total transactions\n",
    "cat(\"Total transactions:\", nrow(transactions_clean), \"\\n\")\n",
    "#   - Date range (earliest to latest)\n",
    "cat(\"Date range (earliest to latest):\", min(na.omit(as.Date(transactions_clean$date_parsed))), \n",
    "\"-\", max(na.omit(as.Date(transactions_clean$date_parsed))), \"\\n\")\n",
    "#   - Busiest weekday\n",
    "cat(\"Busiest weekday:\", as.character(busiest_day), \"\\n\")\n",
    "#   - Weekend transaction percentage\n",
    "cat(\"Percentage of weekend transactions:\", sum(transactions_clean$is_weekend)/ nrow(transactions_clean) *100, \"%\\n\")\n",
    "\n",
    "# Customer Recency\n",
    "cat(\"\\n👥 CUSTOMER RECENCY\\n\")\n",
    "cat(rep(\"─\", 30), \"\\n\")\n",
    "# TODO: Calculate and display:\n",
    "#   - Number of recent customers (< 30 days)\n",
    "#   - Number of at-risk customers (> 90 days)\n",
    "cat(\"Number of at-risk customers\")\n",
    "table(transactions_clean$recency_category) %>% print()\n",
    "\n",
    "#   - Percentage needing re-engagement\n",
    "cat(\"Percentage needing re-engagement:\", (sum(transactions_clean$recency_category == \"At Risk\") / nrow(transactions_clean) * 100), \"%\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "top_products",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90m# A tibble: 5 × 2\u001b[39m\n",
      "  category_clean products\n",
      "  \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m             \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m\n",
      "\u001b[90m1\u001b[39m Electronics          21\n",
      "\u001b[90m2\u001b[39m Computers            15\n",
      "\u001b[90m3\u001b[39m Audio                14\n",
      "\u001b[90m4\u001b[39m Tv                   14\n",
      "\u001b[90m5\u001b[39m Shoes                11\n"
     ]
    }
   ],
   "source": [
    "# Task 7.2: Identify Top Products by Category\n",
    "# TODO: Group products by category_clean and count products in each\n",
    "# TODO: Arrange by count descending\n",
    "# TODO: Display top 5 categories\n",
    "\n",
    "top_categories <- products_clean %>%\n",
    "  # Your code here:\n",
    "  group_by(category_clean) %>%  # TODO: Group by Region and Product_Category\n",
    "  summarize(\n",
    "    # TODO: Calculate total_revenue, transaction_count, avg_profit_margin\n",
    "    products = n(),\n",
    "\n",
    "    .groups = 'drop'\n",
    "  ) %>%\n",
    "  arrange(desc(products)) \n",
    "\n",
    "\n",
    "\n",
    "print(top_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part8_intro",
   "metadata": {},
   "source": [
    "## Part 8: Reflection Questions\n",
    "\n",
    "Answer the following questions based on your analysis. Write your answers in the markdown cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflection1",
   "metadata": {},
   "source": [
    "### Question 8.1: Data Quality Impact\n",
    "\n",
    "**How did cleaning the text data (removing spaces, standardizing case) improve your ability to analyze the data? Provide specific examples from your homework.**\n",
    "\n",
    "Your answer here:\n",
    "\n",
    "In customer_feedback.csv, the feedback column is heavily varied as each row represents a different customers opinion. The program needs this data to be standardized in order to turn \n",
    "it into positive and negative categories. People text in different ways, so love could be spelled like \"LOVEE\", \"Luv\", or \"Love\" R sees all of these as completely different values. \n",
    "    positive_words = str_count(feedback_clean, \"great|excellent|love|amazing\"),\n",
    "    negative_words = str_count(feedback_clean, \"bad|terrible|hate|awful\"),\n",
    "Changing the feedback column to all lowercase using str_to_lower() allowed us to use keywords to identify which reviews were positive and negative. We later made calculations like percentage of positive and negative reviews. Cleaning the text allows a business to manipulate text data and make calculations that reveal a products performance. This also helps with accuracy and saves a company time as the process of someone manually looking through reviews and trying to do the analysis in their head would not be efficient and could produce inaccurate insights.\n",
    "A list of products like \"Apple iPhone 14 Pro - 128GB - Space Black, DELL XPS 13 Laptop - Intel i7 - 16GB RAM, LG 55\" 4K Smart TV - OLED Display\" All the cases are different making it a list thats hard to read and also difficult to remember how to type. Cleaning the data in this way is helpful as most programs need everything to be standardized before analysis can happen or else all the differing values wont be picked up and read as the same thing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflection2",
   "metadata": {},
   "source": [
    "### Question 8.2: Pattern Detection Value\n",
    "\n",
    "**What business insights did you gain from detecting patterns in product names (wireless, premium, gaming)? How could a business use this information?**\n",
    "\n",
    "Your answer here:\n",
    "\n",
    "Detecting patterns in the product names can help gain insight on what product features are being purchased the most. \n",
    "It could also help find out what titles or product feature keywords are performing well. How a product is labeled can influence how a customer feels about it. A company that sell food wants to know what keywords perform well? They could analyze their top sales to identify top keywords like \"Gluten free\", \"Vegan\", \"Kosher\" etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflection3",
   "metadata": {},
   "source": [
    "### Question 8.3: Date Analysis Importance\n",
    "\n",
    "**Why is analyzing transaction dates by weekday and month important for business operations? Provide at least three specific business applications.**\n",
    "\n",
    "Your answer here:\n",
    "\n",
    "1. Knowing when products are most purchased each month would allow a business to stock up on only whats necessary, which can help with saving and preventing overstocking.\n",
    "2. A business that is aware of its most busiest times is able to schedule their staff so that more people are working on the days, hours, months where its needed the most. This is why things like seasonal hires during the holidays for a grocery store exists.\n",
    "3. Sales promotions can be ran to increase sales during a season where they are typically low, making even more profit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflection4",
   "metadata": {},
   "source": [
    "### Question 8.4: Customer Recency Strategy\n",
    "\n",
    "**Based on your recency analysis, what specific actions would you recommend for customers in each category (Recent, Moderate, At Risk)? How would you prioritize these actions?**\n",
    "\n",
    "Your answer here:\n",
    "With recent customers, it may be important to keep them engaged and capture feedback. Sending them a promotional email that highlights exiting new upcoming product launches, or an email that gives them an opportunity to rate their experience to stay on top of anything that could cause that customer to find a replacement. If their most recent purchase was not great, good customer service to solve the problem and providing a deal for their next purchase as an apology can help keep them as a customer.\n",
    "\n",
    "An at risk or moderate customer that hasn't purchased for a long time has likely found a replacement. If its for a subscription service, I would suggest sending them a deal that includes a time frame. For example, \"$X.XX for 3 months of music\" Not only could this be a better deal than the current price of their replacement, the deal requires them to come back as a customer for an expended period of time, potentially making them enjoy the service for longer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflection5",
   "metadata": {},
   "source": [
    "### Question 8.5: Sentiment Analysis Application\n",
    "\n",
    "**How could the sentiment analysis you performed be used to improve products or customer service? What are the limitations of this simple sentiment analysis approach?**\n",
    "\n",
    "Your answer here:\n",
    "\n",
    "With the sentiment analysis we did in this assignment, an online retailer would be able to see a list of their products along with the percentages of positive and negative reviews for each of them. They can sort this to see their most negatively rated products and figure out how to improve them, possibly by comparing it to their products with the highest ratings.\n",
    "\n",
    "limitations: an example of limitations would be when we sorted positive and negative reviews using simple keywords. This would be incapable of processing critical reviews that contain positive or passive words and phrases. For example, a reviewer that says \"I don't love the way___\" would be picked up as a positive review using this method because it contains the word \"love\", when this review is actually a critical one because \"I don't love\" is just a passive way of saying \"I don't like\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflection6",
   "metadata": {},
   "source": [
    "### Question 8.6: Real-World Application\n",
    "\n",
    "**Describe a real business scenario where you would need to combine string manipulation and date analysis (like you did in this homework). What insights would you be trying to discover?**\n",
    "\n",
    "Your answer here:\n",
    "\n",
    "An international retailer is trying to produce insights on things like top performing country/region for each month. If the data is being recorded differently, the dates and times could be formatted all kinds of ways for each region, like military time for Japan, and a 12hr clock in The US. The dates would have to be parsed or changed to be formatted in the same way in order to begin sorting something by the date or time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary and Submission\n",
    "\n",
    "### What You've Accomplished\n",
    "\n",
    "In this homework, you've successfully:\n",
    "- ✅ Cleaned and standardized messy text data using `stringr` functions\n",
    "- ✅ Detected patterns and extracted information from text\n",
    "- ✅ Parsed dates and extracted temporal components using `lubridate`\n",
    "- ✅ Calculated customer recency for segmentation\n",
    "- ✅ Analyzed transaction patterns by time periods\n",
    "- ✅ Combined string and date operations for business insights\n",
    "- ✅ Created personalized customer communications\n",
    "- ✅ Generated executive-ready business intelligence summaries\n",
    "\n",
    "### Key Skills Mastered\n",
    "\n",
    "**String Manipulation:**\n",
    "- `str_trim()`, `str_squish()` - Whitespace handling\n",
    "- `str_to_lower()`, `str_to_upper()`, `str_to_title()` - Case conversion\n",
    "- `str_detect()` - Pattern detection\n",
    "- `str_extract()` - Information extraction\n",
    "- `str_count()` - Pattern counting\n",
    "\n",
    "**Date/Time Operations:**\n",
    "- `ymd()`, `mdy()`, `dmy()` - Date parsing\n",
    "- `year()`, `month()`, `day()`, `wday()` - Component extraction\n",
    "- `quarter()` - Period extraction\n",
    "- `today()` - Current date\n",
    "- Date arithmetic - Calculating differences\n",
    "\n",
    "**Business Applications:**\n",
    "- Data cleaning and standardization\n",
    "- Customer segmentation by recency\n",
    "- Sentiment analysis\n",
    "- Pattern identification\n",
    "- Temporal trend analysis\n",
    "- Personalized communication\n",
    "\n",
    "### Submission Checklist\n",
    "\n",
    "Before submitting, ensure you have:\n",
    "- [ ] Entered your name, student ID, and date at the top\n",
    "- [ ] Completed all code tasks (Parts 1-7)\n",
    "- [ ] Run all cells successfully without errors\n",
    "- [ ] Answered all reflection questions (Part 8)\n",
    "- [ ] Used proper commenting in your code\n",
    "- [ ] Used the pipe operator (`%>%`) where appropriate\n",
    "- [ ] Verified your results make business sense\n",
    "- [ ] Checked for any remaining TODO comments\n",
    "\n",
    "### Grading Criteria\n",
    "\n",
    "Your homework will be evaluated on:\n",
    "- **Code Correctness (40%)**: All tasks completed correctly\n",
    "- **Code Quality (20%)**: Clean, well-commented, efficient code\n",
    "- **Business Understanding (20%)**: Demonstrates understanding of business applications\n",
    "- **Reflection Questions (15%)**: Thoughtful, complete answers\n",
    "- **Presentation (5%)**: Professional formatting and organization\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In Lesson 8, you'll learn:\n",
    "- Advanced data wrangling with complex pipelines\n",
    "- Sophisticated conditional logic with `case_when()`\n",
    "- Data validation and quality checks\n",
    "- Creating reproducible analysis workflows\n",
    "- Professional best practices for business analytics\n",
    "\n",
    "**Great work on completing this assignment! 🎉**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
