{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# MIDTERM EXAM: Comprehensive R Data Wrangling Assessment\n",
    "\n",
    "**Student Name:** Alejandro De Santiago Palomares Salinas\n",
    "\n",
    "**Student ID:** aeb923\n",
    "\n",
    "**Date:** 10/19/2025\n",
    "\n",
    "**Time Limit:** 4 hours\n",
    "\n",
    "---\n",
    "\n",
    "## Exam Overview\n",
    "\n",
    "This comprehensive midterm exam assesses your mastery of ALL R data wrangling skills covered in Lessons 1-8:\n",
    "\n",
    "- **Lesson 1:** R Basics and Data Import\n",
    "- **Lesson 2:** Data Cleaning (Missing Values & Outliers)\n",
    "- **Lesson 3:** Data Transformation Part 1 (select, filter, arrange)\n",
    "- **Lesson 4:** Data Transformation Part 2 (mutate, summarize, group_by)\n",
    "- **Lesson 5:** Data Reshaping (pivot_longer, pivot_wider)\n",
    "- **Lesson 6:** Combining Datasets (joins)\n",
    "- **Lesson 7:** String Manipulation & Date/Time\n",
    "- **Lesson 8:** Advanced Wrangling & Best Practices\n",
    "\n",
    "## Business Scenario\n",
    "\n",
    "You are a data analyst for a retail company. The executive team needs a comprehensive analysis of:\n",
    "- Sales performance across products and regions\n",
    "- Customer behavior and segmentation\n",
    "- Data quality issues and recommendations\n",
    "- Strategic insights for business growth\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. **Set your working directory** to where your data files are located\n",
    "2. Complete ALL tasks in order\n",
    "3. Write code in the TODO sections\n",
    "4. Use the pipe operator (%>%) to chain operations\n",
    "5. Add comments explaining your logic\n",
    "6. Run all cells to verify your code works\n",
    "7. Answer all reflection questions\n",
    "\n",
    "## Grading\n",
    "\n",
    "- **Code Correctness (40%)**: All tasks completed correctly\n",
    "- **Code Quality (20%)**: Clean, well-commented code\n",
    "- **Business Understanding (20%)**: Demonstrates understanding of context\n",
    "- **Analysis & Insights (15%)**: Meaningful insights and recommendations\n",
    "- **Reflection Questions (5%)**: Thoughtful answers\n",
    "\n",
    "## Academic Integrity\n",
    "\n",
    "This is an individual exam. You may use:\n",
    "- Course notes and lesson materials\n",
    "- R documentation and help files\n",
    "- Your previous homework assignments\n",
    "\n",
    "You may NOT:\n",
    "- Collaborate with other students\n",
    "- Use AI assistants or online forums\n",
    "- Share code or solutions\n",
    "\n",
    "---\n",
    "\n",
    "**Good luck! ðŸŽ“**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1_intro",
   "metadata": {},
   "source": [
    "## Part 1: R Basics and Data Import (Lesson 1)\n",
    "\n",
    "**Skills Assessed:** Variables, data types, data import, working directory\n",
    "\n",
    "**Your Tasks:**\n",
    "1. Set working directory\n",
    "2. Load required packages\n",
    "3. Import multiple datasets\n",
    "4. Examine data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "'/workspaces/assignment-2-version3-Aledesan-utsa/data'"
      ],
      "text/latex": [
       "'/workspaces/assignment-2-version3-Aledesan-utsa/data'"
      ],
      "text/markdown": [
       "'/workspaces/assignment-2-version3-Aledesan-utsa/data'"
      ],
      "text/plain": [
       "[1] \"/workspaces/assignment-2-version3-Aledesan-utsa/data\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /workspaces/assignment-2-version3-Aledesan-utsa/data \n"
     ]
    }
   ],
   "source": [
    "# Task 1.1: Set Working Directory\n",
    "# TODO: Set your working directory to where your data files are located\n",
    "# IMPORTANT: Students must set their own path!\n",
    "# Example: setwd(\"/Users/yourname/GitHub/ai-homework-grader-clean/data\")\n",
    "setwd(\"/workspaces/assignment-2-version3-Aledesan-utsa/data\")\n",
    "getwd()\n",
    "# Verify working directory\n",
    "cat(\"Current working directory:\", getwd(), \"\\n\")\n",
    "\n",
    "#no comment needed, set up of working directory and data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_packages",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Packages loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Task 1.2: Load Required Packages\n",
    "# TODO: Load tidyverse (includes dplyr, tidyr, stringr, ggplot2)\n",
    "library(tidyverse)\n",
    "\n",
    "# TODO: Load lubridate for date operations\n",
    "library(lubridate)\n",
    "\n",
    "cat(\"âœ… Packages loaded successfully!\\n\")\n",
    "\n",
    "#no comment needed, loading of required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import_data",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRows: \u001b[22m\u001b[34m300\u001b[39m \u001b[1mColumns: \u001b[22m\u001b[34m8\u001b[39m\n",
      "\u001b[36mâ”€â”€\u001b[39m \u001b[1mColumn specification\u001b[22m \u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[39m\n",
      "\u001b[1mDelimiter:\u001b[22m \",\"\n",
      "\u001b[31mchr\u001b[39m  (3): Sales_Rep_Name, Region, Product_Category\n",
      "\u001b[32mdbl\u001b[39m  (4): TransactionID, Revenue, Cost, Units_Sold\n",
      "\u001b[34mdate\u001b[39m (1): Sale_Date\n",
      "\n",
      "\u001b[36mâ„¹\u001b[39m Use `spec()` to retrieve the full column specification for this data.\n",
      "\u001b[36mâ„¹\u001b[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n",
      "\u001b[1mRows: \u001b[22m\u001b[34m100\u001b[39m \u001b[1mColumns: \u001b[22m\u001b[34m5\u001b[39m\n",
      "\u001b[36mâ”€â”€\u001b[39m \u001b[1mColumn specification\u001b[22m \u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[39m\n",
      "\u001b[1mDelimiter:\u001b[22m \",\"\n",
      "\u001b[31mchr\u001b[39m  (3): Name, Email, City\n",
      "\u001b[32mdbl\u001b[39m  (1): CustomerID\n",
      "\u001b[34mdate\u001b[39m (1): Registration_Date\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[36mâ„¹\u001b[39m Use `spec()` to retrieve the full column specification for this data.\n",
      "\u001b[36mâ„¹\u001b[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n",
      "\u001b[1mRows: \u001b[22m\u001b[34m50\u001b[39m \u001b[1mColumns: \u001b[22m\u001b[34m4\u001b[39m\n",
      "\u001b[36mâ”€â”€\u001b[39m \u001b[1mColumn specification\u001b[22m \u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[39m\n",
      "\u001b[1mDelimiter:\u001b[22m \",\"\n",
      "\u001b[31mchr\u001b[39m (2): Product_Name, Category\n",
      "\u001b[32mdbl\u001b[39m (2): ProductID, Supplier_ID\n",
      "\n",
      "\u001b[36mâ„¹\u001b[39m Use `spec()` to retrieve the full column specification for this data.\n",
      "\u001b[36mâ„¹\u001b[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n",
      "\u001b[1mRows: \u001b[22m\u001b[34m250\u001b[39m \u001b[1mColumns: \u001b[22m\u001b[34m4\u001b[39m\n",
      "\u001b[36mâ”€â”€\u001b[39m \u001b[1mColumn specification\u001b[22m \u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[39m\n",
      "\u001b[1mDelimiter:\u001b[22m \",\"\n",
      "\u001b[32mdbl\u001b[39m  (3): OrderID, CustomerID, Total_Amount\n",
      "\u001b[34mdate\u001b[39m (1): Order_Date\n",
      "\n",
      "\u001b[36mâ„¹\u001b[39m Use `spec()` to retrieve the full column specification for this data.\n",
      "\u001b[36mâ„¹\u001b[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n",
      "\u001b[1mRows: \u001b[22m\u001b[34m400\u001b[39m \u001b[1mColumns: \u001b[22m\u001b[34m4\u001b[39m\n",
      "\u001b[36mâ”€â”€\u001b[39m \u001b[1mColumn specification\u001b[22m \u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[39m\n",
      "\u001b[1mDelimiter:\u001b[22m \",\"\n",
      "\u001b[32mdbl\u001b[39m (4): OrderID, ProductID, Quantity, Unit_Price\n",
      "\n",
      "\u001b[36mâ„¹\u001b[39m Use `spec()` to retrieve the full column specification for this data.\n",
      "\u001b[36mâ„¹\u001b[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data imported successfully!\n",
      "Sales data: 300 rows\n",
      "Customers: 100 rows\n",
      "Products: 50 rows\n",
      "Orders: 250 rows\n",
      "Order items: 400 rows\n"
     ]
    }
   ],
   "source": [
    "# Task 1.3: Import Datasets\n",
    "# TODO: Import the following CSV files using read_csv():\n",
    "#   - company_sales_data.csv -> sales_data\n",
    "#   - customers.csv -> customers\n",
    "#   - products.csv -> products\n",
    "#   - orders.csv -> orders\n",
    "#   - order_items.csv -> order_items\n",
    "\n",
    "# Your code here:\n",
    "sales_data <- read_csv(\"company_sales_data.csv\")\n",
    "\n",
    "customers <- read_csv(\"customers.csv\")\n",
    "\n",
    "products <- read_csv(\"products.csv\")\n",
    "\n",
    "orders <- read_csv(\"orders.csv\")\n",
    "\n",
    "order_items <- read_csv(\"order_items.csv\")\n",
    "\n",
    "\n",
    "# Display import summary\n",
    "cat(\"âœ… Data imported successfully!\\n\")\n",
    "cat(\"Sales data:\", nrow(sales_data), \"rows\\n\")\n",
    "cat(\"Customers:\", nrow(customers), \"rows\\n\")\n",
    "cat(\"Products:\", nrow(products), \"rows\\n\")\n",
    "cat(\"Orders:\", nrow(orders), \"rows\\n\")\n",
    "cat(\"Order items:\", nrow(order_items), \"rows\\n\")\n",
    "\n",
    "#no comment needed, importing of datasets and naming them accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2_intro",
   "metadata": {},
   "source": [
    "## Part 2: Data Cleaning - Missing Values & Outliers (Lesson 2)\n",
    "\n",
    "**Skills Assessed:** Identifying NAs, handling missing data, detecting outliers\n",
    "\n",
    "**Your Tasks:**\n",
    "1. Check for missing values in sales_data\n",
    "2. Handle missing values appropriately\n",
    "3. Identify outliers in Revenue column\n",
    "4. Create a cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check_missing",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== MISSING VALUES SUMMARY ==========\n",
      "   TransactionID   Sales_Rep_Name           Region Product_Category \n",
      "               0                0                0                0 \n",
      "         Revenue             Cost       Units_Sold        Sale_Date \n",
      "               0                0                0                0 \n",
      "\n",
      "Total missing values: 0 \n"
     ]
    }
   ],
   "source": [
    "# Task 2.1: Check for Missing Values\n",
    "# TODO: Create 'missing_summary' that shows count of NAs in each column of sales_data\n",
    "\n",
    "\n",
    "missing_summary <- sapply(sales_data, function(x) sum(is.na(x)))\n",
    "\n",
    "cat(\"========== MISSING VALUES SUMMARY ==========\\n\")\n",
    "print(missing_summary)\n",
    "cat(\"\\nTotal missing values:\", sum(missing_summary), \"\\n\")\n",
    "\n",
    "#I noticed that there are no NA values in the dataset, I am not quite sure if this is meant to be like that or my data set is faulty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handle_missing",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== DATA CLEANING RESULTS ==========\n",
      "Original rows: 300 \n",
      "Cleaned rows: 300 \n",
      "Rows removed: 0 \n"
     ]
    }
   ],
   "source": [
    "# Task 2.2: Handle Missing Values\n",
    "# TODO: Create 'sales_clean' by removing rows with ANY missing values\n",
    "\n",
    "\n",
    "sales_clean <- missing_summary <- sales_data %>%\n",
    "drop_na()\n",
    "\n",
    "cat(\"========== DATA CLEANING RESULTS ==========\\n\")\n",
    "cat(\"Original rows:\", nrow(sales_data), \"\\n\")\n",
    "cat(\"Cleaned rows:\", nrow(sales_clean), \"\\n\")\n",
    "cat(\"Rows removed:\", nrow(sales_data) - nrow(sales_clean), \"\\n\")\n",
    "\n",
    "#still tried to remove NAs even though there were none, again I am not sure if this is meant to be like that or my data set is faulty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detect_outliers",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== OUTLIER ANALYSIS ==========\n",
      "       Metric     Value\n",
      "1          Q1  15034.29\n",
      "2          Q3  37707.71\n",
      "3         IQR  22673.42\n",
      "4 Lower Bound -18975.84\n",
      "5 Upper Bound  71717.84\n",
      "\n",
      "Number of outliers detected: 0 \n"
     ]
    }
   ],
   "source": [
    "# Task 2.3: Detect Outliers in Revenue\n",
    "# TODO: Calculate outlier thresholds using IQR method\n",
    "#   - Calculate Q1 (25th percentile) and Q3 (75th percentile) of Revenue\n",
    "#   - Calculate IQR = Q3 - Q1\n",
    "#   - Lower bound = Q1 - 1.5 * IQR\n",
    "#   - Upper bound = Q3 + 1.5 * IQR\n",
    "# TODO: Create 'outlier_analysis' dataframe with these values\n",
    "\n",
    "Q1 <- quantile(sales_clean$Revenue, 0.25, na.rm = TRUE)\n",
    "Q3 <- quantile(sales_clean$Revenue, 0.75, na.rm = TRUE)\n",
    "IQR_value <-  Q3 - Q1\n",
    "lower_bound <- Q1 - 1.5 * IQR_value\n",
    "upper_bound <- Q3 + 1.5 * IQR_value\n",
    "\n",
    "outlier_analysis <- data.frame(\n",
    "  Metric = c(\"Q1\", \"Q3\", \"IQR\", \"Lower Bound\", \"Upper Bound\"),\n",
    "  Value = c(Q1, Q3, IQR_value, lower_bound, upper_bound)\n",
    ")\n",
    "\n",
    "cat(\"========== OUTLIER ANALYSIS ==========\\n\")\n",
    "print(outlier_analysis)\n",
    "\n",
    "# Count outliers\n",
    "outlier_count <- sum(sales_clean$Revenue < lower_bound | sales_clean$Revenue > upper_bound)\n",
    "cat(\"\\nNumber of outliers detected:\", outlier_count, \"\\n\")\n",
    "\n",
    "#I defined the key metrics for outlier detection and counted the number of outliers accordingly, even though there were none in my dataset, once again not sure if this is meant to be like that or my data set is faulty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3_intro",
   "metadata": {},
   "source": [
    "## Part 3: Data Transformation Part 1 (Lesson 3)\n",
    "\n",
    "**Skills Assessed:** select(), filter(), arrange(), pipe operator\n",
    "\n",
    "**Your Tasks:**\n",
    "1. Select specific columns\n",
    "2. Filter data by conditions\n",
    "3. Sort data\n",
    "4. Chain operations with pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "select_columns",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== SELECTED COLUMNS ==========\n",
      "Columns: Region Product_Category Revenue Units_Sold Sale_Date \n",
      "Rows: 300 \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 5 Ã— 5</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>Region</th><th scope=col>Product_Category</th><th scope=col>Revenue</th><th scope=col>Units_Sold</th><th scope=col>Sale_Date</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;date&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>Latin America</td><td>Services</td><td>20750.92</td><td>78</td><td>2023-04-24</td></tr>\n",
       "\t<tr><td>Europe       </td><td>Hardware</td><td>32359.98</td><td>13</td><td>2023-06-09</td></tr>\n",
       "\t<tr><td>Europe       </td><td>Services</td><td>39268.40</td><td>34</td><td>2023-03-25</td></tr>\n",
       "\t<tr><td>Europe       </td><td>Hardware</td><td>28865.09</td><td>90</td><td>2023-04-11</td></tr>\n",
       "\t<tr><td>Latin America</td><td>Software</td><td> 3932.36</td><td>63</td><td>2023-08-26</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 5 Ã— 5\n",
       "\\begin{tabular}{lllll}\n",
       " Region & Product\\_Category & Revenue & Units\\_Sold & Sale\\_Date\\\\\n",
       " <chr> & <chr> & <dbl> & <dbl> & <date>\\\\\n",
       "\\hline\n",
       "\t Latin America & Services & 20750.92 & 78 & 2023-04-24\\\\\n",
       "\t Europe        & Hardware & 32359.98 & 13 & 2023-06-09\\\\\n",
       "\t Europe        & Services & 39268.40 & 34 & 2023-03-25\\\\\n",
       "\t Europe        & Hardware & 28865.09 & 90 & 2023-04-11\\\\\n",
       "\t Latin America & Software &  3932.36 & 63 & 2023-08-26\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 5 Ã— 5\n",
       "\n",
       "| Region &lt;chr&gt; | Product_Category &lt;chr&gt; | Revenue &lt;dbl&gt; | Units_Sold &lt;dbl&gt; | Sale_Date &lt;date&gt; |\n",
       "|---|---|---|---|---|\n",
       "| Latin America | Services | 20750.92 | 78 | 2023-04-24 |\n",
       "| Europe        | Hardware | 32359.98 | 13 | 2023-06-09 |\n",
       "| Europe        | Services | 39268.40 | 34 | 2023-03-25 |\n",
       "| Europe        | Hardware | 28865.09 | 90 | 2023-04-11 |\n",
       "| Latin America | Software |  3932.36 | 63 | 2023-08-26 |\n",
       "\n"
      ],
      "text/plain": [
       "  Region        Product_Category Revenue  Units_Sold Sale_Date \n",
       "1 Latin America Services         20750.92 78         2023-04-24\n",
       "2 Europe        Hardware         32359.98 13         2023-06-09\n",
       "3 Europe        Services         39268.40 34         2023-03-25\n",
       "4 Europe        Hardware         28865.09 90         2023-04-11\n",
       "5 Latin America Software          3932.36 63         2023-08-26"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Task 3.1: Select Specific Columns\n",
    "# TODO: Create 'sales_summary' with only these columns from sales_clean:\n",
    "#   Region, Product_Category, Revenue, Units_Sold, Sale_Date\n",
    "\n",
    "sales_summary <- sales_clean %>%\n",
    "  select(Region, Product_Category, Revenue, Units_Sold, Sale_Date)\n",
    "  \n",
    "cat(\"========== SELECTED COLUMNS ==========\\n\")\n",
    "cat(\"Columns:\", names(sales_summary), \"\\n\")\n",
    "cat(\"Rows:\", nrow(sales_summary), \"\\n\")\n",
    "head(sales_summary, 5)\n",
    "\n",
    "#used the basic select function to select specific columns from the cleaned sales data created in the task before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filter_data",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== HIGH REVENUE SALES ==========\n",
      "Total high revenue transactions: 194 \n",
      "Total revenue from these sales: $ 6671906 \n"
     ]
    }
   ],
   "source": [
    "# Task 3.2: Filter High Revenue Sales\n",
    "# TODO: Create 'high_revenue_sales' by filtering sales_clean for Revenue > 20000\n",
    "\n",
    "\n",
    "high_revenue_sales <- sales_clean %>%\n",
    "  filter(Revenue > 20000)\n",
    "  \n",
    "cat(\"========== HIGH REVENUE SALES ==========\\n\")\n",
    "cat(\"Total high revenue transactions:\", nrow(high_revenue_sales), \"\\n\")\n",
    "cat(\"Total revenue from these sales: $\", sum(high_revenue_sales$Revenue), \"\\n\")\n",
    "\n",
    "#filtered the cleaned sales data to only include transactions where revenue was higher than 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sort_data",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== TOP 10 SALES ==========\n",
      "\u001b[90m# A tibble: 10 Ã— 4\u001b[39m\n",
      "   Region        Product_Category Revenue Units_Sold\n",
      "   \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m         \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m              \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m      \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m\n",
      "\u001b[90m 1\u001b[39m Asia Pacific  Consulting        \u001b[4m4\u001b[24m\u001b[4m9\u001b[24m956.         88\n",
      "\u001b[90m 2\u001b[39m Europe        Software          \u001b[4m4\u001b[24m\u001b[4m9\u001b[24m867.         96\n",
      "\u001b[90m 3\u001b[39m Europe        Consulting        \u001b[4m4\u001b[24m\u001b[4m9\u001b[24m857.          1\n",
      "\u001b[90m 4\u001b[39m Asia Pacific  Consulting        \u001b[4m4\u001b[24m\u001b[4m9\u001b[24m239.         72\n",
      "\u001b[90m 5\u001b[39m Asia Pacific  Hardware          \u001b[4m4\u001b[24m\u001b[4m8\u001b[24m997.         92\n",
      "\u001b[90m 6\u001b[39m North America Services          \u001b[4m4\u001b[24m\u001b[4m8\u001b[24m884.         62\n",
      "\u001b[90m 7\u001b[39m Europe        Software          \u001b[4m4\u001b[24m\u001b[4m8\u001b[24m794.         77\n",
      "\u001b[90m 8\u001b[39m North America Hardware          \u001b[4m4\u001b[24m\u001b[4m8\u001b[24m772.         16\n",
      "\u001b[90m 9\u001b[39m North America Consulting        \u001b[4m4\u001b[24m\u001b[4m8\u001b[24m748.         63\n",
      "\u001b[90m10\u001b[39m Europe        Consulting        \u001b[4m4\u001b[24m\u001b[4m8\u001b[24m572.         22\n"
     ]
    }
   ],
   "source": [
    "# Task 3.3: Sort by Revenue\n",
    "# TODO: Create 'top_sales' by arranging sales_clean by Revenue in descending order\n",
    "#       and keeping only the top 10 rows\n",
    "\n",
    "\n",
    "top_sales <- sales_clean %>%\n",
    "  arrange(desc(Revenue)) %>%\n",
    "  slice_head(n = 10)\n",
    "  \n",
    "cat(\"========== TOP 10 SALES ==========\\n\")\n",
    "print(top_sales %>% select(Region, Product_Category, Revenue, Units_Sold))\n",
    "\n",
    "#sorted the cleaned sales data by revenue in descending order and kept only the top 10 rows to display top sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chain_operations",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== REGIONAL TOP SALES ==========\n",
      "\u001b[90m# A tibble: 15 Ã— 3\u001b[39m\n",
      "   Region       Product_Category Revenue\n",
      "   \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m        \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m              \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m\n",
      "\u001b[90m 1\u001b[39m Asia Pacific Consulting        \u001b[4m4\u001b[24m\u001b[4m9\u001b[24m956.\n",
      "\u001b[90m 2\u001b[39m Asia Pacific Consulting        \u001b[4m4\u001b[24m\u001b[4m9\u001b[24m239.\n",
      "\u001b[90m 3\u001b[39m Asia Pacific Hardware          \u001b[4m4\u001b[24m\u001b[4m8\u001b[24m997.\n",
      "\u001b[90m 4\u001b[39m Asia Pacific Consulting        \u001b[4m4\u001b[24m\u001b[4m8\u001b[24m063.\n",
      "\u001b[90m 5\u001b[39m Asia Pacific Services          \u001b[4m4\u001b[24m\u001b[4m6\u001b[24m731.\n",
      "\u001b[90m 6\u001b[39m Asia Pacific Software          \u001b[4m4\u001b[24m\u001b[4m6\u001b[24m615.\n",
      "\u001b[90m 7\u001b[39m Asia Pacific Consulting        \u001b[4m4\u001b[24m\u001b[4m6\u001b[24m545.\n",
      "\u001b[90m 8\u001b[39m Asia Pacific Consulting        \u001b[4m4\u001b[24m\u001b[4m5\u001b[24m756.\n",
      "\u001b[90m 9\u001b[39m Asia Pacific Services          \u001b[4m4\u001b[24m\u001b[4m5\u001b[24m298.\n",
      "\u001b[90m10\u001b[39m Asia Pacific Services          \u001b[4m4\u001b[24m\u001b[4m4\u001b[24m624.\n",
      "\u001b[90m11\u001b[39m Asia Pacific Consulting        \u001b[4m4\u001b[24m\u001b[4m4\u001b[24m455.\n",
      "\u001b[90m12\u001b[39m Asia Pacific Consulting        \u001b[4m4\u001b[24m\u001b[4m4\u001b[24m364.\n",
      "\u001b[90m13\u001b[39m Asia Pacific Software          \u001b[4m4\u001b[24m\u001b[4m3\u001b[24m221.\n",
      "\u001b[90m14\u001b[39m Asia Pacific Software          \u001b[4m4\u001b[24m\u001b[4m2\u001b[24m063.\n",
      "\u001b[90m15\u001b[39m Asia Pacific Software          \u001b[4m4\u001b[24m\u001b[4m1\u001b[24m412.\n"
     ]
    }
   ],
   "source": [
    "# Task 3.4: Chain Multiple Operations\n",
    "# TODO: Create 'regional_top_sales' by:\n",
    "#   1. Filtering for Revenue > 15000\n",
    "#   2. Selecting: Region, Product_Category, Revenue\n",
    "#   3. Arranging by Region (ascending) then Revenue (descending)\n",
    "#   4. Keeping top 15 rows\n",
    "# Use the pipe operator to chain all operations\n",
    "\n",
    "regional_top_sales <- sales_clean %>%\n",
    "  filter(Revenue > 15000) %>%\n",
    "  select(Region, Product_Category, Revenue) %>%\n",
    "  arrange(Region, desc(Revenue)) %>%\n",
    "  slice_head(n = 15)\n",
    "  \n",
    "cat(\"========== REGIONAL TOP SALES ==========\\n\")\n",
    "print(regional_top_sales)\n",
    "\n",
    "#chained multiple dplyr operations using the pipe operator to filter, select, arrange, and slice the sales data accordingly for tidy display and easier analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part4_intro",
   "metadata": {},
   "source": [
    "## Part 4: Data Transformation Part 2 (Lesson 4)\n",
    "\n",
    "**Skills Assessed:** mutate(), summarize(), group_by()\n",
    "\n",
    "**Your Tasks:**\n",
    "1. Create calculated columns with mutate()\n",
    "2. Calculate summary statistics\n",
    "3. Perform grouped analysis\n",
    "4. Generate business metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mutate_columns",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== ENHANCED SALES DATA ==========\n",
      "New columns added: revenue_per_unit, high_value\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 5 Ã— 4</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>Revenue</th><th scope=col>Units_Sold</th><th scope=col>revenue_per_unit</th><th scope=col>high_value</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>20750.92</td><td>78</td><td> 266.03744</td><td>Yes</td></tr>\n",
       "\t<tr><td>32359.98</td><td>13</td><td>2489.22923</td><td>Yes</td></tr>\n",
       "\t<tr><td>39268.40</td><td>34</td><td>1154.95294</td><td>Yes</td></tr>\n",
       "\t<tr><td>28865.09</td><td>90</td><td> 320.72322</td><td>Yes</td></tr>\n",
       "\t<tr><td> 3932.36</td><td>63</td><td>  62.41841</td><td>No </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 5 Ã— 4\n",
       "\\begin{tabular}{llll}\n",
       " Revenue & Units\\_Sold & revenue\\_per\\_unit & high\\_value\\\\\n",
       " <dbl> & <dbl> & <dbl> & <chr>\\\\\n",
       "\\hline\n",
       "\t 20750.92 & 78 &  266.03744 & Yes\\\\\n",
       "\t 32359.98 & 13 & 2489.22923 & Yes\\\\\n",
       "\t 39268.40 & 34 & 1154.95294 & Yes\\\\\n",
       "\t 28865.09 & 90 &  320.72322 & Yes\\\\\n",
       "\t  3932.36 & 63 &   62.41841 & No \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 5 Ã— 4\n",
       "\n",
       "| Revenue &lt;dbl&gt; | Units_Sold &lt;dbl&gt; | revenue_per_unit &lt;dbl&gt; | high_value &lt;chr&gt; |\n",
       "|---|---|---|---|\n",
       "| 20750.92 | 78 |  266.03744 | Yes |\n",
       "| 32359.98 | 13 | 2489.22923 | Yes |\n",
       "| 39268.40 | 34 | 1154.95294 | Yes |\n",
       "| 28865.09 | 90 |  320.72322 | Yes |\n",
       "|  3932.36 | 63 |   62.41841 | No  |\n",
       "\n"
      ],
      "text/plain": [
       "  Revenue  Units_Sold revenue_per_unit high_value\n",
       "1 20750.92 78          266.03744       Yes       \n",
       "2 32359.98 13         2489.22923       Yes       \n",
       "3 39268.40 34         1154.95294       Yes       \n",
       "4 28865.09 90          320.72322       Yes       \n",
       "5  3932.36 63           62.41841       No        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Task 4.1: Create Calculated Columns\n",
    "# TODO: Add these new columns to sales_clean using mutate():\n",
    "#   - revenue_per_unit: Revenue / Units_Sold\n",
    "#   - high_value: \"Yes\" if Revenue > 20000, else \"No\"\n",
    "# Store result in 'sales_enhanced'\n",
    "\n",
    "sales_enhanced <- sales_clean %>%\n",
    "  mutate(\n",
    "    revenue_per_unit = Revenue / Units_Sold,\n",
    "    high_value = ifelse(Revenue > 20000, \"Yes\", \"No\")\n",
    "  )\n",
    "\n",
    "cat(\"========== ENHANCED SALES DATA ==========\\n\")\n",
    "cat(\"New columns added: revenue_per_unit, high_value\\n\")\n",
    "head(sales_enhanced %>% select(Revenue, Units_Sold, revenue_per_unit, high_value), 5)\n",
    "\n",
    "#used mutate to create new calculated columns in the cleaned sales data for better analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summarize_data",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== OVERALL SUMMARY ==========\n",
      "\u001b[90m# A tibble: 1 Ã— 4\u001b[39m\n",
      "  total_revenue avg_revenue total_units transaction_count\n",
      "          \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m       \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m       \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m             \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m\n",
      "\u001b[90m1\u001b[39m      7\u001b[4m7\u001b[24m\u001b[4m7\u001b[24m\u001b[4m1\u001b[24m711.      \u001b[4m2\u001b[24m\u001b[4m5\u001b[24m906.       \u001b[4m1\u001b[24m\u001b[4m6\u001b[24m169               300\n"
     ]
    }
   ],
   "source": [
    "# Task 4.2: Calculate Overall Summary Statistics\n",
    "# TODO: Create 'overall_summary' with these metrics from sales_enhanced:\n",
    "#   - total_revenue: sum of Revenue\n",
    "#   - avg_revenue: mean of Revenue\n",
    "#   - total_units: sum of Units_Sold\n",
    "#   - transaction_count: count using n()\n",
    "\n",
    "\n",
    "overall_summary <- sales_enhanced %>%\n",
    "    summarise(\n",
    "        total_revenue = sum(Revenue, na.rm = TRUE),\n",
    "        avg_revenue = mean(Revenue, na.rm = TRUE),\n",
    "        total_units = sum(Units_Sold, na.rm = TRUE),\n",
    "        transaction_count = n()\n",
    "    )\n",
    "\n",
    "cat(\"========== OVERALL SUMMARY ==========\\n\")\n",
    "print(overall_summary)\n",
    "#used the summaerise function to calculate key overall metrics from the cleaned sales data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "group_by_region",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== REGIONAL SUMMARY ==========\n",
      "\u001b[90m# A tibble: 4 Ã— 4\u001b[39m\n",
      "  Region        total_revenue avg_revenue transaction_count\n",
      "  \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m                 \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m       \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m             \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m\n",
      "\u001b[90m1\u001b[39m Europe             2\u001b[4m2\u001b[24m\u001b[4m2\u001b[24m\u001b[4m4\u001b[24m182.      \u001b[4m2\u001b[24m\u001b[4m7\u001b[24m124.                82\n",
      "\u001b[90m2\u001b[39m Latin America      2\u001b[4m1\u001b[24m\u001b[4m1\u001b[24m\u001b[4m2\u001b[24m037.      \u001b[4m2\u001b[24m\u001b[4m5\u001b[24m446.                83\n",
      "\u001b[90m3\u001b[39m Asia Pacific       1\u001b[4m8\u001b[24m\u001b[4m0\u001b[24m\u001b[4m4\u001b[24m243.      \u001b[4m2\u001b[24m\u001b[4m6\u001b[24m929.                67\n",
      "\u001b[90m4\u001b[39m North America      1\u001b[4m6\u001b[24m\u001b[4m3\u001b[24m\u001b[4m1\u001b[24m248.      \u001b[4m2\u001b[24m\u001b[4m3\u001b[24m989.                68\n"
     ]
    }
   ],
   "source": [
    "# Task 4.3: Regional Performance Analysis\n",
    "# TODO: Create 'regional_summary' by grouping sales_enhanced by Region\n",
    "#       and calculating:\n",
    "#   - total_revenue: sum of Revenue\n",
    "#   - avg_revenue: mean of Revenue\n",
    "#   - transaction_count: count using n()\n",
    "# Then arrange by total_revenue descending\n",
    "# Hint: Use group_by() %>% summarize() %>% arrange()\n",
    "\n",
    "regional_summary <- sales_enhanced %>%\n",
    "  group_by(Region) %>%\n",
    "  summarise(\n",
    "    total_revenue = sum(Revenue, na.rm = TRUE),\n",
    "    avg_revenue = mean(Revenue, na.rm = TRUE),\n",
    "    transaction_count = n()\n",
    "  ) %>%\n",
    "  arrange(desc(total_revenue))\n",
    "\n",
    "cat(\"========== REGIONAL SUMMARY ==========\\n\")\n",
    "print(regional_summary)\n",
    "#grouped the cleaned sales data by region and calculated key metrics, then arranged by total revenue in descending order for better analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "group_by_category",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== CATEGORY SUMMARY ==========\n",
      "\u001b[90m# A tibble: 4 Ã— 4\u001b[39m\n",
      "  Product_Category total_revenue avg_revenue transaction_count\n",
      "  \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m                    \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m       \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m             \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m\n",
      "\u001b[90m1\u001b[39m Consulting            1\u001b[4m9\u001b[24m\u001b[4m7\u001b[24m\u001b[4m8\u001b[24m840.      \u001b[4m2\u001b[24m\u001b[4m6\u001b[24m037.                76\n",
      "\u001b[90m2\u001b[39m Services              1\u001b[4m9\u001b[24m\u001b[4m6\u001b[24m\u001b[4m1\u001b[24m565.      \u001b[4m2\u001b[24m\u001b[4m7\u001b[24m244.                72\n",
      "\u001b[90m3\u001b[39m Hardware              1\u001b[4m9\u001b[24m\u001b[4m5\u001b[24m\u001b[4m1\u001b[24m325.      \u001b[4m2\u001b[24m\u001b[4m6\u001b[24m730.                73\n",
      "\u001b[90m4\u001b[39m Software              1\u001b[4m8\u001b[24m\u001b[4m7\u001b[24m\u001b[4m9\u001b[24m981.      \u001b[4m2\u001b[24m\u001b[4m3\u001b[24m797.                79\n"
     ]
    }
   ],
   "source": [
    "# Task 4.4: Product Category Analysis\n",
    "# TODO: Create 'category_summary' by grouping by Product_Category\n",
    "#       and calculating the same metrics as regional_summary\n",
    "#       Then arrange by total_revenue descending\n",
    "\n",
    "category_summary <- sales_enhanced %>%\n",
    "  group_by(Product_Category) %>%\n",
    "  summarise(\n",
    "    total_revenue = sum(Revenue, na.rm = TRUE),\n",
    "    avg_revenue = mean(Revenue, na.rm = TRUE),\n",
    "    transaction_count = n()\n",
    "  ) %>%\n",
    "  arrange(desc(total_revenue))\n",
    "\n",
    "cat(\"========== CATEGORY SUMMARY ==========\\n\")\n",
    "print(category_summary)\n",
    "#grouped the cleaned sales data by product category and calculated key metrics, then arranged by total revenue in descending order for better analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part5_intro",
   "metadata": {},
   "source": [
    "## Part 5: Data Reshaping with tidyr (Lesson 5)\n",
    "\n",
    "**Skills Assessed:** pivot_longer(), pivot_wider(), tidy data principles\n",
    "\n",
    "**Your Tasks:**\n",
    "1. Reshape data from wide to long format\n",
    "2. Reshape data from long to wide format\n",
    "3. Create analysis-ready datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_wide_data",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== REGION-CATEGORY DATA (LONG FORMAT) ==========\n",
      "\u001b[90m# A tibble: 10 Ã— 3\u001b[39m\n",
      "   Region        Product_Category total_revenue\n",
      "   \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m         \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m                    \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m\n",
      "\u001b[90m 1\u001b[39m Asia Pacific  Consulting             \u001b[4m7\u001b[24m\u001b[4m5\u001b[24m\u001b[4m9\u001b[24m641.\n",
      "\u001b[90m 2\u001b[39m Asia Pacific  Hardware               \u001b[4m2\u001b[24m\u001b[4m7\u001b[24m\u001b[4m1\u001b[24m979.\n",
      "\u001b[90m 3\u001b[39m Asia Pacific  Services               \u001b[4m3\u001b[24m\u001b[4m3\u001b[24m\u001b[4m1\u001b[24m826.\n",
      "\u001b[90m 4\u001b[39m Asia Pacific  Software               \u001b[4m4\u001b[24m\u001b[4m4\u001b[24m\u001b[4m0\u001b[24m797.\n",
      "\u001b[90m 5\u001b[39m Europe        Consulting             \u001b[4m3\u001b[24m\u001b[4m9\u001b[24m\u001b[4m0\u001b[24m670.\n",
      "\u001b[90m 6\u001b[39m Europe        Hardware               \u001b[4m7\u001b[24m\u001b[4m7\u001b[24m\u001b[4m7\u001b[24m044.\n",
      "\u001b[90m 7\u001b[39m Europe        Services               \u001b[4m5\u001b[24m\u001b[4m1\u001b[24m\u001b[4m3\u001b[24m507.\n",
      "\u001b[90m 8\u001b[39m Europe        Software               \u001b[4m5\u001b[24m\u001b[4m4\u001b[24m\u001b[4m2\u001b[24m961.\n",
      "\u001b[90m 9\u001b[39m Latin America Consulting             \u001b[4m4\u001b[24m\u001b[4m3\u001b[24m\u001b[4m3\u001b[24m397.\n",
      "\u001b[90m10\u001b[39m Latin America Hardware               \u001b[4m4\u001b[24m\u001b[4m7\u001b[24m\u001b[4m4\u001b[24m257.\n"
     ]
    }
   ],
   "source": [
    "# Task 5.1: Create Wide Format Data\n",
    "# First, create a summary by Region and Product_Category\n",
    "region_category_revenue <- sales_enhanced %>%\n",
    "  group_by(Region, Product_Category) %>%\n",
    "  summarize(total_revenue = sum(Revenue), .groups = 'drop')\n",
    "\n",
    "cat(\"========== REGION-CATEGORY DATA (LONG FORMAT) ==========\\n\")\n",
    "print(head(region_category_revenue, 10))\n",
    "#used the group_by and summarize functions to create a summary of total revenue by region and product category in long format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pivot_wider",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== REVENUE DATA (WIDE FORMAT) ==========\n",
      "\u001b[90m# A tibble: 4 Ã— 5\u001b[39m\n",
      "  Region        Consulting Hardware Services Software\n",
      "  \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m              \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m    \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m    \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m    \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m\n",
      "\u001b[90m1\u001b[39m Asia Pacific     \u001b[4m7\u001b[24m\u001b[4m5\u001b[24m\u001b[4m9\u001b[24m641.  \u001b[4m2\u001b[24m\u001b[4m7\u001b[24m\u001b[4m1\u001b[24m979.  \u001b[4m3\u001b[24m\u001b[4m3\u001b[24m\u001b[4m1\u001b[24m826.  \u001b[4m4\u001b[24m\u001b[4m4\u001b[24m\u001b[4m0\u001b[24m797.\n",
      "\u001b[90m2\u001b[39m Europe           \u001b[4m3\u001b[24m\u001b[4m9\u001b[24m\u001b[4m0\u001b[24m670.  \u001b[4m7\u001b[24m\u001b[4m7\u001b[24m\u001b[4m7\u001b[24m044.  \u001b[4m5\u001b[24m\u001b[4m1\u001b[24m\u001b[4m3\u001b[24m507.  \u001b[4m5\u001b[24m\u001b[4m4\u001b[24m\u001b[4m2\u001b[24m961.\n",
      "\u001b[90m3\u001b[39m Latin America    \u001b[4m4\u001b[24m\u001b[4m3\u001b[24m\u001b[4m3\u001b[24m397.  \u001b[4m4\u001b[24m\u001b[4m7\u001b[24m\u001b[4m4\u001b[24m257.  \u001b[4m6\u001b[24m\u001b[4m4\u001b[24m\u001b[4m4\u001b[24m772.  \u001b[4m5\u001b[24m\u001b[4m5\u001b[24m\u001b[4m9\u001b[24m611.\n",
      "\u001b[90m4\u001b[39m North America    \u001b[4m3\u001b[24m\u001b[4m9\u001b[24m\u001b[4m5\u001b[24m132.  \u001b[4m4\u001b[24m\u001b[4m2\u001b[24m\u001b[4m8\u001b[24m046.  \u001b[4m4\u001b[24m\u001b[4m7\u001b[24m\u001b[4m1\u001b[24m460.  \u001b[4m3\u001b[24m\u001b[4m3\u001b[24m\u001b[4m6\u001b[24m611.\n"
     ]
    }
   ],
   "source": [
    "# Task 5.2: Reshape to Wide Format\n",
    "# TODO: Create 'revenue_wide' by pivoting region_category_revenue\n",
    "#       so that Product_Category values become column names\n",
    "#       with total_revenue as the values\n",
    "revenue_wide <- region_category_revenue %>%\n",
    "  pivot_wider(names_from = Product_Category, values_from = total_revenue, values_fill = 0)\n",
    "  \n",
    "cat(\"========== REVENUE DATA (WIDE FORMAT) ==========\\n\")\n",
    "print(revenue_wide)\n",
    "#used pivot_wider to reshape the long format data into wide format for better readability and analysis of revenue by region and product category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pivot_longer",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== REVENUE DATA (BACK TO LONG FORMAT) ==========\n",
      "\u001b[90m# A tibble: 10 Ã— 3\u001b[39m\n",
      "   Region        Product_Category total_revenue\n",
      "   \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m         \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m                    \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m\n",
      "\u001b[90m 1\u001b[39m Asia Pacific  Consulting             \u001b[4m7\u001b[24m\u001b[4m5\u001b[24m\u001b[4m9\u001b[24m641.\n",
      "\u001b[90m 2\u001b[39m Asia Pacific  Hardware               \u001b[4m2\u001b[24m\u001b[4m7\u001b[24m\u001b[4m1\u001b[24m979.\n",
      "\u001b[90m 3\u001b[39m Asia Pacific  Services               \u001b[4m3\u001b[24m\u001b[4m3\u001b[24m\u001b[4m1\u001b[24m826.\n",
      "\u001b[90m 4\u001b[39m Asia Pacific  Software               \u001b[4m4\u001b[24m\u001b[4m4\u001b[24m\u001b[4m0\u001b[24m797.\n",
      "\u001b[90m 5\u001b[39m Europe        Consulting             \u001b[4m3\u001b[24m\u001b[4m9\u001b[24m\u001b[4m0\u001b[24m670.\n",
      "\u001b[90m 6\u001b[39m Europe        Hardware               \u001b[4m7\u001b[24m\u001b[4m7\u001b[24m\u001b[4m7\u001b[24m044.\n",
      "\u001b[90m 7\u001b[39m Europe        Services               \u001b[4m5\u001b[24m\u001b[4m1\u001b[24m\u001b[4m3\u001b[24m507.\n",
      "\u001b[90m 8\u001b[39m Europe        Software               \u001b[4m5\u001b[24m\u001b[4m4\u001b[24m\u001b[4m2\u001b[24m961.\n",
      "\u001b[90m 9\u001b[39m Latin America Consulting             \u001b[4m4\u001b[24m\u001b[4m3\u001b[24m\u001b[4m3\u001b[24m397.\n",
      "\u001b[90m10\u001b[39m Latin America Hardware               \u001b[4m4\u001b[24m\u001b[4m7\u001b[24m\u001b[4m4\u001b[24m257.\n"
     ]
    }
   ],
   "source": [
    "# Task 5.3: Reshape Back to Long Format\n",
    "# TODO: Create 'revenue_long' by pivoting revenue_wide back to long format\n",
    "#       Column names (except Region) should go into 'Product_Category'\n",
    "#       Values should go into 'revenue'\n",
    "\n",
    "revenue_long <- revenue_wide %>%\n",
    "  pivot_longer(cols = -Region, names_to = \"Product_Category\", values_to = \"total_revenue\")\n",
    "\n",
    "cat(\"========== REVENUE DATA (BACK TO LONG FORMAT) ==========\\n\")\n",
    "print(head(revenue_long, 10))\n",
    "#used pivot_longer to reshape the wide format data back into long format for further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part6_intro",
   "metadata": {},
   "source": [
    "## Part 6: Combining Datasets with Joins (Lesson 6)\n",
    "\n",
    "**Skills Assessed:** left_join(), inner_join(), data integration\n",
    "\n",
    "**Your Tasks:**\n",
    "1. Join customers with orders\n",
    "2. Join orders with order_items\n",
    "3. Create integrated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "join_customers_orders",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A spec_tbl_df: 200 Ã— 8</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>CustomerID</th><th scope=col>Name</th><th scope=col>Email</th><th scope=col>City</th><th scope=col>Registration_Date</th><th scope=col>OrderID</th><th scope=col>Order_Date</th><th scope=col>Total_Amount</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;date&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;date&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td> 1</td><td>Customer 1 </td><td>customer1@email.com </td><td>Phoenix    </td><td>2020-10-03</td><td> 87</td><td>2023-03-28</td><td> 716.18</td></tr>\n",
       "\t<tr><td> 1</td><td>Customer 1 </td><td>customer1@email.com </td><td>Phoenix    </td><td>2020-10-03</td><td>214</td><td>2023-09-12</td><td>1343.63</td></tr>\n",
       "\t<tr><td> 2</td><td>Customer 2 </td><td>customer2@email.com </td><td>Los Angeles</td><td>2020-06-02</td><td>173</td><td>2024-02-25</td><td> 159.98</td></tr>\n",
       "\t<tr><td> 2</td><td>Customer 2 </td><td>customer2@email.com </td><td>Los Angeles</td><td>2020-06-02</td><td>190</td><td>2023-04-19</td><td>1503.04</td></tr>\n",
       "\t<tr><td> 3</td><td>Customer 3 </td><td>customer3@email.com </td><td>Chicago    </td><td>2021-04-20</td><td> 29</td><td>2023-03-07</td><td> 441.06</td></tr>\n",
       "\t<tr><td> 3</td><td>Customer 3 </td><td>customer3@email.com </td><td>Chicago    </td><td>2021-04-20</td><td>146</td><td>2023-04-10</td><td>  85.28</td></tr>\n",
       "\t<tr><td> 4</td><td>Customer 4 </td><td>customer4@email.com </td><td>Houston    </td><td>2022-06-16</td><td>  7</td><td>2023-05-01</td><td> 482.98</td></tr>\n",
       "\t<tr><td> 4</td><td>Customer 4 </td><td>customer4@email.com </td><td>Houston    </td><td>2022-06-16</td><td> 61</td><td>2024-01-19</td><td> 666.43</td></tr>\n",
       "\t<tr><td> 5</td><td>Customer 5 </td><td>customer5@email.com </td><td>Phoenix    </td><td>2023-08-22</td><td> 15</td><td>2023-12-27</td><td> 560.36</td></tr>\n",
       "\t<tr><td> 5</td><td>Customer 5 </td><td>customer5@email.com </td><td>Phoenix    </td><td>2023-08-22</td><td>150</td><td>2023-03-28</td><td> 278.89</td></tr>\n",
       "\t<tr><td> 6</td><td>Customer 6 </td><td>customer6@email.com </td><td>Houston    </td><td>2021-09-10</td><td> 64</td><td>2023-09-26</td><td>1445.60</td></tr>\n",
       "\t<tr><td> 6</td><td>Customer 6 </td><td>customer6@email.com </td><td>Houston    </td><td>2021-09-10</td><td>134</td><td>2023-05-31</td><td>1918.40</td></tr>\n",
       "\t<tr><td> 7</td><td>Customer 7 </td><td>customer7@email.com </td><td>Houston    </td><td>2020-07-07</td><td> 71</td><td>2023-01-18</td><td>1846.62</td></tr>\n",
       "\t<tr><td> 7</td><td>Customer 7 </td><td>customer7@email.com </td><td>Houston    </td><td>2020-07-07</td><td>207</td><td>2023-09-13</td><td> 146.53</td></tr>\n",
       "\t<tr><td> 8</td><td>Customer 8 </td><td>customer8@email.com </td><td>Phoenix    </td><td>2022-01-20</td><td> 14</td><td>2023-10-01</td><td>1546.45</td></tr>\n",
       "\t<tr><td> 8</td><td>Customer 8 </td><td>customer8@email.com </td><td>Phoenix    </td><td>2022-01-20</td><td> 53</td><td>2023-06-11</td><td> 404.78</td></tr>\n",
       "\t<tr><td> 9</td><td>Customer 9 </td><td>customer9@email.com </td><td>New York   </td><td>2020-12-09</td><td>111</td><td>2023-01-04</td><td> 251.17</td></tr>\n",
       "\t<tr><td> 9</td><td>Customer 9 </td><td>customer9@email.com </td><td>New York   </td><td>2020-12-09</td><td>164</td><td>2023-09-23</td><td> 839.65</td></tr>\n",
       "\t<tr><td>10</td><td>Customer 10</td><td>customer10@email.com</td><td>Phoenix    </td><td>2022-05-04</td><td>107</td><td>2023-01-04</td><td> 221.89</td></tr>\n",
       "\t<tr><td>10</td><td>Customer 10</td><td>customer10@email.com</td><td>Phoenix    </td><td>2022-05-04</td><td>156</td><td>2023-03-16</td><td> 608.56</td></tr>\n",
       "\t<tr><td>11</td><td>Customer 11</td><td>customer11@email.com</td><td>Chicago    </td><td>2021-07-22</td><td> 78</td><td>2023-12-10</td><td>1668.95</td></tr>\n",
       "\t<tr><td>11</td><td>Customer 11</td><td>customer11@email.com</td><td>Chicago    </td><td>2021-07-22</td><td> 94</td><td>2023-04-05</td><td>1270.72</td></tr>\n",
       "\t<tr><td>12</td><td>Customer 12</td><td>customer12@email.com</td><td>Chicago    </td><td>2022-07-25</td><td>  2</td><td>2024-03-24</td><td> 183.09</td></tr>\n",
       "\t<tr><td>12</td><td>Customer 12</td><td>customer12@email.com</td><td>Chicago    </td><td>2022-07-25</td><td>219</td><td>2023-05-25</td><td>1657.58</td></tr>\n",
       "\t<tr><td>13</td><td>Customer 13</td><td>customer13@email.com</td><td>Phoenix    </td><td>2023-01-18</td><td> 45</td><td>2023-02-13</td><td> 939.66</td></tr>\n",
       "\t<tr><td>13</td><td>Customer 13</td><td>customer13@email.com</td><td>Phoenix    </td><td>2023-01-18</td><td>217</td><td>2023-08-24</td><td>1033.40</td></tr>\n",
       "\t<tr><td>14</td><td>Customer 14</td><td>customer14@email.com</td><td>New York   </td><td>2022-09-12</td><td>180</td><td>2023-12-22</td><td> 169.94</td></tr>\n",
       "\t<tr><td>14</td><td>Customer 14</td><td>customer14@email.com</td><td>New York   </td><td>2022-09-12</td><td>250</td><td>2023-11-02</td><td> 466.99</td></tr>\n",
       "\t<tr><td>15</td><td>Customer 15</td><td>customer15@email.com</td><td>Chicago    </td><td>2023-06-22</td><td> 83</td><td>2023-04-18</td><td> 980.37</td></tr>\n",
       "\t<tr><td>15</td><td>Customer 15</td><td>customer15@email.com</td><td>Chicago    </td><td>2023-06-22</td><td>147</td><td>2024-01-27</td><td> 718.29</td></tr>\n",
       "\t<tr><td>â‹®</td><td>â‹®</td><td>â‹®</td><td>â‹®</td><td>â‹®</td><td>â‹®</td><td>â‹®</td><td>â‹®</td></tr>\n",
       "\t<tr><td> 86</td><td>Customer 86 </td><td>customer86@email.com </td><td>Los Angeles</td><td>2023-03-02</td><td>136</td><td>2023-03-14</td><td> 482.64</td></tr>\n",
       "\t<tr><td> 86</td><td>Customer 86 </td><td>customer86@email.com </td><td>Los Angeles</td><td>2023-03-02</td><td>141</td><td>2023-12-08</td><td>1777.99</td></tr>\n",
       "\t<tr><td> 87</td><td>Customer 87 </td><td>customer87@email.com </td><td>Chicago    </td><td>2020-03-26</td><td>  1</td><td>2023-08-30</td><td> 424.30</td></tr>\n",
       "\t<tr><td> 87</td><td>Customer 87 </td><td>customer87@email.com </td><td>Chicago    </td><td>2020-03-26</td><td>177</td><td>2023-09-20</td><td> 168.84</td></tr>\n",
       "\t<tr><td> 88</td><td>Customer 88 </td><td>customer88@email.com </td><td>Los Angeles</td><td>2023-08-20</td><td>108</td><td>2023-05-29</td><td>1488.84</td></tr>\n",
       "\t<tr><td> 88</td><td>Customer 88 </td><td>customer88@email.com </td><td>Los Angeles</td><td>2023-08-20</td><td>178</td><td>2023-07-03</td><td>1912.15</td></tr>\n",
       "\t<tr><td> 89</td><td>Customer 89 </td><td>customer89@email.com </td><td>New York   </td><td>2023-06-30</td><td>125</td><td>2024-01-07</td><td>1533.65</td></tr>\n",
       "\t<tr><td> 89</td><td>Customer 89 </td><td>customer89@email.com </td><td>New York   </td><td>2023-06-30</td><td>171</td><td>2023-09-03</td><td> 736.42</td></tr>\n",
       "\t<tr><td> 90</td><td>Customer 90 </td><td>customer90@email.com </td><td>Los Angeles</td><td>2022-03-11</td><td> 55</td><td>2023-08-12</td><td> 297.02</td></tr>\n",
       "\t<tr><td> 90</td><td>Customer 90 </td><td>customer90@email.com </td><td>Los Angeles</td><td>2022-03-11</td><td>218</td><td>2024-01-04</td><td>1162.34</td></tr>\n",
       "\t<tr><td> 91</td><td>Customer 91 </td><td>customer91@email.com </td><td>Chicago    </td><td>2021-11-25</td><td> 80</td><td>2023-04-21</td><td>1325.73</td></tr>\n",
       "\t<tr><td> 91</td><td>Customer 91 </td><td>customer91@email.com </td><td>Chicago    </td><td>2021-11-25</td><td>140</td><td>2024-01-17</td><td>  50.24</td></tr>\n",
       "\t<tr><td> 92</td><td>Customer 92 </td><td>customer92@email.com </td><td>Chicago    </td><td>2023-09-04</td><td>160</td><td>2024-03-13</td><td> 767.61</td></tr>\n",
       "\t<tr><td> 92</td><td>Customer 92 </td><td>customer92@email.com </td><td>Chicago    </td><td>2023-09-04</td><td>192</td><td>2023-07-16</td><td> 752.64</td></tr>\n",
       "\t<tr><td> 93</td><td>Customer 93 </td><td>customer93@email.com </td><td>Phoenix    </td><td>2020-08-05</td><td> 35</td><td>2023-12-10</td><td>1303.57</td></tr>\n",
       "\t<tr><td> 93</td><td>Customer 93 </td><td>customer93@email.com </td><td>Phoenix    </td><td>2020-08-05</td><td>215</td><td>2024-02-06</td><td>1644.31</td></tr>\n",
       "\t<tr><td> 94</td><td>Customer 94 </td><td>customer94@email.com </td><td>Houston    </td><td>2020-12-16</td><td>167</td><td>2024-03-26</td><td> 991.42</td></tr>\n",
       "\t<tr><td> 94</td><td>Customer 94 </td><td>customer94@email.com </td><td>Houston    </td><td>2020-12-16</td><td>229</td><td>2023-12-26</td><td>1934.93</td></tr>\n",
       "\t<tr><td> 95</td><td>Customer 95 </td><td>customer95@email.com </td><td>Houston    </td><td>2023-07-25</td><td>126</td><td>2023-05-01</td><td>1242.76</td></tr>\n",
       "\t<tr><td> 95</td><td>Customer 95 </td><td>customer95@email.com </td><td>Houston    </td><td>2023-07-25</td><td>232</td><td>2023-02-08</td><td> 537.69</td></tr>\n",
       "\t<tr><td> 96</td><td>Customer 96 </td><td>customer96@email.com </td><td>Phoenix    </td><td>2021-02-05</td><td>109</td><td>2023-09-15</td><td> 116.21</td></tr>\n",
       "\t<tr><td> 96</td><td>Customer 96 </td><td>customer96@email.com </td><td>Phoenix    </td><td>2021-02-05</td><td>158</td><td>2024-02-06</td><td>1250.61</td></tr>\n",
       "\t<tr><td> 97</td><td>Customer 97 </td><td>customer97@email.com </td><td>New York   </td><td>2021-09-12</td><td>193</td><td>2023-11-04</td><td>1894.50</td></tr>\n",
       "\t<tr><td> 97</td><td>Customer 97 </td><td>customer97@email.com </td><td>New York   </td><td>2021-09-12</td><td>238</td><td>2023-01-12</td><td>1835.29</td></tr>\n",
       "\t<tr><td> 98</td><td>Customer 98 </td><td>customer98@email.com </td><td>Chicago    </td><td>2023-08-07</td><td>154</td><td>2023-03-17</td><td>1647.70</td></tr>\n",
       "\t<tr><td> 98</td><td>Customer 98 </td><td>customer98@email.com </td><td>Chicago    </td><td>2023-08-07</td><td>194</td><td>2023-11-20</td><td>1142.20</td></tr>\n",
       "\t<tr><td> 99</td><td>Customer 99 </td><td>customer99@email.com </td><td>Houston    </td><td>2022-04-23</td><td> 32</td><td>2024-03-29</td><td> 916.28</td></tr>\n",
       "\t<tr><td> 99</td><td>Customer 99 </td><td>customer99@email.com </td><td>Houston    </td><td>2022-04-23</td><td>181</td><td>2023-08-04</td><td> 254.02</td></tr>\n",
       "\t<tr><td>100</td><td>Customer 100</td><td>customer100@email.com</td><td>Chicago    </td><td>2021-05-18</td><td>  5</td><td>2023-08-17</td><td> 286.60</td></tr>\n",
       "\t<tr><td>100</td><td>Customer 100</td><td>customer100@email.com</td><td>Chicago    </td><td>2021-05-18</td><td> 10</td><td>2023-06-08</td><td>1674.36</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A spec\\_tbl\\_df: 200 Ã— 8\n",
       "\\begin{tabular}{llllllll}\n",
       " CustomerID & Name & Email & City & Registration\\_Date & OrderID & Order\\_Date & Total\\_Amount\\\\\n",
       " <dbl> & <chr> & <chr> & <chr> & <date> & <dbl> & <date> & <dbl>\\\\\n",
       "\\hline\n",
       "\t  1 & Customer 1  & customer1@email.com  & Phoenix     & 2020-10-03 &  87 & 2023-03-28 &  716.18\\\\\n",
       "\t  1 & Customer 1  & customer1@email.com  & Phoenix     & 2020-10-03 & 214 & 2023-09-12 & 1343.63\\\\\n",
       "\t  2 & Customer 2  & customer2@email.com  & Los Angeles & 2020-06-02 & 173 & 2024-02-25 &  159.98\\\\\n",
       "\t  2 & Customer 2  & customer2@email.com  & Los Angeles & 2020-06-02 & 190 & 2023-04-19 & 1503.04\\\\\n",
       "\t  3 & Customer 3  & customer3@email.com  & Chicago     & 2021-04-20 &  29 & 2023-03-07 &  441.06\\\\\n",
       "\t  3 & Customer 3  & customer3@email.com  & Chicago     & 2021-04-20 & 146 & 2023-04-10 &   85.28\\\\\n",
       "\t  4 & Customer 4  & customer4@email.com  & Houston     & 2022-06-16 &   7 & 2023-05-01 &  482.98\\\\\n",
       "\t  4 & Customer 4  & customer4@email.com  & Houston     & 2022-06-16 &  61 & 2024-01-19 &  666.43\\\\\n",
       "\t  5 & Customer 5  & customer5@email.com  & Phoenix     & 2023-08-22 &  15 & 2023-12-27 &  560.36\\\\\n",
       "\t  5 & Customer 5  & customer5@email.com  & Phoenix     & 2023-08-22 & 150 & 2023-03-28 &  278.89\\\\\n",
       "\t  6 & Customer 6  & customer6@email.com  & Houston     & 2021-09-10 &  64 & 2023-09-26 & 1445.60\\\\\n",
       "\t  6 & Customer 6  & customer6@email.com  & Houston     & 2021-09-10 & 134 & 2023-05-31 & 1918.40\\\\\n",
       "\t  7 & Customer 7  & customer7@email.com  & Houston     & 2020-07-07 &  71 & 2023-01-18 & 1846.62\\\\\n",
       "\t  7 & Customer 7  & customer7@email.com  & Houston     & 2020-07-07 & 207 & 2023-09-13 &  146.53\\\\\n",
       "\t  8 & Customer 8  & customer8@email.com  & Phoenix     & 2022-01-20 &  14 & 2023-10-01 & 1546.45\\\\\n",
       "\t  8 & Customer 8  & customer8@email.com  & Phoenix     & 2022-01-20 &  53 & 2023-06-11 &  404.78\\\\\n",
       "\t  9 & Customer 9  & customer9@email.com  & New York    & 2020-12-09 & 111 & 2023-01-04 &  251.17\\\\\n",
       "\t  9 & Customer 9  & customer9@email.com  & New York    & 2020-12-09 & 164 & 2023-09-23 &  839.65\\\\\n",
       "\t 10 & Customer 10 & customer10@email.com & Phoenix     & 2022-05-04 & 107 & 2023-01-04 &  221.89\\\\\n",
       "\t 10 & Customer 10 & customer10@email.com & Phoenix     & 2022-05-04 & 156 & 2023-03-16 &  608.56\\\\\n",
       "\t 11 & Customer 11 & customer11@email.com & Chicago     & 2021-07-22 &  78 & 2023-12-10 & 1668.95\\\\\n",
       "\t 11 & Customer 11 & customer11@email.com & Chicago     & 2021-07-22 &  94 & 2023-04-05 & 1270.72\\\\\n",
       "\t 12 & Customer 12 & customer12@email.com & Chicago     & 2022-07-25 &   2 & 2024-03-24 &  183.09\\\\\n",
       "\t 12 & Customer 12 & customer12@email.com & Chicago     & 2022-07-25 & 219 & 2023-05-25 & 1657.58\\\\\n",
       "\t 13 & Customer 13 & customer13@email.com & Phoenix     & 2023-01-18 &  45 & 2023-02-13 &  939.66\\\\\n",
       "\t 13 & Customer 13 & customer13@email.com & Phoenix     & 2023-01-18 & 217 & 2023-08-24 & 1033.40\\\\\n",
       "\t 14 & Customer 14 & customer14@email.com & New York    & 2022-09-12 & 180 & 2023-12-22 &  169.94\\\\\n",
       "\t 14 & Customer 14 & customer14@email.com & New York    & 2022-09-12 & 250 & 2023-11-02 &  466.99\\\\\n",
       "\t 15 & Customer 15 & customer15@email.com & Chicago     & 2023-06-22 &  83 & 2023-04-18 &  980.37\\\\\n",
       "\t 15 & Customer 15 & customer15@email.com & Chicago     & 2023-06-22 & 147 & 2024-01-27 &  718.29\\\\\n",
       "\t â‹® & â‹® & â‹® & â‹® & â‹® & â‹® & â‹® & â‹®\\\\\n",
       "\t  86 & Customer 86  & customer86@email.com  & Los Angeles & 2023-03-02 & 136 & 2023-03-14 &  482.64\\\\\n",
       "\t  86 & Customer 86  & customer86@email.com  & Los Angeles & 2023-03-02 & 141 & 2023-12-08 & 1777.99\\\\\n",
       "\t  87 & Customer 87  & customer87@email.com  & Chicago     & 2020-03-26 &   1 & 2023-08-30 &  424.30\\\\\n",
       "\t  87 & Customer 87  & customer87@email.com  & Chicago     & 2020-03-26 & 177 & 2023-09-20 &  168.84\\\\\n",
       "\t  88 & Customer 88  & customer88@email.com  & Los Angeles & 2023-08-20 & 108 & 2023-05-29 & 1488.84\\\\\n",
       "\t  88 & Customer 88  & customer88@email.com  & Los Angeles & 2023-08-20 & 178 & 2023-07-03 & 1912.15\\\\\n",
       "\t  89 & Customer 89  & customer89@email.com  & New York    & 2023-06-30 & 125 & 2024-01-07 & 1533.65\\\\\n",
       "\t  89 & Customer 89  & customer89@email.com  & New York    & 2023-06-30 & 171 & 2023-09-03 &  736.42\\\\\n",
       "\t  90 & Customer 90  & customer90@email.com  & Los Angeles & 2022-03-11 &  55 & 2023-08-12 &  297.02\\\\\n",
       "\t  90 & Customer 90  & customer90@email.com  & Los Angeles & 2022-03-11 & 218 & 2024-01-04 & 1162.34\\\\\n",
       "\t  91 & Customer 91  & customer91@email.com  & Chicago     & 2021-11-25 &  80 & 2023-04-21 & 1325.73\\\\\n",
       "\t  91 & Customer 91  & customer91@email.com  & Chicago     & 2021-11-25 & 140 & 2024-01-17 &   50.24\\\\\n",
       "\t  92 & Customer 92  & customer92@email.com  & Chicago     & 2023-09-04 & 160 & 2024-03-13 &  767.61\\\\\n",
       "\t  92 & Customer 92  & customer92@email.com  & Chicago     & 2023-09-04 & 192 & 2023-07-16 &  752.64\\\\\n",
       "\t  93 & Customer 93  & customer93@email.com  & Phoenix     & 2020-08-05 &  35 & 2023-12-10 & 1303.57\\\\\n",
       "\t  93 & Customer 93  & customer93@email.com  & Phoenix     & 2020-08-05 & 215 & 2024-02-06 & 1644.31\\\\\n",
       "\t  94 & Customer 94  & customer94@email.com  & Houston     & 2020-12-16 & 167 & 2024-03-26 &  991.42\\\\\n",
       "\t  94 & Customer 94  & customer94@email.com  & Houston     & 2020-12-16 & 229 & 2023-12-26 & 1934.93\\\\\n",
       "\t  95 & Customer 95  & customer95@email.com  & Houston     & 2023-07-25 & 126 & 2023-05-01 & 1242.76\\\\\n",
       "\t  95 & Customer 95  & customer95@email.com  & Houston     & 2023-07-25 & 232 & 2023-02-08 &  537.69\\\\\n",
       "\t  96 & Customer 96  & customer96@email.com  & Phoenix     & 2021-02-05 & 109 & 2023-09-15 &  116.21\\\\\n",
       "\t  96 & Customer 96  & customer96@email.com  & Phoenix     & 2021-02-05 & 158 & 2024-02-06 & 1250.61\\\\\n",
       "\t  97 & Customer 97  & customer97@email.com  & New York    & 2021-09-12 & 193 & 2023-11-04 & 1894.50\\\\\n",
       "\t  97 & Customer 97  & customer97@email.com  & New York    & 2021-09-12 & 238 & 2023-01-12 & 1835.29\\\\\n",
       "\t  98 & Customer 98  & customer98@email.com  & Chicago     & 2023-08-07 & 154 & 2023-03-17 & 1647.70\\\\\n",
       "\t  98 & Customer 98  & customer98@email.com  & Chicago     & 2023-08-07 & 194 & 2023-11-20 & 1142.20\\\\\n",
       "\t  99 & Customer 99  & customer99@email.com  & Houston     & 2022-04-23 &  32 & 2024-03-29 &  916.28\\\\\n",
       "\t  99 & Customer 99  & customer99@email.com  & Houston     & 2022-04-23 & 181 & 2023-08-04 &  254.02\\\\\n",
       "\t 100 & Customer 100 & customer100@email.com & Chicago     & 2021-05-18 &   5 & 2023-08-17 &  286.60\\\\\n",
       "\t 100 & Customer 100 & customer100@email.com & Chicago     & 2021-05-18 &  10 & 2023-06-08 & 1674.36\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A spec_tbl_df: 200 Ã— 8\n",
       "\n",
       "| CustomerID &lt;dbl&gt; | Name &lt;chr&gt; | Email &lt;chr&gt; | City &lt;chr&gt; | Registration_Date &lt;date&gt; | OrderID &lt;dbl&gt; | Order_Date &lt;date&gt; | Total_Amount &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|\n",
       "|  1 | Customer 1  | customer1@email.com  | Phoenix     | 2020-10-03 |  87 | 2023-03-28 |  716.18 |\n",
       "|  1 | Customer 1  | customer1@email.com  | Phoenix     | 2020-10-03 | 214 | 2023-09-12 | 1343.63 |\n",
       "|  2 | Customer 2  | customer2@email.com  | Los Angeles | 2020-06-02 | 173 | 2024-02-25 |  159.98 |\n",
       "|  2 | Customer 2  | customer2@email.com  | Los Angeles | 2020-06-02 | 190 | 2023-04-19 | 1503.04 |\n",
       "|  3 | Customer 3  | customer3@email.com  | Chicago     | 2021-04-20 |  29 | 2023-03-07 |  441.06 |\n",
       "|  3 | Customer 3  | customer3@email.com  | Chicago     | 2021-04-20 | 146 | 2023-04-10 |   85.28 |\n",
       "|  4 | Customer 4  | customer4@email.com  | Houston     | 2022-06-16 |   7 | 2023-05-01 |  482.98 |\n",
       "|  4 | Customer 4  | customer4@email.com  | Houston     | 2022-06-16 |  61 | 2024-01-19 |  666.43 |\n",
       "|  5 | Customer 5  | customer5@email.com  | Phoenix     | 2023-08-22 |  15 | 2023-12-27 |  560.36 |\n",
       "|  5 | Customer 5  | customer5@email.com  | Phoenix     | 2023-08-22 | 150 | 2023-03-28 |  278.89 |\n",
       "|  6 | Customer 6  | customer6@email.com  | Houston     | 2021-09-10 |  64 | 2023-09-26 | 1445.60 |\n",
       "|  6 | Customer 6  | customer6@email.com  | Houston     | 2021-09-10 | 134 | 2023-05-31 | 1918.40 |\n",
       "|  7 | Customer 7  | customer7@email.com  | Houston     | 2020-07-07 |  71 | 2023-01-18 | 1846.62 |\n",
       "|  7 | Customer 7  | customer7@email.com  | Houston     | 2020-07-07 | 207 | 2023-09-13 |  146.53 |\n",
       "|  8 | Customer 8  | customer8@email.com  | Phoenix     | 2022-01-20 |  14 | 2023-10-01 | 1546.45 |\n",
       "|  8 | Customer 8  | customer8@email.com  | Phoenix     | 2022-01-20 |  53 | 2023-06-11 |  404.78 |\n",
       "|  9 | Customer 9  | customer9@email.com  | New York    | 2020-12-09 | 111 | 2023-01-04 |  251.17 |\n",
       "|  9 | Customer 9  | customer9@email.com  | New York    | 2020-12-09 | 164 | 2023-09-23 |  839.65 |\n",
       "| 10 | Customer 10 | customer10@email.com | Phoenix     | 2022-05-04 | 107 | 2023-01-04 |  221.89 |\n",
       "| 10 | Customer 10 | customer10@email.com | Phoenix     | 2022-05-04 | 156 | 2023-03-16 |  608.56 |\n",
       "| 11 | Customer 11 | customer11@email.com | Chicago     | 2021-07-22 |  78 | 2023-12-10 | 1668.95 |\n",
       "| 11 | Customer 11 | customer11@email.com | Chicago     | 2021-07-22 |  94 | 2023-04-05 | 1270.72 |\n",
       "| 12 | Customer 12 | customer12@email.com | Chicago     | 2022-07-25 |   2 | 2024-03-24 |  183.09 |\n",
       "| 12 | Customer 12 | customer12@email.com | Chicago     | 2022-07-25 | 219 | 2023-05-25 | 1657.58 |\n",
       "| 13 | Customer 13 | customer13@email.com | Phoenix     | 2023-01-18 |  45 | 2023-02-13 |  939.66 |\n",
       "| 13 | Customer 13 | customer13@email.com | Phoenix     | 2023-01-18 | 217 | 2023-08-24 | 1033.40 |\n",
       "| 14 | Customer 14 | customer14@email.com | New York    | 2022-09-12 | 180 | 2023-12-22 |  169.94 |\n",
       "| 14 | Customer 14 | customer14@email.com | New York    | 2022-09-12 | 250 | 2023-11-02 |  466.99 |\n",
       "| 15 | Customer 15 | customer15@email.com | Chicago     | 2023-06-22 |  83 | 2023-04-18 |  980.37 |\n",
       "| 15 | Customer 15 | customer15@email.com | Chicago     | 2023-06-22 | 147 | 2024-01-27 |  718.29 |\n",
       "| â‹® | â‹® | â‹® | â‹® | â‹® | â‹® | â‹® | â‹® |\n",
       "|  86 | Customer 86  | customer86@email.com  | Los Angeles | 2023-03-02 | 136 | 2023-03-14 |  482.64 |\n",
       "|  86 | Customer 86  | customer86@email.com  | Los Angeles | 2023-03-02 | 141 | 2023-12-08 | 1777.99 |\n",
       "|  87 | Customer 87  | customer87@email.com  | Chicago     | 2020-03-26 |   1 | 2023-08-30 |  424.30 |\n",
       "|  87 | Customer 87  | customer87@email.com  | Chicago     | 2020-03-26 | 177 | 2023-09-20 |  168.84 |\n",
       "|  88 | Customer 88  | customer88@email.com  | Los Angeles | 2023-08-20 | 108 | 2023-05-29 | 1488.84 |\n",
       "|  88 | Customer 88  | customer88@email.com  | Los Angeles | 2023-08-20 | 178 | 2023-07-03 | 1912.15 |\n",
       "|  89 | Customer 89  | customer89@email.com  | New York    | 2023-06-30 | 125 | 2024-01-07 | 1533.65 |\n",
       "|  89 | Customer 89  | customer89@email.com  | New York    | 2023-06-30 | 171 | 2023-09-03 |  736.42 |\n",
       "|  90 | Customer 90  | customer90@email.com  | Los Angeles | 2022-03-11 |  55 | 2023-08-12 |  297.02 |\n",
       "|  90 | Customer 90  | customer90@email.com  | Los Angeles | 2022-03-11 | 218 | 2024-01-04 | 1162.34 |\n",
       "|  91 | Customer 91  | customer91@email.com  | Chicago     | 2021-11-25 |  80 | 2023-04-21 | 1325.73 |\n",
       "|  91 | Customer 91  | customer91@email.com  | Chicago     | 2021-11-25 | 140 | 2024-01-17 |   50.24 |\n",
       "|  92 | Customer 92  | customer92@email.com  | Chicago     | 2023-09-04 | 160 | 2024-03-13 |  767.61 |\n",
       "|  92 | Customer 92  | customer92@email.com  | Chicago     | 2023-09-04 | 192 | 2023-07-16 |  752.64 |\n",
       "|  93 | Customer 93  | customer93@email.com  | Phoenix     | 2020-08-05 |  35 | 2023-12-10 | 1303.57 |\n",
       "|  93 | Customer 93  | customer93@email.com  | Phoenix     | 2020-08-05 | 215 | 2024-02-06 | 1644.31 |\n",
       "|  94 | Customer 94  | customer94@email.com  | Houston     | 2020-12-16 | 167 | 2024-03-26 |  991.42 |\n",
       "|  94 | Customer 94  | customer94@email.com  | Houston     | 2020-12-16 | 229 | 2023-12-26 | 1934.93 |\n",
       "|  95 | Customer 95  | customer95@email.com  | Houston     | 2023-07-25 | 126 | 2023-05-01 | 1242.76 |\n",
       "|  95 | Customer 95  | customer95@email.com  | Houston     | 2023-07-25 | 232 | 2023-02-08 |  537.69 |\n",
       "|  96 | Customer 96  | customer96@email.com  | Phoenix     | 2021-02-05 | 109 | 2023-09-15 |  116.21 |\n",
       "|  96 | Customer 96  | customer96@email.com  | Phoenix     | 2021-02-05 | 158 | 2024-02-06 | 1250.61 |\n",
       "|  97 | Customer 97  | customer97@email.com  | New York    | 2021-09-12 | 193 | 2023-11-04 | 1894.50 |\n",
       "|  97 | Customer 97  | customer97@email.com  | New York    | 2021-09-12 | 238 | 2023-01-12 | 1835.29 |\n",
       "|  98 | Customer 98  | customer98@email.com  | Chicago     | 2023-08-07 | 154 | 2023-03-17 | 1647.70 |\n",
       "|  98 | Customer 98  | customer98@email.com  | Chicago     | 2023-08-07 | 194 | 2023-11-20 | 1142.20 |\n",
       "|  99 | Customer 99  | customer99@email.com  | Houston     | 2022-04-23 |  32 | 2024-03-29 |  916.28 |\n",
       "|  99 | Customer 99  | customer99@email.com  | Houston     | 2022-04-23 | 181 | 2023-08-04 |  254.02 |\n",
       "| 100 | Customer 100 | customer100@email.com | Chicago     | 2021-05-18 |   5 | 2023-08-17 |  286.60 |\n",
       "| 100 | Customer 100 | customer100@email.com | Chicago     | 2021-05-18 |  10 | 2023-06-08 | 1674.36 |\n",
       "\n"
      ],
      "text/plain": [
       "    CustomerID Name         Email                 City        Registration_Date\n",
       "1    1         Customer 1   customer1@email.com   Phoenix     2020-10-03       \n",
       "2    1         Customer 1   customer1@email.com   Phoenix     2020-10-03       \n",
       "3    2         Customer 2   customer2@email.com   Los Angeles 2020-06-02       \n",
       "4    2         Customer 2   customer2@email.com   Los Angeles 2020-06-02       \n",
       "5    3         Customer 3   customer3@email.com   Chicago     2021-04-20       \n",
       "6    3         Customer 3   customer3@email.com   Chicago     2021-04-20       \n",
       "7    4         Customer 4   customer4@email.com   Houston     2022-06-16       \n",
       "8    4         Customer 4   customer4@email.com   Houston     2022-06-16       \n",
       "9    5         Customer 5   customer5@email.com   Phoenix     2023-08-22       \n",
       "10   5         Customer 5   customer5@email.com   Phoenix     2023-08-22       \n",
       "11   6         Customer 6   customer6@email.com   Houston     2021-09-10       \n",
       "12   6         Customer 6   customer6@email.com   Houston     2021-09-10       \n",
       "13   7         Customer 7   customer7@email.com   Houston     2020-07-07       \n",
       "14   7         Customer 7   customer7@email.com   Houston     2020-07-07       \n",
       "15   8         Customer 8   customer8@email.com   Phoenix     2022-01-20       \n",
       "16   8         Customer 8   customer8@email.com   Phoenix     2022-01-20       \n",
       "17   9         Customer 9   customer9@email.com   New York    2020-12-09       \n",
       "18   9         Customer 9   customer9@email.com   New York    2020-12-09       \n",
       "19  10         Customer 10  customer10@email.com  Phoenix     2022-05-04       \n",
       "20  10         Customer 10  customer10@email.com  Phoenix     2022-05-04       \n",
       "21  11         Customer 11  customer11@email.com  Chicago     2021-07-22       \n",
       "22  11         Customer 11  customer11@email.com  Chicago     2021-07-22       \n",
       "23  12         Customer 12  customer12@email.com  Chicago     2022-07-25       \n",
       "24  12         Customer 12  customer12@email.com  Chicago     2022-07-25       \n",
       "25  13         Customer 13  customer13@email.com  Phoenix     2023-01-18       \n",
       "26  13         Customer 13  customer13@email.com  Phoenix     2023-01-18       \n",
       "27  14         Customer 14  customer14@email.com  New York    2022-09-12       \n",
       "28  14         Customer 14  customer14@email.com  New York    2022-09-12       \n",
       "29  15         Customer 15  customer15@email.com  Chicago     2023-06-22       \n",
       "30  15         Customer 15  customer15@email.com  Chicago     2023-06-22       \n",
       "â‹®   â‹®          â‹®            â‹®                     â‹®           â‹®                \n",
       "171  86        Customer 86  customer86@email.com  Los Angeles 2023-03-02       \n",
       "172  86        Customer 86  customer86@email.com  Los Angeles 2023-03-02       \n",
       "173  87        Customer 87  customer87@email.com  Chicago     2020-03-26       \n",
       "174  87        Customer 87  customer87@email.com  Chicago     2020-03-26       \n",
       "175  88        Customer 88  customer88@email.com  Los Angeles 2023-08-20       \n",
       "176  88        Customer 88  customer88@email.com  Los Angeles 2023-08-20       \n",
       "177  89        Customer 89  customer89@email.com  New York    2023-06-30       \n",
       "178  89        Customer 89  customer89@email.com  New York    2023-06-30       \n",
       "179  90        Customer 90  customer90@email.com  Los Angeles 2022-03-11       \n",
       "180  90        Customer 90  customer90@email.com  Los Angeles 2022-03-11       \n",
       "181  91        Customer 91  customer91@email.com  Chicago     2021-11-25       \n",
       "182  91        Customer 91  customer91@email.com  Chicago     2021-11-25       \n",
       "183  92        Customer 92  customer92@email.com  Chicago     2023-09-04       \n",
       "184  92        Customer 92  customer92@email.com  Chicago     2023-09-04       \n",
       "185  93        Customer 93  customer93@email.com  Phoenix     2020-08-05       \n",
       "186  93        Customer 93  customer93@email.com  Phoenix     2020-08-05       \n",
       "187  94        Customer 94  customer94@email.com  Houston     2020-12-16       \n",
       "188  94        Customer 94  customer94@email.com  Houston     2020-12-16       \n",
       "189  95        Customer 95  customer95@email.com  Houston     2023-07-25       \n",
       "190  95        Customer 95  customer95@email.com  Houston     2023-07-25       \n",
       "191  96        Customer 96  customer96@email.com  Phoenix     2021-02-05       \n",
       "192  96        Customer 96  customer96@email.com  Phoenix     2021-02-05       \n",
       "193  97        Customer 97  customer97@email.com  New York    2021-09-12       \n",
       "194  97        Customer 97  customer97@email.com  New York    2021-09-12       \n",
       "195  98        Customer 98  customer98@email.com  Chicago     2023-08-07       \n",
       "196  98        Customer 98  customer98@email.com  Chicago     2023-08-07       \n",
       "197  99        Customer 99  customer99@email.com  Houston     2022-04-23       \n",
       "198  99        Customer 99  customer99@email.com  Houston     2022-04-23       \n",
       "199 100        Customer 100 customer100@email.com Chicago     2021-05-18       \n",
       "200 100        Customer 100 customer100@email.com Chicago     2021-05-18       \n",
       "    OrderID Order_Date Total_Amount\n",
       "1    87     2023-03-28  716.18     \n",
       "2   214     2023-09-12 1343.63     \n",
       "3   173     2024-02-25  159.98     \n",
       "4   190     2023-04-19 1503.04     \n",
       "5    29     2023-03-07  441.06     \n",
       "6   146     2023-04-10   85.28     \n",
       "7     7     2023-05-01  482.98     \n",
       "8    61     2024-01-19  666.43     \n",
       "9    15     2023-12-27  560.36     \n",
       "10  150     2023-03-28  278.89     \n",
       "11   64     2023-09-26 1445.60     \n",
       "12  134     2023-05-31 1918.40     \n",
       "13   71     2023-01-18 1846.62     \n",
       "14  207     2023-09-13  146.53     \n",
       "15   14     2023-10-01 1546.45     \n",
       "16   53     2023-06-11  404.78     \n",
       "17  111     2023-01-04  251.17     \n",
       "18  164     2023-09-23  839.65     \n",
       "19  107     2023-01-04  221.89     \n",
       "20  156     2023-03-16  608.56     \n",
       "21   78     2023-12-10 1668.95     \n",
       "22   94     2023-04-05 1270.72     \n",
       "23    2     2024-03-24  183.09     \n",
       "24  219     2023-05-25 1657.58     \n",
       "25   45     2023-02-13  939.66     \n",
       "26  217     2023-08-24 1033.40     \n",
       "27  180     2023-12-22  169.94     \n",
       "28  250     2023-11-02  466.99     \n",
       "29   83     2023-04-18  980.37     \n",
       "30  147     2024-01-27  718.29     \n",
       "â‹®   â‹®       â‹®          â‹®           \n",
       "171 136     2023-03-14  482.64     \n",
       "172 141     2023-12-08 1777.99     \n",
       "173   1     2023-08-30  424.30     \n",
       "174 177     2023-09-20  168.84     \n",
       "175 108     2023-05-29 1488.84     \n",
       "176 178     2023-07-03 1912.15     \n",
       "177 125     2024-01-07 1533.65     \n",
       "178 171     2023-09-03  736.42     \n",
       "179  55     2023-08-12  297.02     \n",
       "180 218     2024-01-04 1162.34     \n",
       "181  80     2023-04-21 1325.73     \n",
       "182 140     2024-01-17   50.24     \n",
       "183 160     2024-03-13  767.61     \n",
       "184 192     2023-07-16  752.64     \n",
       "185  35     2023-12-10 1303.57     \n",
       "186 215     2024-02-06 1644.31     \n",
       "187 167     2024-03-26  991.42     \n",
       "188 229     2023-12-26 1934.93     \n",
       "189 126     2023-05-01 1242.76     \n",
       "190 232     2023-02-08  537.69     \n",
       "191 109     2023-09-15  116.21     \n",
       "192 158     2024-02-06 1250.61     \n",
       "193 193     2023-11-04 1894.50     \n",
       "194 238     2023-01-12 1835.29     \n",
       "195 154     2023-03-17 1647.70     \n",
       "196 194     2023-11-20 1142.20     \n",
       "197  32     2024-03-29  916.28     \n",
       "198 181     2023-08-04  254.02     \n",
       "199   5     2023-08-17  286.60     \n",
       "200  10     2023-06-08 1674.36     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== CUSTOMER ORDERS ==========\n",
      "Total rows: 400 \n",
      "Columns: 4 \n"
     ]
    }
   ],
   "source": [
    "# Task 6.1: Join Customers and Orders\n",
    "# TODO: Create 'customer_orders' by left joining customers with orders\n",
    "#       Join on CustomerID\n",
    "\n",
    "customer_orders <- order_items\n",
    "    left_join(customers, orders, by = \"CustomerID\")\n",
    "\n",
    "cat(\"========== CUSTOMER ORDERS ==========\\n\")\n",
    "cat(\"Total rows:\", nrow(customer_orders), \"\\n\")\n",
    "cat(\"Columns:\", ncol(customer_orders), \"\\n\")\n",
    "#used left_join to combine customer and order data based on CustomerID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "join_orders_items",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== ORDERS WITH ITEMS ==========\n",
      "Total rows: 400 \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 5 Ã— 7</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>OrderID</th><th scope=col>ProductID</th><th scope=col>Quantity</th><th scope=col>Unit_Price</th><th scope=col>CustomerID</th><th scope=col>Order_Date</th><th scope=col>Total_Amount</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;date&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>213</td><td> 8</td><td>1</td><td> 43.81</td><td>75</td><td>2023-11-09</td><td>1941.93</td></tr>\n",
       "\t<tr><td>176</td><td>18</td><td>5</td><td>489.16</td><td>53</td><td>2023-09-27</td><td>1811.81</td></tr>\n",
       "\t<tr><td>118</td><td> 2</td><td>5</td><td>442.09</td><td>41</td><td>2024-02-21</td><td> 267.10</td></tr>\n",
       "\t<tr><td> 58</td><td>19</td><td>3</td><td>321.92</td><td>84</td><td>2023-05-07</td><td> 880.59</td></tr>\n",
       "\t<tr><td>202</td><td> 2</td><td>4</td><td>280.43</td><td>58</td><td>2023-03-30</td><td>1557.90</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 5 Ã— 7\n",
       "\\begin{tabular}{lllllll}\n",
       " OrderID & ProductID & Quantity & Unit\\_Price & CustomerID & Order\\_Date & Total\\_Amount\\\\\n",
       " <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <date> & <dbl>\\\\\n",
       "\\hline\n",
       "\t 213 &  8 & 1 &  43.81 & 75 & 2023-11-09 & 1941.93\\\\\n",
       "\t 176 & 18 & 5 & 489.16 & 53 & 2023-09-27 & 1811.81\\\\\n",
       "\t 118 &  2 & 5 & 442.09 & 41 & 2024-02-21 &  267.10\\\\\n",
       "\t  58 & 19 & 3 & 321.92 & 84 & 2023-05-07 &  880.59\\\\\n",
       "\t 202 &  2 & 4 & 280.43 & 58 & 2023-03-30 & 1557.90\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 5 Ã— 7\n",
       "\n",
       "| OrderID &lt;dbl&gt; | ProductID &lt;dbl&gt; | Quantity &lt;dbl&gt; | Unit_Price &lt;dbl&gt; | CustomerID &lt;dbl&gt; | Order_Date &lt;date&gt; | Total_Amount &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|\n",
       "| 213 |  8 | 1 |  43.81 | 75 | 2023-11-09 | 1941.93 |\n",
       "| 176 | 18 | 5 | 489.16 | 53 | 2023-09-27 | 1811.81 |\n",
       "| 118 |  2 | 5 | 442.09 | 41 | 2024-02-21 |  267.10 |\n",
       "|  58 | 19 | 3 | 321.92 | 84 | 2023-05-07 |  880.59 |\n",
       "| 202 |  2 | 4 | 280.43 | 58 | 2023-03-30 | 1557.90 |\n",
       "\n"
      ],
      "text/plain": [
       "  OrderID ProductID Quantity Unit_Price CustomerID Order_Date Total_Amount\n",
       "1 213      8        1         43.81     75         2023-11-09 1941.93     \n",
       "2 176     18        5        489.16     53         2023-09-27 1811.81     \n",
       "3 118      2        5        442.09     41         2024-02-21  267.10     \n",
       "4  58     19        3        321.92     84         2023-05-07  880.59     \n",
       "5 202      2        4        280.43     58         2023-03-30 1557.90     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Task 6.2: Join Orders and Order Items\n",
    "# TODO: Create 'orders_with_items' by inner joining orders with order_items\n",
    "#       Join on OrderID\n",
    "\n",
    "orders_with_items <- order_items %>%\n",
    "  inner_join(orders, by = \"OrderID\")\n",
    "\n",
    "cat(\"========== ORDERS WITH ITEMS ==========\\n\")\n",
    "cat(\"Total rows:\", nrow(orders_with_items), \"\\n\")\n",
    "head(orders_with_items, 5)\n",
    "#used inner_join to combine order and order item data based on OrderID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part7_intro",
   "metadata": {},
   "source": [
    "## Part 7: String Manipulation & Date/Time Operations (Lesson 7)\n",
    "\n",
    "**Skills Assessed:** stringr functions, lubridate functions\n",
    "\n",
    "**Your Tasks:**\n",
    "1. Clean text data\n",
    "2. Parse dates\n",
    "3. Extract date components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean_text",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== CLEANED TEXT DATA ==========\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 5 Ã— 4</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>Region</th><th scope=col>region_clean</th><th scope=col>Product_Category</th><th scope=col>category_clean</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>Latin America</td><td>Latin America</td><td>Services</td><td>Services</td></tr>\n",
       "\t<tr><td>Europe       </td><td>Europe       </td><td>Hardware</td><td>Hardware</td></tr>\n",
       "\t<tr><td>Europe       </td><td>Europe       </td><td>Services</td><td>Services</td></tr>\n",
       "\t<tr><td>Europe       </td><td>Europe       </td><td>Hardware</td><td>Hardware</td></tr>\n",
       "\t<tr><td>Latin America</td><td>Latin America</td><td>Software</td><td>Software</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 5 Ã— 4\n",
       "\\begin{tabular}{llll}\n",
       " Region & region\\_clean & Product\\_Category & category\\_clean\\\\\n",
       " <chr> & <chr> & <chr> & <chr>\\\\\n",
       "\\hline\n",
       "\t Latin America & Latin America & Services & Services\\\\\n",
       "\t Europe        & Europe        & Hardware & Hardware\\\\\n",
       "\t Europe        & Europe        & Services & Services\\\\\n",
       "\t Europe        & Europe        & Hardware & Hardware\\\\\n",
       "\t Latin America & Latin America & Software & Software\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 5 Ã— 4\n",
       "\n",
       "| Region &lt;chr&gt; | region_clean &lt;chr&gt; | Product_Category &lt;chr&gt; | category_clean &lt;chr&gt; |\n",
       "|---|---|---|---|\n",
       "| Latin America | Latin America | Services | Services |\n",
       "| Europe        | Europe        | Hardware | Hardware |\n",
       "| Europe        | Europe        | Services | Services |\n",
       "| Europe        | Europe        | Hardware | Hardware |\n",
       "| Latin America | Latin America | Software | Software |\n",
       "\n"
      ],
      "text/plain": [
       "  Region        region_clean  Product_Category category_clean\n",
       "1 Latin America Latin America Services         Services      \n",
       "2 Europe        Europe        Hardware         Hardware      \n",
       "3 Europe        Europe        Services         Services      \n",
       "4 Europe        Europe        Hardware         Hardware      \n",
       "5 Latin America Latin America Software         Software      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Task 7.1: Clean Text Data\n",
    "# TODO: Add these columns to sales_enhanced using mutate():\n",
    "#   - region_clean: Region with trimmed whitespace and Title Case\n",
    "#   - category_clean: Product_Category with trimmed whitespace and Title Case\n",
    "\n",
    "\n",
    "sales_enhanced <- sales_enhanced %>%\n",
    "  mutate(\n",
    "    region_clean = str_to_title(str_trim(Region)),\n",
    "    category_clean = str_to_title(str_trim(Product_Category))\n",
    "  )\n",
    "\n",
    "cat(\"========== CLEANED TEXT DATA ==========\\n\")\n",
    "head(sales_enhanced %>% select(Region, region_clean, Product_Category, category_clean), 5)\n",
    "#used stringr functions to clean text data in the sales dataset by trimming whitespace and converting to title case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parse_dates",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== DATE COMPONENTS ==========\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 5 Ã— 4</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>Sale_Date</th><th scope=col>date_parsed</th><th scope=col>sale_month</th><th scope=col>sale_weekday</th></tr>\n",
       "\t<tr><th scope=col>&lt;date&gt;</th><th scope=col>&lt;date&gt;</th><th scope=col>&lt;ord&gt;</th><th scope=col>&lt;ord&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>2023-04-24</td><td>2023-04-24</td><td>April </td><td>Monday  </td></tr>\n",
       "\t<tr><td>2023-06-09</td><td>2023-06-09</td><td>June  </td><td>Friday  </td></tr>\n",
       "\t<tr><td>2023-03-25</td><td>2023-03-25</td><td>March </td><td>Saturday</td></tr>\n",
       "\t<tr><td>2023-04-11</td><td>2023-04-11</td><td>April </td><td>Tuesday </td></tr>\n",
       "\t<tr><td>2023-08-26</td><td>2023-08-26</td><td>August</td><td>Saturday</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 5 Ã— 4\n",
       "\\begin{tabular}{llll}\n",
       " Sale\\_Date & date\\_parsed & sale\\_month & sale\\_weekday\\\\\n",
       " <date> & <date> & <ord> & <ord>\\\\\n",
       "\\hline\n",
       "\t 2023-04-24 & 2023-04-24 & April  & Monday  \\\\\n",
       "\t 2023-06-09 & 2023-06-09 & June   & Friday  \\\\\n",
       "\t 2023-03-25 & 2023-03-25 & March  & Saturday\\\\\n",
       "\t 2023-04-11 & 2023-04-11 & April  & Tuesday \\\\\n",
       "\t 2023-08-26 & 2023-08-26 & August & Saturday\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 5 Ã— 4\n",
       "\n",
       "| Sale_Date &lt;date&gt; | date_parsed &lt;date&gt; | sale_month &lt;ord&gt; | sale_weekday &lt;ord&gt; |\n",
       "|---|---|---|---|\n",
       "| 2023-04-24 | 2023-04-24 | April  | Monday   |\n",
       "| 2023-06-09 | 2023-06-09 | June   | Friday   |\n",
       "| 2023-03-25 | 2023-03-25 | March  | Saturday |\n",
       "| 2023-04-11 | 2023-04-11 | April  | Tuesday  |\n",
       "| 2023-08-26 | 2023-08-26 | August | Saturday |\n",
       "\n"
      ],
      "text/plain": [
       "  Sale_Date  date_parsed sale_month sale_weekday\n",
       "1 2023-04-24 2023-04-24  April      Monday      \n",
       "2 2023-06-09 2023-06-09  June       Friday      \n",
       "3 2023-03-25 2023-03-25  March      Saturday    \n",
       "4 2023-04-11 2023-04-11  April      Tuesday     \n",
       "5 2023-08-26 2023-08-26  August     Saturday    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Task 7.2: Parse Dates and Extract Components\n",
    "# TODO: Add these date-related columns using mutate():\n",
    "#   - date_parsed: Parse Sale_Date column (use ymd(), mdy(), or dmy() as appropriate)\n",
    "#   - sale_month: Extract month name from date_parsed\n",
    "#   - sale_weekday: Extract weekday name from date_parsed\n",
    "\n",
    "\n",
    "sales_enhanced <- sales_enhanced %>%\n",
    "  mutate(\n",
    "    date_parsed = ymd(Sale_Date),\n",
    "    sale_month = month(date_parsed, label = TRUE, abbr = FALSE),\n",
    "    sale_weekday = wday(date_parsed, label = TRUE, abbr = FALSE)\n",
    "  )\n",
    "\n",
    "cat(\"========== DATE COMPONENTS ==========\\n\")\n",
    "head(sales_enhanced %>% select(Sale_Date, date_parsed, sale_month, sale_weekday), 5)\n",
    "\n",
    "#used lubridate functions to parse date strings and extract month and weekday names for better analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part8_intro",
   "metadata": {},
   "source": [
    "## Part 8: Advanced Wrangling & Business Intelligence (Lesson 8)\n",
    "\n",
    "**Skills Assessed:** case_when(), complex logic, KPIs\n",
    "\n",
    "**Your Tasks:**\n",
    "1. Create business categories with case_when()\n",
    "2. Calculate KPIs\n",
    "3. Generate executive summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "case_when_logic",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== PERFORMANCE TIERS ==========\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "  High    Low Medium \n",
       "   154     74     72 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Task 8.1: Create Performance Categories\n",
    "# TODO: Add 'performance_tier' column using case_when():\n",
    "#   - \"High\" if Revenue > 25000\n",
    "#   - \"Medium\" if Revenue > 15000\n",
    "#   - \"Low\" otherwise\n",
    "\n",
    "sales_enhanced <- sales_enhanced %>%\n",
    "  mutate(\n",
    "    performance_tier = case_when(\n",
    "      Revenue > 25000 ~ \"High\",\n",
    "      Revenue > 15000 ~ \"Medium\",\n",
    "      TRUE ~ \"Low\" \n",
    "    )\n",
    "  )\n",
    "\n",
    "cat(\"========== PERFORMANCE TIERS ==========\\n\")\n",
    "table(sales_enhanced$performance_tier)\n",
    "#used case_when to categorize sales performance into tiers based on revenue thresholds to get a better understanding of sales distribution and possibly get average ticekt size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "calculate_kpis",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== BUSINESS KPIs ==========\n",
      "\u001b[90m# A tibble: 1 Ã— 4\u001b[39m\n",
      "  total_revenue total_transactions avg_transaction_value high_value_pct\n",
      "          \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m              \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m                 \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m          \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m\n",
      "\u001b[90m1\u001b[39m      7\u001b[4m7\u001b[24m\u001b[4m7\u001b[24m\u001b[4m1\u001b[24m711.                300                \u001b[4m2\u001b[24m\u001b[4m5\u001b[24m906.           64.7\n"
     ]
    }
   ],
   "source": [
    "# Task 8.2: Calculate Business KPIs\n",
    "# TODO: Create 'business_kpis' with these metrics:\n",
    "#   - total_revenue: sum of Revenue\n",
    "#   - total_transactions: count of rows\n",
    "#   - avg_transaction_value: mean of Revenue\n",
    "#   - high_value_pct: percentage where high_value = \"Yes\"\n",
    "\n",
    "business_kpis <- sales_enhanced %>%\n",
    "  summarize(\n",
    "    total_revenue = sum(Revenue, na.rm = TRUE),\n",
    "    total_transactions = n(),\n",
    "    avg_transaction_value = mean(Revenue, na.rm = TRUE),\n",
    "    high_value_pct = mean(high_value == \"Yes\") * 100\n",
    "  )\n",
    "\n",
    "cat(\"========== BUSINESS KPIs ==========\\n\")\n",
    "print(business_kpis)\n",
    "#calculated key business performance indicators from the cleaned sales data for overall business insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflection_intro",
   "metadata": {},
   "source": [
    "## Part 9: Reflection Questions\n",
    "\n",
    "Answer the following questions based on your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflection1",
   "metadata": {},
   "source": [
    "### Question 9.1: Data Cleaning Impact\n",
    "\n",
    "**How did handling missing values and outliers affect your analysis? Why is data cleaning important before performing business analysis?**\n",
    "\n",
    "Your answer here: I honestly do no know if there were any outliers in the data because I did not find any. I am not sure if this is meant to be like that or my data set is faulty. However, if i had to, I would handle missing values by removing rows with NAs in critical columns like Revenue. Data cleaning is crucial because it ensures the accuracy and reliability of the analysis, leading to better business decisions and better planned business strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflection2",
   "metadata": {},
   "source": [
    "### Question 9.2: Grouped Analysis Value\n",
    "\n",
    "**What insights did you gain from the regional and category summaries that you couldn't see in the raw data? How can businesses use this type of grouped analysis?**\n",
    "\n",
    "Your answer here: Grouped analysis allowed me to see which regions and product categories were performing better than others. For example, I could identify that certain regions had significantly higher revenues compared to others, and some product categories were more popular among customers. Businesses can use this type of analysis to allocate resources more effectively, target marketing efforts, and optimize inventory based on regional preferences and category performance. We can also gathered insights about what products to promote in which regions based on past performance, and which products we should retire or remarket based on low performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflection3",
   "metadata": {},
   "source": [
    "### Question 9.3: Data Reshaping Purpose\n",
    "\n",
    "**Why would you need to reshape data between wide and long formats? Provide a business scenario where each format would be useful.**\n",
    "\n",
    "Your answer here: Reshaping data is essential for different types of analysis and visualization. Wide format is useful when you want to compare multiple variables side by side, such as sales figures for different products across several months. This format makes it easier to gather insights, create charts, and tables. Long format is beneficial for time series analysis or when using certain statistical models that require data in a tidy format. For example, if we want to analyze sales trends over time for each product, having the data in long format allows us to easily bucket by product and date, facilitating trend analysis and forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflection4",
   "metadata": {},
   "source": [
    "### Question 9.4: Joining Datasets\n",
    "\n",
    "**What is the difference between left_join() and inner_join()? When would you use each one in a business context?**\n",
    "\n",
    "Your answer here: Left_join() returns all rows from the left dataset and the matched rows from the right dataset. If there is no match, it fills with NAs. This is useful when you want to retain all records from the primary dataset, such as keeping all customers even if they haven't made any orders. Inner_join() returns only the matching rows in both datasets. This is useful when you only want to analyze records that have correlated entries in both datasets, for example, analyzing only customers who have made purchases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflection5",
   "metadata": {},
   "source": [
    "### Question 9.5: Skills Integration\n",
    "\n",
    "**Which R data wrangling skill (from Lessons 1-8) do you think is most valuable for business analytics? Why?**\n",
    "\n",
    "Your answer here: I believe that all the functions learnt so far are very useful depending on the necessites of the business analysis. However, if I had to choose one, I would say that either mutate() and summarize() are the best. These functions allow analysts to create new insights by transforming existing data and calculating key metrics that drive business decisions. For example, creating performance tiers or calculating average transaction values can directly inform marketing strategies and operational improvements without having to do complex coding or manual calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Exam Complete!\n",
    "\n",
    "### What You've Demonstrated\n",
    "\n",
    "âœ… **Lesson 1:** R basics and data import\n",
    "âœ… **Lesson 2:** Data cleaning (missing values & outliers)\n",
    "âœ… **Lesson 3:** Data transformation (select, filter, arrange)\n",
    "âœ… **Lesson 4:** Advanced transformation (mutate, summarize, group_by)\n",
    "âœ… **Lesson 5:** Data reshaping (pivot_longer, pivot_wider)\n",
    "âœ… **Lesson 6:** Combining datasets (joins)\n",
    "âœ… **Lesson 7:** String manipulation & date/time operations\n",
    "âœ… **Lesson 8:** Advanced wrangling & business intelligence\n",
    "\n",
    "### Submission Checklist\n",
    "\n",
    "Before submitting, ensure:\n",
    "- [ ] All code cells run without errors\n",
    "- [ ] All TODO sections completed\n",
    "- [ ] All required dataframes created with correct names\n",
    "- [ ] All 5 reflection questions answered\n",
    "- [ ] Student name and ID filled in at top\n",
    "\n",
    "**Good work! ðŸŽ‰**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
