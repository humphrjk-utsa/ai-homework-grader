{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# MIDTERM EXAM: Comprehensive R Data Wrangling Assessment\n",
    "\n",
    "**Student Name:** Sean Johnson\n",
    "\n",
    "**Student ID:** pfb463\n",
    "\n",
    "**Date:** 10/18/2025\n",
    "\n",
    "**Time Limit:** 4 hours\n",
    "\n",
    "---\n",
    "\n",
    "## Exam Overview\n",
    "\n",
    "This comprehensive midterm exam assesses your mastery of ALL R data wrangling skills covered in Lessons 1-8:\n",
    "\n",
    "- **Lesson 1:** R Basics and Data Import\n",
    "- **Lesson 2:** Data Cleaning (Missing Values & Outliers)\n",
    "- **Lesson 3:** Data Transformation Part 1 (select, filter, arrange)\n",
    "- **Lesson 4:** Data Transformation Part 2 (mutate, summarize, group_by)\n",
    "- **Lesson 5:** Data Reshaping (pivot_longer, pivot_wider)\n",
    "- **Lesson 6:** Combining Datasets (joins)\n",
    "- **Lesson 7:** String Manipulation & Date/Time\n",
    "- **Lesson 8:** Advanced Wrangling & Best Practices\n",
    "\n",
    "## Business Scenario\n",
    "\n",
    "You are a data analyst for a retail company. The executive team needs a comprehensive analysis of:\n",
    "- Sales performance across products and regions\n",
    "- Customer behavior and segmentation\n",
    "- Data quality issues and recommendations\n",
    "- Strategic insights for business growth\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. **Set your working directory** to where your data files are located\n",
    "2. Complete ALL tasks in order\n",
    "3. Write code in the TODO sections\n",
    "4. Use the pipe operator (%>%) to chain operations\n",
    "5. Add comments explaining your logic\n",
    "6. Run all cells to verify your code works\n",
    "7. Answer all reflection questions\n",
    "\n",
    "## Grading\n",
    "\n",
    "- **Code Correctness (40%)**: All tasks completed correctly\n",
    "- **Code Quality (20%)**: Clean, well-commented code\n",
    "- **Business Understanding (20%)**: Demonstrates understanding of context\n",
    "- **Analysis & Insights (15%)**: Meaningful insights and recommendations\n",
    "- **Reflection Questions (5%)**: Thoughtful answers\n",
    "\n",
    "## Academic Integrity\n",
    "\n",
    "This is an individual exam. You may use:\n",
    "- Course notes and lesson materials\n",
    "- R documentation and help files\n",
    "- Your previous homework assignments\n",
    "\n",
    "You may NOT:\n",
    "- Collaborate with other students\n",
    "- Use AI assistants or online forums\n",
    "- Share code or solutions\n",
    "\n",
    "---\n",
    "\n",
    "**Good luck! ðŸŽ“**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1_intro",
   "metadata": {},
   "source": [
    "## Part 1: R Basics and Data Import (Lesson 1)\n",
    "\n",
    "**Skills Assessed:** Variables, data types, data import, working directory\n",
    "\n",
    "**Your Tasks:**\n",
    "1. Set working directory\n",
    "2. Load required packages\n",
    "3. Import multiple datasets\n",
    "4. Examine data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "setup",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory set using here::here(): / \n",
      "Current working directory: / \n",
      "Current working directory: / \n"
     ]
    }
   ],
   "source": [
    "# Task 1.1: Set Working Directory\n",
    "# This snippet attempts to set a sensible working directory for the student:\n",
    "# 1. If the repo root contains a 'data' folder, use that\n",
    "# 2. Otherwise, try here::here() if available\n",
    "# 3. Otherwise, instruct the student to set the path manually\n",
    "\n",
    "# Helper: prefer the workspace 'data' folder if it exists\n",
    "project_data_path <- file.path(getwd(), \"data\")\n",
    "if (dir.exists(project_data_path)) {\n",
    "  setwd(project_data_path)\n",
    "  cat(\"Working directory set to project data folder:\", getwd(), \"\\n\")\n",
    "} else if (requireNamespace(\"here\", quietly = TRUE)) {\n",
    "  new_wd <- here::here()\n",
    "  setwd(new_wd)\n",
    "  cat(\"Working directory set using here::here():\", getwd(), \"\\n\")\n",
    "} else {\n",
    "  cat(\"Could not auto-detect a project data folder.\\n\")\n",
    "  cat(\"Please set your working directory to the folder containing the data files, for example:\\n\")\n",
    "  cat(\"setwd('/path/to/your/data/folder')\\n\")\n",
    "}\n",
    "\n",
    "# Verify working directory\n",
    "cat(\"Current working directory:\", getwd(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11149de",
   "metadata": {},
   "source": [
    "I set up the code so it first checks for a local â€œdataâ€ folder. This makes sure that any relative paths to data files work the same way every time I run the notebook. If that folder isnâ€™t there, it automatically uses here::here() to find the project root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "load_packages",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in parse(text = input): <text>:25:13: unexpected symbol\n24: # Create 'missing_summary' that shows count of NAs in each column of sales_data\n25: if (exists(\"sales_data\n                ^\n",
     "output_type": "error",
     "traceback": [
      "Error in parse(text = input): <text>:25:13: unexpected symbol\n24: # Create 'missing_summary' that shows count of NAs in each column of sales_data\n25: if (exists(\"sales_data\n                ^\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "# Task 1.2: Load Required Packages\n",
    "# This cell loads (and installs if missing) the packages used in the exam: tidyverse and lubridate.\n",
    "required_pkgs <- c(\"tidyverse\", \"lubridate\")\n",
    "to_install <- required_pkgs[!required_pkgs %in% installed.packages()[,\"Package\"]]\n",
    "if (length(to_install) > 0) {\n",
    "  cat('Installing missing packages:', paste(to_install, collapse = ', '), '\\n')\n",
    "  install.packages(to_install, repos = 'https://cloud.r-project.org')\n",
    "}\n",
    "# Load packages quietly\n",
    "invisible(lapply(required_pkgs, function(p) {\n",
    "  suppressPackageStartupMessages(library(p, character.only = TRUE))\n",
    "}))\n",
    "# Quick check\n",
    "loaded <- sapply(required_pkgs, function(p) requireNamespace(p, quietly = TRUE))\n",
    "if (all(loaded)) {\n",
    "  cat(\"âœ… Packages loaded successfully: \n",
    "\\n\")\n",
    "} else {\n",
    "  cat(\"âš ï¸ Some packages failed to load: \n",
    "\\n\")\n",
    "  cat(\"If load failed, try restarting the kernel or install manually.\n",
    "\n",
    "source\n",
    "# Create 'missing_summary' that shows count of NAs in each column of sales_data\n",
    "if (exists(\"sales_data\") && !(ncol(sales_data) == 1 && names(sales_data)[1] == '.warning')) {\n",
    "  missing_summary <- sapply(sales_data, function(col) sum(is.na(col)))\n",
    "  missing_summary <- as.data.frame(t(missing_summary), stringsAsFactors = FALSE)\n",
    "  names(missing_summary) <- names(sales_data)\n",
    "} else {\n",
    "  missing_summary <- data.frame(note = 'sales_data not available; ensure Task 1.3 ran successfully')\n",
    "}\n",
    "cat('========== MISSING VALUES SUMMARY ==========\\n')\n",
    "print(missing_summary)\n",
    "if (is.data.frame(missing_summary) && 'note' %in% names(missing_summary)) {\n",
    "  cat('\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "}  cat('\n",
    "Total missing values:', total_missing, '\\n')  total_missing <- sum(sapply(sales_data, function(col) sum(is.na(col))))} else if (exists(\"sales_data\")) {Note: sales_data is not available; complete the import step.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2_intro",
   "metadata": {},
   "source": [
    "## Part 2: Data Cleaning - Missing Values & Outliers (Lesson 2)\n",
    "\n",
    "**Skills Assessed:** Identifying NAs, handling missing data, detecting outliers\n",
    "\n",
    "**Your Tasks:**\n",
    "1. Check for missing values in sales_data\n",
    "2. Handle missing values appropriately\n",
    "3. Identify outliers in Revenue column\n",
    "4. Create a cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "check_missing",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in parse(text = input): <text>:20:1: unexpected symbol\n19: }  cat('\n20: Total\n    ^\n",
     "output_type": "error",
     "traceback": [
      "Error in parse(text = input): <text>:20:1: unexpected symbol\n19: }  cat('\n20: Total\n    ^\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "# Task 2.1: Check for Missing Values\n",
    "# Create 'missing_summary' that shows count of NAs in each column of sales_data\n",
    "if (exists('sales_data') && !(ncol(sales_data) == 1 && names(sales_data)[1] == '.warning')) {\n",
    "  missing_summary <- sapply(sales_data, function(col) sum(is.na(col)))\n",
    "  missing_summary <- as.data.frame(t(missing_summary), stringsAsFactors = FALSE)\n",
    "  names(missing_summary) <- names(sales_data)\n",
    "} else {\n",
    "  missing_summary <- data.frame(note = 'sales_data not available; ensure Task 1.3 ran successfully')\n",
    "}\n",
    "cat('========== MISSING VALUES SUMMARY ==========\\n')\n",
    "print(missing_summary)\n",
    "if (exists('missing_summary') && is.data.frame(missing_summary) && 'note' %in% names(missing_summary)) {\n",
    "  cat('\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "}  cat('\n",
    "Total missing values:', total_missing, '\\n')  total_missing <- sum(sapply(sales_data, function(col) sum(is.na(col))))} else if (exists('missing_summary')) {Note: sales_data is not available; complete the import step.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "handle_missing",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== DATA CLEANING RESULTS ==========\n",
      "sales_data not available; sales_clean is a placeholder tibble.\n",
      "sales_data not available; sales_clean is a placeholder tibble.\n"
     ]
    }
   ],
   "source": [
    "# Task 2.2: Handle Missing Values\n",
    "# Create 'sales_clean' by removing rows with ANY missing values\n",
    "if (exists(\"sales_data\") && !(ncol(sales_data) == 1 && names(sales_data)[1] == '.warning')) {\n",
    "  # Remove rows with any NA values\n",
    "  sales_clean <- sales_data[complete.cases(sales_data), , drop = FALSE]\n",
    "} else {\n",
    "  sales_clean <- tibble::tibble(.warning = 'sales_data not available; cannot create sales_clean')\n",
    "}\n",
    "cat('========== DATA CLEANING RESULTS ==========\\n')\n",
    "if (exists(\"sales_data\") && !(ncol(sales_data) == 1 && names(sales_data)[1] == '.warning')) {\n",
    "  cat('Original rows:', nrow(sales_data), '\\n')\n",
    "  cat('Cleaned rows:', nrow(sales_clean), '\\n')\n",
    "  cat('Rows removed:', nrow(sales_data) - nrow(sales_clean), '\\n')\n",
    "} else {\n",
    "  cat('sales_data not available; sales_clean is a placeholder tibble.\\n')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc997c8",
   "metadata": {},
   "source": [
    "I removed any rows that contained NA values so that sales_clean only included complete records for the analysis. This helps prevent missing values from carrying over into summary statistics or models later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "detect_outliers",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sales_clean not available; cannot compute outliers. Make sure Task 2.2 ran successfully.\n"
     ]
    }
   ],
   "source": [
    "# Task 2.3: Detect Outliers (IQR method)\n",
    "# Compute IQR-based outlier thresholds for numeric columns in sales_clean.\n",
    "# Produces `outlier_analysis` with variable, Q1, Q3, IQR, lower_bound, upper_bound, outlier_count, outlier_pct.\n",
    "\n",
    "# Defensive check: ensure sales_clean exists and is not a placeholder\n",
    "if (exists(\"sales_clean\") && !(ncol(sales_clean) == 1 && names(sales_clean)[1] == '.warning')) {\n",
    "  # Identify numeric columns\n",
    "  numeric_cols <- names(sales_clean)[vapply(sales_clean, is.numeric, logical(1))]\n",
    "\n",
    "  if (length(numeric_cols) == 0) {\n",
    "    message(\"No numeric columns found in sales_clean. outlier_analysis will be empty.\")\n",
    "    outlier_analysis <- tibble::tibble(\n",
    "      variable = character(),\n",
    "      Q1 = double(),\n",
    "      Q3 = double(),\n",
    "      IQR = double(),\n",
    "      lower_bound = double(),\n",
    "      upper_bound = double(),\n",
    "      outlier_count = integer(),\n",
    "      outlier_pct = double()\n",
    "    )\n",
    "  } else {\n",
    "    outlier_list <- lapply(numeric_cols, function(col) {\n",
    "      vec <- sales_clean[[col]]\n",
    "      vec <- vec[!is.na(vec)]\n",
    "      q1 <- unname(stats::quantile(vec, 0.25, na.rm = TRUE, type = 7))\n",
    "      q3 <- unname(stats::quantile(vec, 0.75, na.rm = TRUE, type = 7))\n",
    "      iqr <- q3 - q1\n",
    "      lower <- q1 - 1.5 * iqr\n",
    "      upper <- q3 + 1.5 * iqr\n",
    "      out_count <- sum(vec < lower | vec > upper, na.rm = TRUE)\n",
    "      total <- length(vec)\n",
    "      out_pct <- if (total > 0) round(100 * out_count / total, 2) else NA_real_\n",
    "\n",
    "      tibble::tibble(\n",
    "        variable = col, Q1 = q1, Q3 = q3, IQR = iqr,\n",
    "        lower_bound = lower, upper_bound = upper,\n",
    "        outlier_count = out_count, outlier_pct = out_pct\n",
    "      )\n",
    "    })\n",
    "\n",
    "    outlier_analysis <- dplyr::bind_rows(outlier_list) %>% dplyr::arrange(dplyr::desc(outlier_count))\n",
    "  }\n",
    "\n",
    "  # Print concise summaries\n",
    "  message(\"IQR outlier detection complete. Numeric columns checked: \", length(numeric_cols))\n",
    "  if (exists(\"outlier_analysis\") && nrow(outlier_analysis) > 0) {\n",
    "    if ('Revenue' %in% outlier_analysis$variable) {\n",
    "      cat(\"========== REVENUE OUTLIER SUMMARY ==========\\n\")\n",
    "      print(outlier_analysis %>% dplyr::filter(variable == 'Revenue'))\n",
    "      cat('\\nNumber of revenue outliers detected: ', outlier_analysis %>% dplyr::filter(variable == 'Revenue') %>% dplyr::pull(outlier_count), '\\n')\n",
    "    } else {\n",
    "      cat(\"No 'Revenue' column found. Showing outlier analysis for numeric columns:\\n\")\n",
    "      print(outlier_analysis)\n",
    "    }\n",
    "  } else {\n",
    "    message(\"outlier_analysis is empty (no numeric columns or no data).\")\n",
    "  }\n",
    "\n",
    "} else {\n",
    "  outlier_analysis <- tibble::tibble(variable = character(), Q1 = double(), Q3 = double(), IQR = double(), lower_bound = double(), upper_bound = double(), outlier_count = integer(), outlier_pct = double())\n",
    "  cat(\"sales_clean not available; cannot compute outliers. Make sure Task 2.2 ran successfully.\\n\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b1d67b",
   "metadata": {},
   "source": [
    "I removed all rows containing NA values so that sales_clean only included complete records for analysis. This step ensured that missing values wouldnâ€™t affect summaries or model results later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3_intro",
   "metadata": {},
   "source": [
    "## Part 3: Data Transformation Part 1 (Lesson 3)\n",
    "\n",
    "**Skills Assessed:** select(), filter(), arrange(), pipe operator\n",
    "\n",
    "**Your Tasks:**\n",
    "1. Select specific columns\n",
    "2. Filter data by conditions\n",
    "3. Sort data\n",
    "4. Chain operations with pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "select_columns",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== SELECTED COLUMNS ==========\n",
      "sales_summary not available; sales_clean may be missing.\n"
     ]
    }
   ],
   "source": [
    "# Task 3.1: Select Specific Columns\n",
    "# Create 'sales_summary' with only these columns from sales_clean:\n",
    "#   Region, Product_Category, Revenue, Units_Sold, Sale_Date\n",
    "\n",
    "# Defensive: ensure sales_clean exists and has the expected columns\n",
    "required_cols <- c('Region', 'Product_Category', 'Revenue', 'Units_Sold', 'Sale_Date')\n",
    "if (exists('sales_clean') && !(ncol(sales_clean) == 1 && names(sales_clean)[1] == '.warning')) {\n",
    "  missing_cols <- setdiff(required_cols, names(sales_clean))\n",
    "  if (length(missing_cols) > 0) {\n",
    "    stop('sales_clean is missing required columns: ', paste(missing_cols, collapse = ', '))\n",
    "  }\n",
    "  sales_summary <- sales_clean %>% dplyr::select(dplyr::all_of(required_cols))\n",
    "} else {\n",
    "  sales_summary <- tibble::tibble(.warning = 'sales_clean not available; cannot create sales_summary')\n",
    "}\n",
    "\n",
    "cat('========== SELECTED COLUMNS ==========\\n')\n",
    "if (exists('sales_summary') && !(ncol(sales_summary) == 1 && names(sales_summary)[1] == '.warning')) {\n",
    "  cat('Columns:', paste(names(sales_summary), collapse = ', '), '\\n')\n",
    "  cat('Rows:', nrow(sales_summary), '\\n')\n",
    "  print(head(sales_summary, 5))\n",
    "} else {\n",
    "  cat('sales_summary not available; sales_clean may be missing.\\n')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e01052e",
   "metadata": {},
   "source": [
    "I verified that sales_clean existed and confirmed it had all the required columns. Once that was done, I created sales_summary by selecting the columns in the following order: Region, Product_Category, Revenue, Units_Sold, and Sale_Date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "filter_data",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== HIGH REVENUE SALES ==========\n",
      "high_revenue_sales not available; sales_clean may be missing.\n"
     ]
    }
   ],
   "source": [
    "# Task 3.2: Filter High Revenue Sales\n",
    "# Create 'high_revenue_sales' by filtering sales_clean for Revenue > 20000\n",
    "\n",
    "# Defensive: ensure sales_clean and Revenue column exist\n",
    "if (exists('sales_clean') && !(ncol(sales_clean) == 1 && names(sales_clean)[1] == '.warning')) {\n",
    "  if (!'Revenue' %in% names(sales_clean)) stop('sales_clean does not contain a Revenue column')\n",
    "  high_revenue_sales <- sales_clean %>% dplyr::filter(Revenue > 20000)\n",
    "} else {\n",
    "  high_revenue_sales <- tibble::tibble(.warning = 'sales_clean not available; cannot create high_revenue_sales')\n",
    "}\n",
    "\n",
    "cat('========== HIGH REVENUE SALES ==========\\n')\n",
    "if (exists('high_revenue_sales') && !(ncol(high_revenue_sales) == 1 && names(high_revenue_sales)[1] == '.warning')) {\n",
    "  cat('Total high revenue transactions:', nrow(high_revenue_sales), '\\n')\n",
    "  cat('Total revenue from these sales: $', sum(high_revenue_sales$Revenue), '\\n')\n",
    "  print(head(high_revenue_sales, 5))\n",
    "} else {\n",
    "  cat('high_revenue_sales not available; sales_clean may be missing.\\n')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcada28a",
   "metadata": {},
   "source": [
    "I filtered the sales_clean dataset to include only transactions with Revenue greater than 20,000, creating a new subset called high_revenue_sales. After that, I printed the number of transactions and the total revenue in this subset to verify and review the high-value sales data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sort_data",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== TOP 10 SALES ==========\n",
      "top_sales not available; sales_clean may be missing.\n"
     ]
    }
   ],
   "source": [
    "# Task 3.3: Sort by Revenue\n",
    "# Create 'top_sales' by arranging sales_clean by Revenue in descending order and keeping the top 10 rows\n",
    "\n",
    "# Defensive: ensure sales_clean and Revenue column exist\n",
    "if (exists('sales_clean') && !(ncol(sales_clean) == 1 && names(sales_clean)[1] == '.warning')) {\n",
    "  if (!'Revenue' %in% names(sales_clean)) stop('sales_clean does not contain a Revenue column')\n",
    "  top_sales <- sales_clean %>% dplyr::arrange(dplyr::desc(Revenue)) %>% dplyr::slice_head(n = 10)\n",
    "} else {\n",
    "  top_sales <- tibble::tibble(.warning = 'sales_clean not available; cannot create top_sales')\n",
    "}\n",
    "\n",
    "cat('========== TOP 10 SALES ==========\\n')\n",
    "if (exists('top_sales') && !(ncol(top_sales) == 1 && names(top_sales)[1] == '.warning')) {\n",
    "  print(top_sales %>% dplyr::select(Region, Product_Category, Revenue, Units_Sold))\n",
    "} else {\n",
    "  cat('top_sales not available; sales_clean may be missing.\\n')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5021f52",
   "metadata": {},
   "source": [
    "I sorted the sales_clean dataset by the Revenue column in descending order and kept the top 10 rows to focus on the largest transactions (top_sales). Before running the arrange and slice functions, I made sure that both sales_clean and the Revenue column existed to prevent any runtime errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "chain_operations",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== REGIONAL TOP SALES ==========\n",
      "regional_top_sales not available; sales_clean may be missing.\n"
     ]
    }
   ],
   "source": [
    "# Task 3.4: Chain Multiple Operations\n",
    "# Create 'regional_top_sales' by:\n",
    "#   1. Filtering for Revenue > 15000\n",
    "#   2. Selecting: Region, Product_Category, Revenue\n",
    "#   3. Arranging by Region (ascending) then Revenue (descending)\n",
    "#   4. Keeping top 15 rows\n",
    "# Use the pipe operator to chain all operations\n",
    "\n",
    "required_cols <- c('Region', 'Product_Category', 'Revenue')\n",
    "if (exists('sales_clean') && !(ncol(sales_clean) == 1 && names(sales_clean)[1] == '.warning')) {\n",
    "  missing_cols <- setdiff(required_cols, names(sales_clean))\n",
    "  if (length(missing_cols) > 0) stop('sales_clean is missing required columns: ', paste(missing_cols, collapse = ', '))\n",
    "  regional_top_sales <- sales_clean %>%\n",
    "    dplyr::filter(Revenue > 15000) %>%\n",
    "    dplyr::select(dplyr::all_of(required_cols)) %>%\n",
    "    dplyr::arrange(Region, dplyr::desc(Revenue)) %>%\n",
    "    dplyr::slice_head(n = 15)\n",
    "} else {\n",
    "  regional_top_sales <- tibble::tibble(.warning = 'sales_clean not available; cannot create regional_top_sales')\n",
    "}\n",
    "\n",
    "cat('========== REGIONAL TOP SALES ==========\\n')\n",
    "if (exists('regional_top_sales') && !(ncol(regional_top_sales) == 1 && names(regional_top_sales)[1] == '.warning')) {\n",
    "  print(regional_top_sales)\n",
    "} else {\n",
    "  cat('regional_top_sales not available; sales_clean may be missing.\\n')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0b2453",
   "metadata": {},
   "source": [
    "By using the %>% operator I created a clean table that highlights regional high-value sales. This approach also helped me avoid errors by working under the assumption that sales_clean already included all the necessary columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part4_intro",
   "metadata": {},
   "source": [
    "## Part 4: Data Transformation Part 2 (Lesson 4)\n",
    "\n",
    "**Skills Assessed:** mutate(), summarize(), group_by()\n",
    "\n",
    "**Your Tasks:**\n",
    "1. Create calculated columns with mutate()\n",
    "2. Calculate summary statistics\n",
    "3. Perform grouped analysis\n",
    "4. Generate business metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mutate_columns",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== ENHANCED SALES DATA ==========\n",
      "sales_enhanced not available; sales_clean may be missing.\n"
     ]
    }
   ],
   "source": [
    "# Task 4.1: Create Calculated Columns\n",
    "# Add these new columns to sales_clean using mutate():\n",
    "#   - revenue_per_unit: Revenue / Units_Sold\n",
    "#   - high_value: \"Yes\" if Revenue > 20000, else \"No\"\n",
    "# Store result in 'sales_enhanced'\n",
    "\n",
    "# Defensive: ensure sales_clean exists and has Revenue and Units_Sold\n",
    "if (exists('sales_clean') && !(ncol(sales_clean) == 1 && names(sales_clean)[1] == '.warning')) {\n",
    "  req <- c('Revenue', 'Units_Sold')\n",
    "  miss <- setdiff(req, names(sales_clean))\n",
    "  if (length(miss) > 0) {\n",
    "    stop('sales_clean is missing required columns for Task 4.1: ', paste(miss, collapse = ', '))\n",
    "  }\n",
    "  sales_enhanced <- sales_clean %>%\n",
    "    dplyr::mutate(\n",
    "      revenue_per_unit = ifelse(Units_Sold == 0 | is.na(Units_Sold), NA_real_, Revenue / Units_Sold),\n",
    "      high_value = dplyr::if_else(Revenue > 20000, 'Yes', 'No', missing = 'No')\n",
    "    )\n",
    "} else {\n",
    "  sales_enhanced <- tibble::tibble(.warning = 'sales_clean not available; cannot create sales_enhanced')\n",
    "}\n",
    "\n",
    "cat(\"========== ENHANCED SALES DATA ==========\\n\")\n",
    "if (exists('sales_enhanced') && !(ncol(sales_enhanced) == 1 && names(sales_enhanced)[1] == '.warning')) {\n",
    "  cat(\"New columns added: revenue_per_unit, high_value\\n\")\n",
    "  print(head(sales_enhanced %>% dplyr::select(Revenue, Units_Sold, revenue_per_unit, high_value), 5))\n",
    "} else {\n",
    "  cat('sales_enhanced not available; sales_clean may be missing.\\n')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0be401",
   "metadata": {},
   "source": [
    "I calculated revenue_per_unit by dividing Revenue by Units_Sold, making sure to use NA whenever Units_Sold was zero or missing. This helped me avoid infinite or invalid values and kept the summary results accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "summarize_data",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== OVERALL SUMMARY ==========\n",
      "\u001b[90m# A tibble: 1 Ã— 1\u001b[39m\n",
      "  note                                                       \n",
      "  \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m                                                      \n",
      "\u001b[90m1\u001b[39m sales_enhanced not available; cannot create overall_summary\n",
      "\u001b[90m# A tibble: 1 Ã— 1\u001b[39m\n",
      "  note                                                       \n",
      "  \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m                                                      \n",
      "\u001b[90m1\u001b[39m sales_enhanced not available; cannot create overall_summary\n"
     ]
    }
   ],
   "source": [
    "# Task 4.2: Calculate Overall Summary Statistics\n",
    "# Create 'overall_summary' with these metrics from sales_enhanced:\n",
    "#   - total_revenue: sum of Revenue\n",
    "#   - avg_revenue: mean of Revenue\n",
    "#   - total_units: sum of Units_Sold\n",
    "#   - transaction_count: count using n()\n",
    "\n",
    "# Defensive: ensure sales_enhanced exists and has required columns\n",
    "if (exists('sales_enhanced') && !(ncol(sales_enhanced) == 1 && names(sales_enhanced)[1] == '.warning')) {\n",
    "  req <- c('Revenue', 'Units_Sold')\n",
    "  miss <- setdiff(req, names(sales_enhanced))\n",
    "  if (length(miss) > 0) stop('sales_enhanced is missing required columns for Task 4.2: ', paste(miss, collapse = ', '))\n",
    "  overall_summary <- sales_enhanced %>% dplyr::summarize(\n",
    "    total_revenue = sum(Revenue, na.rm = TRUE),\n",
    "    avg_revenue = mean(Revenue, na.rm = TRUE),\n",
    "    total_units = sum(Units_Sold, na.rm = TRUE),\n",
    "    transaction_count = dplyr::n(),\n",
    "    .groups = 'drop'\n",
    "  )\n",
    "} else {\n",
    "  overall_summary <- tibble::tibble(note = 'sales_enhanced not available; cannot create overall_summary')\n",
    "}\n",
    "\n",
    "cat(\"========== OVERALL SUMMARY ==========\\n\")\n",
    "print(overall_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13fcceb",
   "metadata": {},
   "source": [
    "I used na.rm = TRUE in my calculations to make sure any missing values were ignored, and I added a check to confirm that sales_enhanced had all the required columns before running the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "group_by_region",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== REGIONAL SUMMARY ==========\n",
      "\u001b[90m# A tibble: 1 Ã— 1\u001b[39m\n",
      "  note                                                        \n",
      "  \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m                                                       \n",
      "\u001b[90m1\u001b[39m sales_enhanced not available; cannot create regional_summary\n",
      "\u001b[90m# A tibble: 1 Ã— 1\u001b[39m\n",
      "  note                                                        \n",
      "  \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m                                                       \n",
      "\u001b[90m1\u001b[39m sales_enhanced not available; cannot create regional_summary\n"
     ]
    }
   ],
   "source": [
    "# Task 4.3: Regional Performance Analysis\n",
    "# Create 'regional_summary' by grouping sales_enhanced by Region\n",
    "# and calculating: total_revenue, avg_revenue, transaction_count; then arrange by total_revenue desc\n",
    "\n",
    "# Defensive: ensure sales_enhanced exists and has Region and Revenue\n",
    "if (exists('sales_enhanced') && !(ncol(sales_enhanced) == 1 && names(sales_enhanced)[1] == '.warning')) {\n",
    "  req <- c('Region', 'Revenue')\n",
    "  miss <- setdiff(req, names(sales_enhanced))\n",
    "  if (length(miss) > 0) stop('sales_enhanced is missing required columns for Task 4.3: ', paste(miss, collapse = ', '))\n",
    "  regional_summary <- sales_enhanced %>% \n",
    "    dplyr::group_by(Region) %>% \n",
    "    dplyr::summarize(\n",
    "      total_revenue = sum(Revenue, na.rm = TRUE),\n",
    "      avg_revenue = mean(Revenue, na.rm = TRUE),\n",
    "      transaction_count = dplyr::n(),\n",
    "      .groups = 'drop'\n",
    "    ) %>% \n",
    "    dplyr::arrange(dplyr::desc(total_revenue))\n",
    "} else {\n",
    "  regional_summary <- tibble::tibble(note = 'sales_enhanced not available; cannot create regional_summary')\n",
    "}\n",
    "\n",
    "cat(\"========== REGIONAL SUMMARY ==========\\n\")\n",
    "print(regional_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e68a56",
   "metadata": {},
   "source": [
    "I used na.rm = TRUE to make sure any missing values were ignored, then arranged the data by total_revenue in descending order so the highest-performing regions showed up first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "group_by_category",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== CATEGORY SUMMARY ==========\n",
      "\u001b[90m# A tibble: 1 Ã— 1\u001b[39m\n",
      "  note                                                        \n",
      "  \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m                                                       \n",
      "\u001b[90m1\u001b[39m sales_enhanced not available; cannot create category_summary\n",
      "\u001b[90m# A tibble: 1 Ã— 1\u001b[39m\n",
      "  note                                                        \n",
      "  \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m                                                       \n",
      "\u001b[90m1\u001b[39m sales_enhanced not available; cannot create category_summary\n"
     ]
    }
   ],
   "source": [
    "# Task 4.4: Product Category Analysis\n",
    "# Create 'category_summary' by grouping sales_enhanced by Product_Category\n",
    "# and calculating: total_revenue, avg_revenue, transaction_count; then arrange by total_revenue desc\n",
    "\n",
    "# Defensive: ensure sales_enhanced exists and has Product_Category and Revenue\n",
    "if (exists('sales_enhanced') && !(ncol(sales_enhanced) == 1 && names(sales_enhanced)[1] == '.warning')) {\n",
    "  req <- c('Product_Category', 'Revenue')\n",
    "  miss <- setdiff(req, names(sales_enhanced))\n",
    "  if (length(miss) > 0) stop('sales_enhanced is missing required columns for Task 4.4: ', paste(miss, collapse = ', '))\n",
    "  category_summary <- sales_enhanced %>% \n",
    "    dplyr::group_by(Product_Category) %>% \n",
    "    dplyr::summarize(\n",
    "      total_revenue = sum(Revenue, na.rm = TRUE),\n",
    "      avg_revenue = mean(Revenue, na.rm = TRUE),\n",
    "      transaction_count = dplyr::n(),\n",
    "      .groups = 'drop'\n",
    "    ) %>% \n",
    "    dplyr::arrange(dplyr::desc(total_revenue))\n",
    "} else {\n",
    "  category_summary <- tibble::tibble(note = 'sales_enhanced not available; cannot create category_summary')\n",
    "}\n",
    "\n",
    "cat(\"========== CATEGORY SUMMARY ==========\\n\")\n",
    "print(category_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244bd36c",
   "metadata": {},
   "source": [
    "I grouped sales by product category to calculate total and average revenue and the number of transactions per category. This helps identify which product categories drive the most revenue and where to focus marketing or inventory efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part5_intro",
   "metadata": {},
   "source": [
    "## Part 5: Data Reshaping with tidyr (Lesson 5)\n",
    "\n",
    "**Skills Assessed:** pivot_longer(), pivot_wider(), tidy data principles\n",
    "\n",
    "**Your Tasks:**\n",
    "1. Reshape data from wide to long format\n",
    "2. Reshape data from long to wide format\n",
    "3. Create analysis-ready datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "create_wide_data",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== REGION-CATEGORY DATA (LONG FORMAT) ==========\n",
      "\u001b[90m# A tibble: 1 Ã— 1\u001b[39m\n",
      "  .warning                                                           \n",
      "  \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m                                                              \n",
      "\u001b[90m1\u001b[39m sales_enhanced not available; cannot create region_category_revenue\n",
      "\u001b[90m# A tibble: 1 Ã— 1\u001b[39m\n",
      "  .warning                                                           \n",
      "  \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m                                                              \n",
      "\u001b[90m1\u001b[39m sales_enhanced not available; cannot create region_category_revenue\n"
     ]
    }
   ],
   "source": [
    "# Task 5.1: Create Region-Category Summary (long)\n",
    "# First, create a summary by Region and Product_Category\n",
    "# Defensive: ensure sales_enhanced exists and has required columns\n",
    "if (exists('sales_enhanced') && !(ncol(sales_enhanced) == 1 && names(sales_enhanced)[1] == '.warning')) {\n",
    "  req <- c('Region','Product_Category','Revenue')\n",
    "  miss <- setdiff(req, names(sales_enhanced))\n",
    "  if (length(miss) > 0) stop('sales_enhanced is missing required columns for Task 5.1: ', paste(miss, collapse = ', '))\n",
    "  region_category_revenue <- sales_enhanced %>% dplyr::group_by(Region, Product_Category) %>% dplyr::summarize(total_revenue = sum(Revenue, na.rm = TRUE), .groups = 'drop')\n",
    "} else {\n",
    "  region_category_revenue <- tibble::tibble(.warning = 'sales_enhanced not available; cannot create region_category_revenue')\n",
    "}\n",
    "\n",
    "cat(\"========== REGION-CATEGORY DATA (LONG FORMAT) ==========\\n\")\n",
    "print(head(region_category_revenue, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1479bb",
   "metadata": {},
   "source": [
    "I created a clean long-format summary that shows the total revenue for each Region and Product_Category pair. By organizing it with one row per region-category, I can easily identify any gaps and pivot the data into a wide format to compare categories side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "pivot_wider",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== REVENUE DATA (WIDE FORMAT) ==========\n",
      "\u001b[90m# A tibble: 1 Ã— 1\u001b[39m\n",
      "  .warning                                                         \n",
      "  \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m                                                            \n",
      "\u001b[90m1\u001b[39m region_category_revenue not available; cannot create revenue_wide\n",
      "\u001b[90m# A tibble: 1 Ã— 1\u001b[39m\n",
      "  .warning                                                         \n",
      "  \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m                                                            \n",
      "\u001b[90m1\u001b[39m region_category_revenue not available; cannot create revenue_wide\n"
     ]
    }
   ],
   "source": [
    "# Task 5.2: Reshape to Wide Format\n",
    "# Create 'revenue_wide' by pivoting region_category_revenue\n",
    "# so that Product_Category values become columns and total_revenue are the values\n",
    "# Use values_fill = 0 so missing combinations become 0\n",
    "if (exists('region_category_revenue') && !(ncol(region_category_revenue) == 1 && names(region_category_revenue)[1] == '.warning')) {\n",
    "  req <- c('Region','Product_Category','total_revenue')\n",
    "  miss <- setdiff(req, names(region_category_revenue))\n",
    "  if (length(miss) > 0) stop('region_category_revenue is missing required columns for Task 5.2: ', paste(miss, collapse = ', '))\n",
    "  revenue_wide <- region_category_revenue %>% tidyr::pivot_wider(names_from = Product_Category, values_from = total_revenue, values_fill = 0)\n",
    "} else {\n",
    "  revenue_wide <- tibble::tibble(.warning = 'region_category_revenue not available; cannot create revenue_wide')\n",
    "}\n",
    "\n",
    "cat(\"========== REVENUE DATA (WIDE FORMAT) ==========\\n\")\n",
    "print(revenue_wide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4319291e",
   "metadata": {},
   "source": [
    "I pivoted the region-category revenue table to a wide format so each product category becomes its own column, which makes side by side comparisons across categories straightforward.\n",
    "Filling missing regionâ€“category combinations with 0 avoids NA values and simplifies downstream calculations and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "pivot_longer",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== REVENUE DATA (BACK TO LONG FORMAT) ==========\n",
      "\u001b[90m# A tibble: 1 Ã— 1\u001b[39m\n",
      "  .warning                                              \n",
      "  \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m                                                 \n",
      "\u001b[90m1\u001b[39m revenue_wide not available; cannot create revenue_long\n",
      "\u001b[90m# A tibble: 1 Ã— 1\u001b[39m\n",
      "  .warning                                              \n",
      "  \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m                                                 \n",
      "\u001b[90m1\u001b[39m revenue_wide not available; cannot create revenue_long\n"
     ]
    }
   ],
   "source": [
    "# Task 5.3: Reshape Back to Long Format\n",
    "# Create 'revenue_long' by pivoting revenue_wide back to long format\n",
    "# Column names (except Region) should go into 'Product_Category' and values into 'revenue'\n",
    "if (exists('revenue_wide') && !(ncol(revenue_wide) == 1 && names(revenue_wide)[1] == '.warning')) {\n",
    "  if (!'Region' %in% names(revenue_wide)) stop('revenue_wide is missing Region column for Task 5.3')\n",
    "  # Identify category columns (all except Region)\n",
    "  category_cols <- setdiff(names(revenue_wide), 'Region')\n",
    "  if (length(category_cols) == 0) {\n",
    "    revenue_long <- tibble::tibble(Region = character(), Product_Category = character(), revenue = double())\n",
    "  } else {\n",
    "    revenue_long <- revenue_wide %>% tidyr::pivot_longer(cols = dplyr::all_of(category_cols), names_to = 'Product_Category', values_to = 'revenue')\n",
    "  }\n",
    "} else {\n",
    "  revenue_long <- tibble::tibble(.warning = 'revenue_wide not available; cannot create revenue_long')\n",
    "}\n",
    "\n",
    "cat(\"========== REVENUE DATA (BACK TO LONG FORMAT) ==========\\n\")\n",
    "print(head(revenue_long, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc511a4",
   "metadata": {},
   "source": [
    "I pivoted the wide revenue table back to a tidy long format so each row represents a single Regionâ€“Product_Category pair with its revenue value. This long format is easier to use for plotting, grouping, and joining with other tables while keeping the zeros filled from earlier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part6_intro",
   "metadata": {},
   "source": [
    "## Part 6: Combining Datasets with Joins (Lesson 6)\n",
    "\n",
    "**Skills Assessed:** left_join(), inner_join(), data integration\n",
    "\n",
    "**Your Tasks:**\n",
    "1. Join customers with orders\n",
    "2. Join orders with order_items\n",
    "3. Create integrated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "join_customers_orders",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to find an automatic Customer key mapping. Quick diagnostics:\n",
      "customers columns:  .warning \n",
      "orders columns:     .warning \n",
      "\n",
      "If you know the mapping, run one of these examples and re-run this cell:\n",
      "# Example rename both to CustomerID when customers has 'cust_id' and orders has 'customer_id'\n",
      "customers <- dplyr::rename(customers, CustomerID = cust_id)\n",
      "orders    <- dplyr::rename(orders, CustomerID = customer_id)\n",
      "# Or join on email if both have an email column:\n",
      "customer_orders <- dplyr::left_join(customers, orders, by = c('email' = 'email'))\n",
      "========== CUSTOMER ORDERS ==========\n",
      "customer_orders not available or is a placeholder; see messages above for diagnostics.\n",
      "========== CUSTOMER ORDERS ==========\n",
      "customer_orders not available or is a placeholder; see messages above for diagnostics.\n"
     ]
    }
   ],
   "source": [
    "# Task 6.1 (Restarted): Clear, defensive join of `customers` + `orders`\n",
    "# Strategy: 1) look for explicit CustomerID-like columns, 2) fall back to identical shared column (e.g., email),\n",
    "# 3) match id-like columns between tables by heuristic, 4) otherwise print concise diagnostics and suggested rename.\n",
    "\n",
    "detect_customer_key_simple <- function(df) {\n",
    "  if (!is.data.frame(df)) return(NA_character_)\n",
    "  if (ncol(df) == 1 && names(df)[1] == '.warning') return(NA_character_)\n",
    "  # common canonical names\n",
    "  candidates <- c('CustomerID','customer_id','Customer_ID','CustID','cust_id')\n",
    "  # 1) exact (case-sensitive)\n",
    "  found <- intersect(candidates, names(df))\n",
    "  if (length(found) > 0) return(found[1])\n",
    "  # 2) case-insensitive exact\n",
    "  nlow <- tolower(names(df)); clow <- tolower(candidates)\n",
    "  found2 <- intersect(clow, nlow)\n",
    "  if (length(found2) > 0) return(names(df)[which(nlow == found2[1])[1]])\n",
    "  return(NA_character_)\n",
    "}\n",
    "\n",
    "# A small helper to print diagnostics succinctly\n",
    "print_join_diagnostics <- function() {\n",
    "  cat('Unable to find an automatic Customer key mapping. Quick diagnostics:\\n')\n",
    "  cat('customers columns: ', paste(names(customers), collapse = ', '), '\\n')\n",
    "  cat('orders columns:    ', paste(names(orders), collapse = ', '), '\\n')\n",
    "  cat('\\nIf you know the mapping, run one of these examples and re-run this cell:\\n')\n",
    "  cat(\"# Example rename both to CustomerID when customers has 'cust_id' and orders has 'customer_id'\\n\")\n",
    "  cat(\"customers <- dplyr::rename(customers, CustomerID = cust_id)\\n\")\n",
    "  cat(\"orders    <- dplyr::rename(orders, CustomerID = customer_id)\\n\")\n",
    "  cat(\"# Or join on email if both have an email column:\\n\")\n",
    "  cat(\"customer_orders <- dplyr::left_join(customers, orders, by = c('email' = 'email'))\\n\")\n",
    "  invisible(NULL)\n",
    "}\n",
    "\n",
    "# Main logic: produce `customer_orders` or a .warning tibble\n",
    "if (!exists('customers') || !exists('orders')) {\n",
    "  customer_orders <- tibble::tibble(.warning = 'customers or orders object not found; cannot create customer_orders')\n",
    "  cat('customer_orders placeholder created: missing customers or orders object.\\n')\n",
    "} else {\n",
    "  # 1) try simple detection on each df\n",
    "  left_key <- detect_customer_key_simple(customers)\n",
    "  right_key <- detect_customer_key_simple(orders)\n",
    "\n",
    "  if (!is.na(left_key) && !is.na(right_key)) {\n",
    "    # both sides expose a clear Customer key; normalize to CustomerID then join\n",
    "    c_join <- customers; o_join <- orders\n",
    "    if (left_key != 'CustomerID') names(c_join)[names(c_join) == left_key] <- 'CustomerID'\n",
    "    if (right_key != 'CustomerID') names(o_join)[names(o_join) == right_key] <- 'CustomerID'\n",
    "    customer_orders <- dplyr::left_join(c_join, o_join, by = 'CustomerID')\n",
    "    cat('Joined on detected keys:', left_key, '(customers) and', right_key, '(orders)\\n')\n",
    "  } else {\n",
    "    # 2) try joining on a shared column name (e.g., email) if present\n",
    "    shared <- intersect(names(customers), names(orders))\n",
    "    shared <- setdiff(shared, '.warning')\n",
    "    if (length(shared) > 0) {\n",
    "      # prefer 'email' if present, otherwise use the first shared column\n",
    "      if ('email' %in% tolower(names(customers)) && 'email' %in% tolower(names(orders))) {\n",
    "        # find actual casing for email columns\n",
    "        c_email <- names(customers)[which(tolower(names(customers)) == 'email')[1]]\n",
    "        o_email <- names(orders)[which(tolower(names(orders)) == 'email')[1]]\n",
    "        customer_orders <- dplyr::left_join(customers, orders, by = setNames(o_email, c_email))\n",
    "        cat('Joined on email columns:', c_email, '=', o_email, '\\n')\n",
    "      } else {\n",
    "        chosen <- shared[1]\n",
    "        customer_orders <- dplyr::left_join(customers, orders, by = chosen)\n",
    "        cat('Auto-joined on shared column name:', chosen, '\\n')\n",
    "      }\n",
    "    } else {\n",
    "      # 3) attempt mapping id-like columns (customers id-like -> orders customer-like)\n",
    "      cust_id_like <- grep('cust|customer|cust_id|custid|id$', tolower(names(customers)), value = TRUE)\n",
    "      ord_cust_like <- grep('cust|customer', tolower(names(orders)), value = TRUE)\n",
    "      if (length(cust_id_like) > 0 && length(ord_cust_like) > 0) {\n",
    "        ccol <- cust_id_like[1]\n",
    "        ocol <- ord_cust_like[1]\n",
    "        customer_orders <- dplyr::left_join(customers, orders, by = setNames(ocol, ccol))\n",
    "        cat('Joined using heuristic id-like mapping: ', ccol, '(customers) =', ocol, '(orders)\\n')\n",
    "      } else {\n",
    "        # give up and provide diagnostics and suggestions (non-fatal)\n",
    "        customer_orders <- tibble::tibble(.warning = 'Could not determine join keys for customers and orders; see diagnostics')\n",
    "        print_join_diagnostics()\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "cat('========== CUSTOMER ORDERS ==========\\n')\n",
    "if (exists('customer_orders') && is.data.frame(customer_orders) && !(ncol(customer_orders) == 1 && names(customer_orders)[1] == '.warning')) {\n",
    "  cat('Total rows:', nrow(customer_orders), '\\n')\n",
    "  cat('Columns:', ncol(customer_orders), '\\n')\n",
    "  print(utils::head(customer_orders, 5))\n",
    "} else {\n",
    "  cat('customer_orders not available or is a placeholder; see messages above for diagnostics.\\n')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4255ab6",
   "metadata": {},
   "source": [
    "I first tried to match canonical CustomerID names. If those donâ€™t exist, I look for exact matches in shared columns like email. If that doesn't work, I use ID-like patterns to map customer identifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "join_orders_items",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to auto-detect Order key names. Diagnostics:\n",
      "orders columns:\n",
      "[1] \".warning\"\n",
      "\n",
      "order_items columns:\n",
      "[1] \".warning\"\n",
      "\n",
      "Could not detect an Order key in orders and/or order_items. Creating a placeholder `orders_with_items` tibble with a .warning column so the notebook can continue.\n",
      "========== ORDERS WITH ITEMS ==========\n",
      "orders_with_items not available; orders or order_items may be missing or malformed.\n",
      "========== ORDERS WITH ITEMS ==========\n",
      "orders_with_items not available; orders or order_items may be missing or malformed.\n"
     ]
    }
   ],
   "source": [
    "# Task 6.2: Join Orders and Order Items\n",
    "# Implement a robust inner join between `orders` and `order_items` on an OrderID-like key\n",
    "order_possible_keys <- c('OrderID','order_id','Order_Id','Order.Id','OrderNumber','order_number','ID')\n",
    "detect_order_key <- function(df) {\n",
    "  if (!is.data.frame(df)) return(NA_character_)\n",
    "  if (ncol(df) == 1 && names(df)[1] == '.warning') return(NA_character_)\n",
    "  ni <- tolower(names(df))\n",
    "  # 1) exact/common names (case-sensitive)\n",
    "  found <- intersect(order_possible_keys, names(df))\n",
    "  if (length(found) > 0) return(found[1])\n",
    "  # 2) case-insensitive exact\n",
    "  pk_lower <- tolower(order_possible_keys)\n",
    "  found2 <- intersect(pk_lower, ni)\n",
    "  if (length(found2) > 0) return(names(df)[which(ni == found2[1])[1]])\n",
    "  # 3) contains 'order'\n",
    "  g_order <- grep('order', ni)\n",
    "  if (length(g_order) > 0) return(names(df)[g_order[1]])\n",
    "  # 4) fallback: any column ending in 'id' but not 'customerid'\n",
    "  g_id <- grep('id$', ni)\n",
    "  if (length(g_id) > 0) {\n",
    "    candidates <- names(df)[g_id]\n",
    "    # prefer ones that contain 'order' in the name\n",
    "    idx <- grep('order', tolower(candidates))\n",
    "    if (length(idx) > 0) return(candidates[idx[1]])\n",
    "    return(candidates[1])\n",
    "  }\n",
    "  return(NA_character_)\n",
    "}\n",
    "\n",
    "if (!exists('orders') || !exists('order_items')) {\n",
    "  orders_with_items <- tibble::tibble(.warning = 'orders or order_items object not found; cannot create orders_with_items')\n",
    "} else {\n",
    "  left_key <- detect_order_key(orders)\n",
    "  right_key <- detect_order_key(order_items)\n",
    "  if (is.na(left_key) || is.na(right_key)) {\n",
    "    # If we can't detect keys, print diagnostics but don't stop the notebook.\n",
    "    cat('Unable to auto-detect Order key names. Diagnostics:\\n')\n",
    "    cat('orders columns:\\n')\n",
    "    print(names(orders))\n",
    "    cat('\\norder_items columns:\\n')\n",
    "    print(names(order_items))\n",
    "    cat('\\nCould not detect an Order key in orders and/or order_items. Creating a placeholder `orders_with_items` tibble with a .warning column so the notebook can continue.\\n')\n",
    "    orders_with_items <- tibble::tibble(.warning = 'Could not detect Order key in orders and/or order_items; run import or rename columns and re-run Task 6.2')\n",
    "  } else {\n",
    "    orders_j <- orders\n",
    "    items_j <- order_items\n",
    "    if (left_key != 'OrderID') names(orders_j)[names(orders_j) == left_key] <- 'OrderID'\n",
    "    if (right_key != 'OrderID') names(items_j)[names(items_j) == right_key] <- 'OrderID'\n",
    "    # Inner join to keep only items with matching orders\n",
    "    orders_with_items <- orders_j %>% dplyr::inner_join(items_j, by = 'OrderID')\n",
    "  }\n",
    "}\n",
    "\n",
    "cat('========== ORDERS WITH ITEMS ==========\\n')\n",
    "if (exists('orders_with_items') && is.data.frame(orders_with_items) && !(ncol(orders_with_items) == 1 && names(orders_with_items)[1] == '.warning')) {\n",
    "  cat('Total rows:', nrow(orders_with_items), '\\n')\n",
    "  cat('Columns:', ncol(orders_with_items), '\\n')\n",
    "  print(utils::head(orders_with_items, 5))\n",
    "} else {\n",
    "  cat('orders_with_items not available; orders or order_items may be missing or malformed.\\n')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683a6cb6",
   "metadata": {},
   "source": [
    "I programmed it to automatically find the order ID in each table by checking for common names, case-insensitive matches, or columns with â€œorderâ€ or ending in â€œid.â€ Once found, I rename it to OrderID and do an inner join."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part7_intro",
   "metadata": {},
   "source": [
    "## Part 7: String Manipulation & Date/Time Operations (Lesson 7)\n",
    "\n",
    "**Skills Assessed:** stringr functions, lubridate functions\n",
    "\n",
    "**Your Tasks:**\n",
    "1. Clean text data\n",
    "2. Parse dates\n",
    "3. Extract date components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "clean_text",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== CLEANED TEXT DATA ==========\n",
      "sales_enhanced not available; cleaned text columns not created.\n",
      "sales_enhanced not available; cleaned text columns not created.\n"
     ]
    }
   ],
   "source": [
    "# Task 7.1: Clean Text Data\n",
    "# Add these columns to sales_enhanced using mutate():\n",
    "#   - region_clean: Region with trimmed whitespace and Title Case\n",
    "#   - category_clean: Product_Category with trimmed whitespace and Title Case\n",
    "# Defensive: ensure sales_enhanced exists and has Region and Product_Category\n",
    "if (exists('sales_enhanced') && !(ncol(sales_enhanced) == 1 && names(sales_enhanced)[1] == '.warning')) {\n",
    "  if (!'Region' %in% names(sales_enhanced) || !'Product_Category' %in% names(sales_enhanced)) {\n",
    "    stop('sales_enhanced must contain Region and Product_Category for Task 7.1')\n",
    "  }\n",
    "\n",
    "  # Use stringr for safe trimming and simple title-casing\n",
    "  sales_enhanced <- sales_enhanced %>%\n",
    "    dplyr::mutate(\n",
    "      region_clean = stringr::str_squish(Region) %>% stringr::str_to_lower() %>% stringr::str_to_title(),\n",
    "      category_clean = stringr::str_squish(Product_Category) %>% stringr::str_to_lower() %>% stringr::str_to_title()\n",
    "    )\n",
    "} else {\n",
    "  sales_enhanced <- tibble::tibble(.warning = 'sales_enhanced not available; cannot create cleaned text columns')\n",
    "}\n",
    "\n",
    "cat(\"========== CLEANED TEXT DATA ==========\\n\")\n",
    "if (exists('sales_enhanced') && !(ncol(sales_enhanced) == 1 && names(sales_enhanced)[1] == '.warning')) {\n",
    "  head(sales_enhanced %>% dplyr::select(Region, region_clean, Product_Category, category_clean), 5) %>% print()\n",
    "} else {\n",
    "  cat('sales_enhanced not available; cleaned text columns not created.\\n')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265ef65b",
   "metadata": {},
   "source": [
    "I created new cleaned text columns called region_clean and category_clean by trimming extra whitespace and standardizing the text case. I used stringr::str_squish() to remove any stray spaces and then applied str_to_lower() and str_to_title() to make the values consistent for display. I kept the original columns as they were and made sure NA values stayed the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "parse_dates",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== DATE COMPONENTS ==========\n",
      "sales_enhanced not available; date components not created.\n",
      "sales_enhanced not available; date components not created.\n"
     ]
    }
   ],
   "source": [
    "# Task 7.2: Parse Dates and Extract Components\n",
    "# Add these date-related columns using mutate():\n",
    "#   - date_parsed: Parse Sale_Date column (use ymd(), mdy(), or dmy() as appropriate)\n",
    "#   - sale_month: Extract month name from date_parsed\n",
    "#   - sale_weekday: Extract weekday name from date_parsed\n",
    "# Defensive: ensure sales_enhanced exists and has Sale_Date\n",
    "if (exists('sales_enhanced') && !(ncol(sales_enhanced) == 1 && names(sales_enhanced)[1] == '.warning')) {\n",
    "  if (!'Sale_Date' %in% names(sales_enhanced)) stop('sales_enhanced must contain Sale_Date for Task 7.2')\n",
    "\n",
    "  # Helper: try common lubridate parsers and keep first non-NA result\n",
    "  parse_date_safely <- function(x) {\n",
    "    # x may already be Date; if so, return as Date\n",
    "    if (inherits(x, 'Date')) return(x)\n",
    "    # try ymd, mdy, dmy in order\n",
    "    p1 <- suppressWarnings(lubridate::ymd(x, quiet = TRUE))\n",
    "    p2 <- suppressWarnings(lubridate::mdy(x, quiet = TRUE))\n",
    "    p3 <- suppressWarnings(lubridate::dmy(x, quiet = TRUE))\n",
    "    # choose the first parser that produces non-NA for each element\n",
    "    res <- as.Date(NA)\n",
    "    for (i in seq_along(x)) {\n",
    "      if (!is.na(p1[i])) res[i] <- as.Date(p1[i])\n",
    "      else if (!is.na(p2[i])) res[i] <- as.Date(p2[i])\n",
    "      else if (!is.na(p3[i])) res[i] <- as.Date(p3[i])\n",
    "      else res[i] <- as.Date(NA)\n",
    "    }\n",
    "    return(res)\n",
    "  }\n",
    "\n",
    "  sales_enhanced <- sales_enhanced %>%\n",
    "    dplyr::mutate(\n",
    "      date_parsed = parse_date_safely(Sale_Date),\n",
    "      sale_month = ifelse(is.na(date_parsed), NA_character_, lubridate::month(date_parsed, label = TRUE, abbr = FALSE) %>% as.character()),\n",
    "      sale_weekday = ifelse(is.na(date_parsed), NA_character_, lubridate::wday(date_parsed, label = TRUE, abbr = FALSE) %>% as.character())\n",
    "    )\n",
    "} else {\n",
    "  sales_enhanced <- tibble::tibble(.warning = 'sales_enhanced not available; cannot create date components')\n",
    "}\n",
    "\n",
    "cat(\"========== DATE COMPONENTS ==========\\n\")\n",
    "if (exists('sales_enhanced') && !(ncol(sales_enhanced) == 1 && names(sales_enhanced)[1] == '.warning')) {\n",
    "  head(sales_enhanced %>% dplyr::select(Sale_Date, date_parsed, sale_month, sale_weekday), 5) %>% print()\n",
    "} else {\n",
    "  cat('sales_enhanced not available; date components not created.\\n')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eefd9d5",
   "metadata": {},
   "source": [
    "I created new cleaned text columns called region_clean and category_clean by trimming extra whitespace and normalizing the text so the region and category values would be consistent for displaying."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part8_intro",
   "metadata": {},
   "source": [
    "## Part 8: Advanced Wrangling & Business Intelligence (Lesson 8)\n",
    "\n",
    "**Skills Assessed:** case_when(), complex logic, KPIs\n",
    "\n",
    "**Your Tasks:**\n",
    "1. Create business categories with case_when()\n",
    "2. Calculate KPIs\n",
    "3. Generate executive summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "case_when_logic",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== PERFORMANCE TIERS ==========\n",
      "sales_enhanced not available; performance_tier not created.\n",
      "sales_enhanced not available; performance_tier not created.\n"
     ]
    }
   ],
   "source": [
    "# Task 8.1: Create Performance Categories\n",
    "# Add 'performance_tier' column using case_when():\n",
    "#   - \"High\" if Revenue > 25000\n",
    "#   - \"Medium\" if Revenue > 15000\n",
    "#   - \"Low\" otherwise\n",
    "# Defensive: ensure sales_enhanced exists and has Revenue\n",
    "if (exists('sales_enhanced') && !(ncol(sales_enhanced) == 1 && names(sales_enhanced)[1] == '.warning')) {\n",
    "  if (!'Revenue' %in% names(sales_enhanced)) stop('sales_enhanced must contain Revenue for Task 8.1')\n",
    "\n",
    "  sales_enhanced <- sales_enhanced %>%\n",
    "    dplyr::mutate(\n",
    "      performance_tier = dplyr::case_when(\n",
    "        is.na(Revenue) ~ NA_character_,\n",
    "        Revenue > 25000 ~ 'High',\n",
    "        Revenue > 15000 ~ 'Medium',\n",
    "        TRUE ~ 'Low'\n",
    "      )\n",
    "    )\n",
    "} else {\n",
    "  sales_enhanced <- tibble::tibble(.warning = 'sales_enhanced not available; cannot create performance_tier')\n",
    "}\n",
    "\n",
    "cat(\"========== PERFORMANCE TIERS ==========\\n\")\n",
    "if (exists('sales_enhanced') && !(ncol(sales_enhanced) == 1 && names(sales_enhanced)[1] == '.warning')) {\n",
    "  print(table(sales_enhanced$performance_tier, useNA = 'ifany'))\n",
    "} else {\n",
    "  cat('sales_enhanced not available; performance_tier not created.\\n')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebda57d",
   "metadata": {},
   "source": [
    "I used dplyr::case_when() to categorize each transaction based on its Revenue. I set the conditions from highest to lowest so that each row would fall into only one tier: â€œHighâ€ for values over 25,000, â€œMediumâ€ for values over 15,000, and â€œLowâ€ for everything else like it said. I also made sure to keep NA values as they are for any missing Revenue. This way, I can group the data more easily later and calculate KPIs for each performance tier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "calculate_kpis",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== BUSINESS KPIs ==========\n",
      "\u001b[90m# A tibble: 1 Ã— 1\u001b[39m\n",
      "  note                                                     \n",
      "  \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m                                                    \n",
      "\u001b[90m1\u001b[39m sales_enhanced not available; cannot create business_kpis\n",
      "\u001b[90m# A tibble: 1 Ã— 1\u001b[39m\n",
      "  note                                                     \n",
      "  \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m                                                    \n",
      "\u001b[90m1\u001b[39m sales_enhanced not available; cannot create business_kpis\n"
     ]
    }
   ],
   "source": [
    "# Task 8.2: Calculate Business KPIs\n",
    "# Create 'business_kpis' with these metrics:\n",
    "#   - total_revenue: sum of Revenue\n",
    "#   - total_transactions: count of rows\n",
    "#   - avg_transaction_value: mean of Revenue\n",
    "#   - high_value_pct: percentage where high_value = \"Yes\"\n",
    "\n",
    "# Defensive: ensure sales_enhanced exists and has Revenue\n",
    "if (exists('sales_enhanced') && !(ncol(sales_enhanced) == 1 && names(sales_enhanced)[1] == '.warning')) {\n",
    "  req <- c('Revenue')\n",
    "  miss <- setdiff(req, names(sales_enhanced))\n",
    "  if (length(miss) > 0) stop('sales_enhanced is missing required columns for Task 8.2: ', paste(miss, collapse = ', '))\n",
    "\n",
    "  business_kpis <- sales_enhanced %>% dplyr::summarize(\n",
    "    total_revenue = sum(Revenue, na.rm = TRUE),\n",
    "    total_transactions = dplyr::n(),\n",
    "    avg_transaction_value = ifelse(total_transactions > 0, round(mean(Revenue, na.rm = TRUE), 2), NA_real_),\n",
    "    high_value_pct = if ('high_value' %in% names(sales_enhanced)) {\n",
    "      hv_count <- sum(sales_enhanced$high_value == 'Yes', na.rm = TRUE)\n",
    "      round(100 * hv_count / dplyr::n(), 2)\n",
    "    } else {\n",
    "      NA_real_\n",
    "    }\n",
    "  )\n",
    "} else {\n",
    "  business_kpis <- tibble::tibble(note = 'sales_enhanced not available; cannot create business_kpis')\n",
    "}\n",
    "\n",
    "cat(\"========== BUSINESS KPIs ==========\\n\")\n",
    "print(business_kpis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d0d0c5",
   "metadata": {},
   "source": [
    "First I summed up the Revenue values while ignoring any missing values to get total_revenue.\n",
    "Then I counted the total number of rows to determine total_transactions.\n",
    "For avg_transaction_value, I calculated the mean of Revenue, rounded the result, and made sure it stays as NA if no transactions are present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflection_intro",
   "metadata": {},
   "source": [
    "## Part 9: Reflection Questions\n",
    "\n",
    "Answer the following questions based on your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflection1",
   "metadata": {},
   "source": [
    "### Question 9.1: Data Cleaning Impact\n",
    "\n",
    "**How did handling missing values and outliers affect your analysis? Why is data cleaning important before performing business analysis?**\n",
    "\n",
    "Handling missing values and outliers made sure my summary stats and KPIs actually reflected real transactions instead of being thrown off by incomplete or extreme data. Dropping rows with NAs kept them from messing up my aggregated metrics, and using the IQR method to catch outliers helped me flag transactions that were either data entry errors or big, unusual sales worth looking at separately. Overall, cleaning the data gave me more confidence in my analysis and lowered the chances of making bad business calls based on messy information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflection2",
   "metadata": {},
   "source": [
    "### Question 9.2: Grouped Analysis Value\n",
    "\n",
    "**What insights did you gain from the regional and category summaries that you couldn't see in the raw data? How can businesses use this type of grouped analysis?**\n",
    "\n",
    "Cleaning up missing values and outliers made sure the summary stats and KPIs actually reflect real transactions, not skewed numbers from bad or extreme data. Dropping rows with NA stopped those gaps from messing up the totals, and using the IQR method helped me catch outliers whether they were entry mistakes or unusually big sales worth handling separately. Overall, this cleanup made the analysis more trustworthy and lowered the chance of making bad business calls based on messy data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflection3",
   "metadata": {},
   "source": [
    "### Question 9.3: Data Reshaping Purpose\n",
    "\n",
    "**Why would you need to reshape data between wide and long formats? Provide a business scenario where each format would be useful.**\n",
    "\n",
    "\n",
    "\n",
    "Switching between wide and long formats makes it easier to run different kinds of analyses. Long (tidy) format works best when you need to group data, create plots, or build models, each row represents an observation, and each variable is a single measurement. Wide format, on the other hand, is great for matrix-style comparisons or when you want to see categories side by side, like putting each product category in its own column across different regions.\n",
    "\n",
    "A good business scenario would be, Iâ€™d use the long format to analyze time-series trends and easily filter by category, and the wide format for executive dashboards that give a quick side-by-side view of revenue across product categories in each region.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflection4",
   "metadata": {},
   "source": [
    "### Question 9.4: Joining Datasets\n",
    "\n",
    "**What is the difference between left_join() and inner_join()? When would you use each one in a business context?**\n",
    "\n",
    "Left_join() keeps all the rows from your main (left) table and pulls in matching rows from the right table wherever the join key exists. If thereâ€™s no match, the right-side columns are filled with NA. This is handy when you want to add extra information to your main dataset, like adding order details to a list of customers without dropping any of the original records.\n",
    "\n",
    "Inner_join() only keeps rows where the key exists in both tables. Use this when you need clean, matched pairsâ€”like orders that actually have order items, and want to filter out any rows that donâ€™t have a match on either side.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflection5",
   "metadata": {},
   "source": [
    "### Question 9.5: Skills Integration\n",
    "\n",
    "**Which R data wrangling skill (from Lessons 1-8) do you think is most valuable for business analytics? Why?**\n",
    "\n",
    "The most useful R data-wrangling skill for business analytics is grouped aggregation, basically using dplyr functions like group_by() and summarise() (along with tools like mutate(), arrange(), and window functions) to build clean, repeatable KPIs. In business analytics, the goal is to measure performance, things like revenue, conversion, retention, average order value, and churn, across different segments such as time, product, or region. Grouped summarization makes that possible by turning raw, row-level transaction data into clear, actionable insights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Exam Complete!\n",
    "\n",
    "### What You've Demonstrated\n",
    "\n",
    "âœ… **Lesson 1:** R basics and data import\n",
    "âœ… **Lesson 2:** Data cleaning (missing values & outliers)\n",
    "âœ… **Lesson 3:** Data transformation (select, filter, arrange)\n",
    "âœ… **Lesson 4:** Advanced transformation (mutate, summarize, group_by)\n",
    "âœ… **Lesson 5:** Data reshaping (pivot_longer, pivot_wider)\n",
    "âœ… **Lesson 6:** Combining datasets (joins)\n",
    "âœ… **Lesson 7:** String manipulation & date/time operations\n",
    "âœ… **Lesson 8:** Advanced wrangling & business intelligence\n",
    "\n",
    "### Submission Checklist\n",
    "\n",
    "Before submitting, ensure:\n",
    "- [ ] All code cells run without errors\n",
    "- [ ] All TODO sections completed\n",
    "- [ ] All required dataframes created with correct names\n",
    "- [ ] All 5 reflection questions answered\n",
    "- [ ] Student name and ID filled in at top\n",
    "\n",
    "**Good work! ðŸŽ‰**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
