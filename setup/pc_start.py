#!/usr/bin/env python3
"""
PC Homework Grader Startup Script
Optimized for Windows/Linux systems using llama.cpp
"""

import streamlit as st
import sys
import os
from pathlib import Path

# Add current directory to path for imports
sys.path.append(str(Path(__file__).parent))

from pc_config import validate_pc_setup, print_pc_setup_info, get_pc_config
from pc_llamacpp_client import PCLlamaCppClient, get_available_ai_backends
from pc_two_model_grader import PCTwoModelGrader

# MIG support (optional)
try:
    from mig_manager import MIGManager
    from mig_two_model_grader import MIGTwoModelGrader
    MIG_AVAILABLE = True
except ImportError:
    MIG_AVAILABLE = False

# Ollama support
try:
    from ollama_two_model_grader import OllamaTwoModelGrader
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False

def main():
    """Main PC homework grader application"""
    
    st.set_page_config(
        page_title="PC Homework Grader",
        page_icon="ğŸ–¥ï¸",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    st.title("ğŸ–¥ï¸ PC Homework Grader")
    st.subheader("llama.cpp-based grading system for Windows/Linux")
    
    # Sidebar for configuration
    with st.sidebar:
        st.header("âš™ï¸ Configuration")
        
        # System validation
        if st.button("ğŸ” Check System Setup"):
            with st.spinner("Validating PC setup..."):
                validation = validate_pc_setup()
                
                if validation["ready"]:
                    st.success("âœ… System ready!")
                else:
                    st.error("âŒ Setup incomplete")
                    
                    for error in validation["errors"]:
                        st.error(f"Error: {error}")
                    
                    for warning in validation["warnings"]:
                        st.warning(f"Warning: {warning}")
        
        # Backend selection
        st.subheader("ğŸ¤– AI Backend")
        
        # Check for MIG support
        if MIG_AVAILABLE:
            mig_manager = MIGManager()
            if mig_manager.check_mig_support():
                instances = mig_manager.discover_mig_instances()
                if len(instances) >= 2:
                    st.success(f"ğŸš€ MIG Available: {len(instances)} instances detected")
                    use_mig = st.checkbox("âš¡ Use MIG Parallel Processing", value=True)
                else:
                    st.warning("âš ï¸ MIG supported but not configured")
                    st.info("ğŸ’¡ Run: sudo python setup_mig.py")
                    use_mig = False
            else:
                use_mig = False
        else:
            use_mig = False
        
        backends = get_available_ai_backends()
        
        if backends:
            backend_names = [b["name"] for b in backends]
            selected_backend = st.selectbox("Select Backend:", backend_names)
            
            # Show backend info
            for backend in backends:
                if backend["name"] == selected_backend:
                    st.info(f"â„¹ï¸ {backend['description']}")
                    if backend.get("models_count"):
                        st.info(f"ğŸ“ {backend['models_count']} models available")
        else:
            st.error("âŒ No AI backends available")
            st.info("ğŸ’¡ Install llama-cpp-python and download GGUF models")
            use_mig = False
        
        # Model selection
        st.subheader("ğŸ“ Model Selection")
        
        if st.button("ğŸ” Scan for Models"):
            with st.spinner("Scanning for GGUF models..."):
                client = PCLlamaCppClient()
                models = client.list_available_models()
                
                if models:
                    st.success(f"âœ… Found {len(models)} GGUF models")
                    
                    # Show model details
                    for model in models[:5]:  # Show top 5
                        st.write(f"ğŸ“„ {model['name']} ({model['size_gb']:.1f}GB)")
                else:
                    st.warning("âš ï¸ No GGUF models found")
                    st.info("ğŸ’¡ Download models using LM Studio or HuggingFace")
    
    # Main content area
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ¯ Grade Submissions", "âš™ï¸ Setup", "ğŸ“Š Model Info", "ğŸ“‹ Batch Processing"])
    
    with tab1:
        st.header("ğŸ¯ Grade Individual Submission")
        
        # File upload
        uploaded_file = st.file_uploader(
            "Upload student notebook (.ipynb)",
            type=['ipynb'],
            help="Upload a Jupyter notebook for grading"
        )
        
        if uploaded_file:
            st.success(f"âœ… Uploaded: {uploaded_file.name}")
            
            # Assignment selection
            assignment_type = st.selectbox(
                "Assignment Type:",
                ["Data Cleaning", "Exploratory Analysis", "Statistical Modeling", "Custom"]
            )
            
            # Grading options
            col1, col2 = st.columns(2)
            
            with col1:
                use_two_model = st.checkbox("ğŸ¤– Use Two-Model System", value=True)
                show_progress = st.checkbox("ğŸ“Š Show Progress", value=True)
            
            with col2:
                max_tokens = st.slider("Max Tokens", 1000, 4000, 2000)
                detailed_feedback = st.checkbox("ğŸ“ Detailed Feedback", value=True)
            
            # Grade button
            if st.button("ğŸš€ Start Grading", type="primary"):
                if use_two_model:
                    if 'use_mig' in locals() and use_mig and MIG_AVAILABLE:
                        grade_with_mig_system(uploaded_file, assignment_type, 
                                            max_tokens, show_progress, detailed_feedback)
                    else:
                        grade_with_two_model_system(uploaded_file, assignment_type, 
                                                  max_tokens, show_progress, detailed_feedback)
                else:
                    grade_with_single_model(uploaded_file, assignment_type,
                                          max_tokens, show_progress, detailed_feedback)
    
    with tab2:
        st.header("âš™ï¸ System Setup")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("ğŸ“¦ Installation")
            st.code("""
# Install llama-cpp-python
pip install llama-cpp-python

# For GPU support (NVIDIA)
pip install llama-cpp-python[cuda]

# For GPU support (AMD ROCm)
pip install llama-cpp-python[rocm]
            """)
        
        with col2:
            st.subheader("ğŸ“ Model Downloads")
            st.markdown("""
            **Recommended Models:**
            - Qwen2.5-Coder (for code analysis)
            - Llama 3.1 (for feedback generation)
            - Mistral 7B (lightweight option)
            
            **Download Sources:**
            - LM Studio (easiest)
            - HuggingFace Hub
            - Ollama (with GGUF export)
            """)
        
        # System info
        st.subheader("ğŸ’» System Information")
        if st.button("ğŸ” Show System Info"):
            show_system_info()
    
    with tab3:
        st.header("ğŸ“Š Model Information")
        
        if st.button("ğŸ”„ Refresh Model List"):
            show_model_information()
    
    with tab4:
        st.header("ğŸ“‹ Batch Processing")
        
        st.info("ğŸ’¡ Upload multiple notebooks for batch grading")
        
        uploaded_files = st.file_uploader(
            "Upload multiple notebooks",
            type=['ipynb'],
            accept_multiple_files=True,
            help="Select multiple Jupyter notebooks for batch processing"
        )
        
        if uploaded_files:
            st.success(f"âœ… Uploaded {len(uploaded_files)} files")
            
            # Batch options
            col1, col2 = st.columns(2)
            
            with col1:
                assignment_type = st.selectbox(
                    "Batch Assignment Type:",
                    ["Data Cleaning", "Exploratory Analysis", "Statistical Modeling", "Custom"],
                    key="batch_assignment"
                )
                preload_models = st.checkbox("ğŸš€ Preload Models", value=True)
            
            with col2:
                export_format = st.selectbox("Export Format:", ["CSV", "Excel", "JSON"])
                parallel_processing = st.checkbox("âš¡ Parallel Processing", value=False, disabled=True)
            
            if st.button("ğŸ¯ Start Batch Grading", type="primary"):
                process_batch_grading(uploaded_files, assignment_type, 
                                    preload_models, export_format)

def grade_with_mig_system(uploaded_file, assignment_type, max_tokens, show_progress, detailed_feedback):
    """Grade using MIG parallel processing system"""
    
    try:
        with st.spinner("ğŸš€ Initializing MIG Parallel Grading System..."):
            grader = MIGTwoModelGrader()
            
            # Show MIG status
            mig_status = grader.get_mig_status()
            st.info(f"ğŸ® MIG Instances: {mig_status['total_instances']} total, {mig_status['available_instances']} available")
            
            if mig_status['code_analyzer']:
                st.info(f"ğŸ¤– Code Analyzer: {mig_status['code_analyzer']['name']} on {mig_status['code_analyzer']['mig_instance']}")
            if mig_status['feedback_generator']:
                st.info(f"ğŸ“ Feedback Generator: {mig_status['feedback_generator']['name']} on {mig_status['feedback_generator']['mig_instance']}")
        
        # Process notebook (simplified for demo)
        with st.spinner("ğŸ“– Processing notebook..."):
            # This would normally parse the notebook
            student_code = "# Sample R code\nlibrary(dplyr)\ndata <- read.csv('data.csv')"
            student_markdown = "This analysis explores the dataset..."
            solution_code = "# Reference solution\nlibrary(dplyr)\ndata <- read.csv('data.csv')\nsummary(data)"
            
            assignment_info = {
                "title": f"{assignment_type} Assignment",
                "description": f"Complete the {assignment_type.lower()} tasks"
            }
            
            rubric_elements = {
                "code_correctness": {"weight": 0.4, "max_score": 100},
                "methodology": {"weight": 0.3, "max_score": 100},
                "communication": {"weight": 0.3, "max_score": 100}
            }
        
        # Grade submission with parallel processing
        with st.spinner("âš¡ Grading with MIG parallel processing..."):
            result = grader.grade_submission(
                student_code=student_code,
                student_markdown=student_markdown,
                solution_code=solution_code,
                assignment_info=assignment_info,
                rubric_elements=rubric_elements
            )
        
        # Display results with parallel processing stats
        display_mig_grading_results(result)
        
    except Exception as e:
        st.error(f"âŒ MIG grading failed: {str(e)}")
        st.info("ğŸ’¡ Falling back to regular two-model system...")
        grade_with_two_model_system(uploaded_file, assignment_type, max_tokens, show_progress, detailed_feedback)

def grade_with_two_model_system(uploaded_file, assignment_type, max_tokens, show_progress, detailed_feedback):
    """Grade using PC two-model system"""
    
    try:
        with st.spinner("ğŸš€ Initializing PC Two-Model Grading System..."):
            grader = PCTwoModelGrader()
            
            # Show model info
            model_info = grader.get_model_info()
            st.info(f"ğŸ¤– Code Analyzer: {model_info['code_analyzer']['name']}")
            st.info(f"ğŸ“ Feedback Generator: {model_info['feedback_generator']['name']}")
        
        # Process notebook (simplified for demo)
        with st.spinner("ğŸ“– Processing notebook..."):
            # This would normally parse the notebook
            student_code = "# Sample R code\nlibrary(dplyr)\ndata <- read.csv('data.csv')"
            student_markdown = "This analysis explores the dataset..."
            solution_code = "# Reference solution\nlibrary(dplyr)\ndata <- read.csv('data.csv')\nsummary(data)"
            
            assignment_info = {
                "title": f"{assignment_type} Assignment",
                "description": f"Complete the {assignment_type.lower()} tasks"
            }
            
            rubric_elements = {
                "code_correctness": {"weight": 0.4, "max_score": 100},
                "methodology": {"weight": 0.3, "max_score": 100},
                "communication": {"weight": 0.3, "max_score": 100}
            }
        
        # Grade submission
        with st.spinner("ğŸ¯ Grading submission..."):
            result = grader.grade_submission(
                student_code=student_code,
                student_markdown=student_markdown,
                solution_code=solution_code,
                assignment_info=assignment_info,
                rubric_elements=rubric_elements
            )
        
        # Display results
        display_grading_results(result)
        
    except Exception as e:
        st.error(f"âŒ Grading failed: {str(e)}")

def grade_with_single_model(uploaded_file, assignment_type, max_tokens, show_progress, detailed_feedback):
    """Grade using single PC model"""
    
    try:
        with st.spinner("ğŸš€ Initializing PC Single-Model System..."):
            client = PCLlamaCppClient()
            
            if not client.preload_model():
                st.error("âŒ Failed to load model")
                return
            
            st.info(f"ğŸ¤– Using model: {client.model_name}")
        
        # Simplified grading process
        with st.spinner("ğŸ¯ Grading with single model..."):
            prompt = f"""Grade this {assignment_type} assignment:
            
            Student Code: # Sample code here
            
            Provide a score (0-100) and feedback."""
            
            response = client.generate_response(prompt, max_tokens, show_progress)
            
            if response:
                st.success("âœ… Grading complete!")
                st.text_area("ğŸ“ Grading Results:", response, height=300)
            else:
                st.error("âŒ Failed to generate response")
                
    except Exception as e:
        st.error(f"âŒ Grading failed: {str(e)}")

def process_batch_grading(uploaded_files, assignment_type, preload_models, export_format):
    """Process batch grading"""
    
    try:
        with st.spinner("ğŸš€ Setting up batch processing..."):
            grader = PCTwoModelGrader()
            
            if preload_models:
                grader.preload_models()
        
        # Process each file
        results = []
        progress_bar = st.progress(0)
        
        for i, uploaded_file in enumerate(uploaded_files):
            progress = (i + 1) / len(uploaded_files)
            progress_bar.progress(progress)
            
            st.write(f"ğŸ“‹ Processing {uploaded_file.name}...")
            
            # Simplified processing
            result = {
                "filename": uploaded_file.name,
                "final_score": 85.5,
                "technical_score": 88,
                "feedback_score": 82,
                "grading_time": 45.2
            }
            
            results.append(result)
        
        st.success(f"âœ… Batch grading complete! Processed {len(uploaded_files)} files")
        
        # Display results summary
        import pandas as pd
        df = pd.DataFrame(results)
        st.dataframe(df)
        
        # Export options
        if export_format == "CSV":
            csv = df.to_csv(index=False)
            st.download_button("ğŸ“¥ Download CSV", csv, "batch_results.csv", "text/csv")
        
    except Exception as e:
        st.error(f"âŒ Batch processing failed: {str(e)}")

def display_mig_grading_results(result):
    """Display MIG grading results with parallel processing stats"""
    
    st.success("âœ… MIG Parallel Grading Complete!")
    
    # Score display
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("ğŸ“Š Final Score", f"{result['final_score']}/100")
    
    with col2:
        technical_score = result.get('technical_analysis', {}).get('technical_score', 0)
        st.metric("ğŸ”§ Technical Score", f"{technical_score}/100")
    
    with col3:
        feedback_score = result.get('comprehensive_feedback', {}).get('overall_score', 0)
        st.metric("ğŸ“ Feedback Score", f"{feedback_score}/100")
    
    # Parallel processing stats
    stats = result.get('grading_stats', {})
    if stats:
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("âš¡ Parallel Time", f"{stats.get('parallel_time', 0):.1f}s")
        
        with col2:
            st.metric("ğŸš€ Efficiency Gain", f"{stats.get('parallel_efficiency', 1):.1f}x")
        
        with col3:
            st.metric("ğŸ”§ Code Analysis", f"{stats.get('code_analysis_time', 0):.1f}s")
        
        with col4:
            st.metric("ğŸ“ Feedback Gen", f"{stats.get('feedback_generation_time', 0):.1f}s")
    
    # Show models used
    if stats.get('models_used'):
        st.info(f"ğŸ¤– Models: {stats['models_used']['code_analyzer']} + {stats['models_used']['feedback_generator']}")
    
    # Detailed results
    with st.expander("ğŸ“‹ Detailed Technical Analysis"):
        technical = result.get('technical_analysis', {})
        st.json(technical)
    
    with st.expander("ğŸ’¬ Comprehensive Feedback"):
        feedback = result.get('comprehensive_feedback', {})
        st.json(feedback)
    
    # MIG processing info
    with st.expander("âš¡ MIG Parallel Processing Details"):
        st.write("**Parallel Processing:** âœ… Enabled")
        st.write("**Processing Method:** True parallel execution on separate MIG instances")
        st.write("**GPU Utilization:** Optimized for RTX Pro 6000 with MIG")
        
        if stats:
            st.write(f"**Total Time:** {stats.get('total_time', 0):.1f}s")
            st.write(f"**Parallel Efficiency:** {stats.get('parallel_efficiency', 1):.1f}x speedup vs sequential")

def display_grading_results(result):
    """Display grading results"""
    
    st.success("âœ… Grading Complete!")
    
    # Score display
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("ğŸ“Š Final Score", f"{result['final_score']}/100")
    
    with col2:
        technical_score = result.get('technical_analysis', {}).get('technical_score', 0)
        st.metric("ğŸ”§ Technical Score", f"{technical_score}/100")
    
    with col3:
        feedback_score = result.get('comprehensive_feedback', {}).get('overall_score', 0)
        st.metric("ğŸ“ Feedback Score", f"{feedback_score}/100")
    
    # Detailed results
    with st.expander("ğŸ“‹ Detailed Technical Analysis"):
        technical = result.get('technical_analysis', {})
        st.json(technical)
    
    with st.expander("ğŸ’¬ Comprehensive Feedback"):
        feedback = result.get('comprehensive_feedback', {})
        st.json(feedback)
    
    # Timing information
    stats = result.get('grading_stats', {})
    if stats:
        st.info(f"â±ï¸ Total grading time: {stats.get('total_time', 0):.1f}s")

def show_system_info():
    """Show system information"""
    
    import platform
    import multiprocessing
    
    st.write("**System Information:**")
    st.write(f"- OS: {platform.system()} {platform.release()}")
    st.write(f"- CPU Cores: {multiprocessing.cpu_count()}")
    st.write(f"- Python: {platform.python_version()}")
    
    # GPU info
    from pc_config import detect_gpu_capabilities
    gpu_info = detect_gpu_capabilities()
    
    if gpu_info["nvidia_available"]:
        st.write(f"- GPU: NVIDIA ({gpu_info['gpu_memory_gb']:.1f}GB VRAM)")
    elif gpu_info["amd_available"]:
        st.write(f"- GPU: AMD (ROCm)")
    else:
        st.write("- GPU: None detected")

def show_model_information():
    """Show available model information"""
    
    client = PCLlamaCppClient()
    models = client.list_available_models()
    
    if models:
        st.success(f"âœ… Found {len(models)} GGUF models")
        
        import pandas as pd
        df = pd.DataFrame(models)
        df['size_gb'] = df['size_gb'].round(1)
        
        st.dataframe(df[['name', 'size_gb', 'directory']], use_container_width=True)
    else:
        st.warning("âš ï¸ No GGUF models found")
        st.info("ğŸ’¡ Download models using LM Studio or from HuggingFace")

if __name__ == "__main__":
    main()